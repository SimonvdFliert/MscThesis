{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import WebNLG_xmlReader.benchmark_reader as xml_reader\n",
    "import os.path\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Cleaning and Preperations\n",
    "\n",
    "CACAPO contains data for both pipeline and neural end-to-end structures. As this project only focuses on E2E models, we will not need a majority of the data. The code below extracts the data and makes it easier to retrieve for model fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_train_dataset = [\n",
    "                '../Data/CACAPO/en/Incidents/WebNLGFormatTrain.xml', \n",
    "                '../Data/CACAPO/en/Sports/WebNLGFormatTrain.xml', \n",
    "                '../Data/CACAPO/en/Stocks/WebNLGFormatTrain.xml', \n",
    "                '../Data/CACAPO/en/Weather/WebNLGFormatTrain.xml', \n",
    "                '../Data/CACAPO/nl/Incidents/WebNLGFormatTrain.xml', \n",
    "                '../Data/CACAPO/nl/Sports/WebNLGFormatTrain.xml', \n",
    "                '../Data/CACAPO/nl/Stocks/WebNLGFormatTrain.xml', \n",
    "                '../Data/CACAPO/nl/Weather/WebNLGFormatTrain.xml']\n",
    "\n",
    "combined_dev_dataset = [\n",
    "                '../Data/CACAPO/en/Incidents/WebNLGFormatDev.xml', \n",
    "                '../Data/CACAPO/en/Sports/WebNLGFormatDev.xml', \n",
    "                '../Data/CACAPO/en/Stocks//WebNLGFormatDev.xml', \n",
    "                '../Data/CACAPO/en/Weather/WebNLGFormatDev.xml', \n",
    "                '../Data/CACAPO/nl/Incidents/WebNLGFormatDev.xml', \n",
    "                '../Data/CACAPO/nl/Sports/WebNLGFormatDev.xml', \n",
    "                '../Data/CACAPO/nl/Stocks/WebNLGFormatDev.xml', \n",
    "                '../Data/CACAPO/nl/Weather/WebNLGFormatDev.xml']\n",
    "\n",
    "combined_test_dataset = [\n",
    "                '../Data/CACAPO/en/Incidents/WebNLGFormatTest.xml', \n",
    "                '../Data/CACAPO/en/Sports/WebNLGFormatTest.xml', \n",
    "                '../Data/CACAPO/en/Stocks/WebNLGFormatTest.xml', \n",
    "                '../Data/CACAPO/en/Weather/WebNLGFormatTest.xml', \n",
    "                '../Data/CACAPO/nl/Incidents/WebNLGFormatTest.xml', \n",
    "                '../Data/CACAPO/nl/Sports/WebNLGFormatTest.xml', \n",
    "                '../Data/CACAPO/nl/Stocks/WebNLGFormatTest.xml', \n",
    "                '../Data/CACAPO/nl/Weather/WebNLGFormatTest.xml']\n",
    "\n",
    "all_data = [combined_train_dataset, combined_dev_dataset, combined_test_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# create instance of benchmark class to transform xml\n",
    "train_instance =  xml_reader.Benchmark()\n",
    "dev_instance =  xml_reader.Benchmark()\n",
    "test_instance =  xml_reader.Benchmark()\n",
    "\n",
    "\n",
    "\n",
    "def transform_data(data):\n",
    "    \"\"\"\n",
    "    This function calls the xml_reader code to transform the xml into a more suitable code version to use.\n",
    "    The function takes in the 3 different datasets, which it then transforms and collects into usable variables.\n",
    "    \"\"\"\n",
    "    #loop through the different datasets groups and transform the xml into usable code\n",
    "    for datasets in data:\n",
    "        #choose the right files\n",
    "        files = xml_reader.select_files(datasets)\n",
    "        \n",
    "        #For each datasplit, transform the xml and store the transformation into a usable variable\n",
    "        try:\n",
    "            if datasets == combined_train_dataset:\n",
    "                train_instance.fill_benchmark(files)\n",
    "            elif datasets == combined_dev_dataset:\n",
    "                dev_instance.fill_benchmark(files)\n",
    "            elif datasets == combined_test_dataset:\n",
    "                test_instance.fill_benchmark(files)\n",
    "            \n",
    "            print(f'Completed the transformation of the datasets \\n')\n",
    "        except:\n",
    "            print(\"Error: The proper datasets have not been found. Please check that all dataset splits are available\")\n",
    "    \n",
    "    return train_instance, dev_instance, test_instance\n",
    "\n",
    "\n",
    "def total_data_check(data_instance, iteration):\n",
    "    labels = ['Train', 'Dev', 'Test']\n",
    "    print(f\"Number of entries: in {labels[iteration]}:      {data_instance.entry_count()} \") \n",
    "    print(f\"Number of texts: in {labels[iteration]}:      {data_instance.total_lexcount()} \")\n",
    "    print(f\"Number of distinct properties in {labels[iteration]}:      {len(list(data_instance.unique_p_mtriples()))}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "    \n",
    "def single_entry_check(data_instance):\n",
    "    \n",
    "    for entry in data_instance.entries:\n",
    "        #print(f'entry.id        {entry.id}')\n",
    "        if entry.id == 'Id1':\n",
    "            print(f\"Info about {entry.id} in category '{entry.category}' in size '{entry.size}':\")\n",
    "            print(\"# of lexicalisations\", entry.count_lexs())\n",
    "            print(\"Properties: \", entry.relations())\n",
    "            print(\"RDF triples: \", entry.list_triples())\n",
    "            print(\"Subject:\", entry.modifiedtripleset.triples[0].s)\n",
    "            print(\"Predicate:\", entry.modifiedtripleset.triples[0].p)\n",
    "            print(\"Lexicalisation:\", entry.lexs[0].lex)\n",
    "            #print(\"Another lexicalisation:\", entry.lexs[1].lex)\n",
    "            if entry.dbpedialinks:\n",
    "                # dbpedialinks is a list where each element is a Triple instance\n",
    "                print(\"DB link, en:\", entry.dbpedialinks[0].s)  # subject in English\n",
    "\n",
    "            print(\"Article text\", entry.lexs[0].return_text()) \n",
    "            \n",
    "\n",
    "def extract_data(data_instance):\n",
    "    RDF_set, text_set = [], []\n",
    "\n",
    "    for entry in data_instance.entries:\n",
    "        #RDF_text_set.append((entry.list_triples(), entry.lexs[0].return_text()))\n",
    "\n",
    "        RDF_set.append(entry.list_triples())\n",
    "        text_set.append(entry.lexs[0].return_text())\n",
    "\n",
    "\n",
    "    #print(len(RDF_text_set))\n",
    "    return RDF_set, text_set \n",
    "\n",
    "def write_to_file(data, iteration, data_type):\n",
    "    \"\"\"\n",
    "    Data = dataset\n",
    "    Iteration = iteration to determine the dataset split\n",
    "    data_type = is the dataset RDF or text\n",
    "    \"\"\"\n",
    "    labels = ['Train', 'Dev', 'Test']\n",
    "\n",
    "    #print(f\"path check  {os.path.exists(f'Data/Cleaned_data/{labels[iteration]}')}\")\n",
    "\n",
    "    try:\n",
    "\n",
    "        if (os.path.exists(f'Data/Cleaned_data/{labels[iteration]}') == False) :\n",
    "            print(\"Entered path check\")\n",
    "\n",
    "            save_path = 'C:/Users/Simon/Desktop/Arria Thesis/MscThesis/Data/Cleaned_data/'\n",
    "\n",
    "            name_of_file = f'{labels[iteration]}_{data_type}'\n",
    "\n",
    "            completeName = os.path.join(save_path, name_of_file+\".pkl\")         \n",
    "\n",
    "            with open(completeName, 'wb') as fp:\n",
    "                print(f\"Entered pickle check         {completeName}\")\n",
    "\n",
    "                pickle.dump(data, fp)\n",
    "\n",
    "    except:\n",
    "        print(f'file for {labels[iteration]} already exists')\n",
    "\n",
    "\n",
    "def retrieve_data(file_name):\n",
    "    \n",
    "    dataset_path = f\"../Data/Cleaned_data/{file_name}.pkl\"\n",
    "    with open(dataset_path, 'rb') as f:\n",
    "        dataset = pickle.load(f)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "def Overal_function(data):\n",
    "    transformed_train, transformed_dev, transformed_test = transform_data(data)\n",
    "    # print(f'train:      {transformed_train}')\n",
    "    # print(f'dev:      {transformed_dev}')\n",
    "    # print(f'test:      {transformed_test}')\n",
    "\n",
    "    combined_transformation = [transformed_train, transformed_dev, transformed_test]\n",
    "    RDF_text_datasets =[]\n",
    "\n",
    "    for iteration, dataset in enumerate(combined_transformation):\n",
    "        \n",
    "             \n",
    "        # print(f'dataset {dataset}')\n",
    "        # print(f'iteration {iteration}')\n",
    "        \n",
    "        #total_data_check(dataset, iteration)\n",
    "        #single_entry_check(dataset)\n",
    "\n",
    "        RDF_set, Text_set = extract_data(dataset)\n",
    "        #RDF_text_datasets.append()\n",
    "        write_to_file(RDF_set, iteration, 'RDF')\n",
    "        write_to_file(Text_set, iteration, 'text')\n",
    "        print(\"\\n\\n\\n\")\n",
    "\n",
    "    #return RDF_text_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed the transformation of the datasets \n",
      "\n",
      "Completed the transformation of the datasets \n",
      "\n",
      "Completed the transformation of the datasets \n",
      "\n",
      "0\n",
      "Entered path check\n",
      "Entered pickle check         C:/Users/Simon/Desktop/Arria Thesis/MscThesis/Data/Cleaned_data/Train_RDF.pkl\n",
      "0\n",
      "Entered path check\n",
      "Entered pickle check         C:/Users/Simon/Desktop/Arria Thesis/MscThesis/Data/Cleaned_data/Train_text.pkl\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1\n",
      "Entered path check\n",
      "Entered pickle check         C:/Users/Simon/Desktop/Arria Thesis/MscThesis/Data/Cleaned_data/Dev_RDF.pkl\n",
      "1\n",
      "Entered path check\n",
      "Entered pickle check         C:/Users/Simon/Desktop/Arria Thesis/MscThesis/Data/Cleaned_data/Dev_text.pkl\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "2\n",
      "Entered path check\n",
      "Entered pickle check         C:/Users/Simon/Desktop/Arria Thesis/MscThesis/Data/Cleaned_data/Test_RDF.pkl\n",
      "2\n",
      "Entered path check\n",
      "Entered pickle check         C:/Users/Simon/Desktop/Arria Thesis/MscThesis/Data/Cleaned_data/Test_text.pkl\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Overal_function(all_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.97-cp37-cp37m-win_amd64.whl (1.1 MB)\n",
      "Installing collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.1.97\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Anaconda\\envs\\tensorflow_env\\lib\\site-packages\\transformers\\models\\t5\\tokenization_t5_fast.py:166: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_data(data):\n",
    "    \"\"\"\n",
    "    Data is a list of tuples consisting of (RDF_triple, article text)\n",
    "    \"\"\"\n",
    "    \n",
    "    for entry in data:\n",
    "        tokenizer\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data\n",
    "datasets = Overal_function(all_data)\n",
    "tokenized_data = tokenizer(dataset[\"text\"], return_tensors=\"np\", padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "model.compile(optimizer=Adam(3e-5))\n",
    "\n",
    "model.fit(tokenized_data, labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.11 ('tensorflow_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "781095052831e6ea176298bd2000c18958454b0122d42f81f8945b008a9c1419"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
