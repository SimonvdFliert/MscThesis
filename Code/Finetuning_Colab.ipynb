{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a4dc19e3f1394c988add13c5f293e85f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_11a270ac0d334daeb83bbb5fb95f6eb4",
              "IPY_MODEL_304b8c57448b4452a8bbb0cef094c9ec",
              "IPY_MODEL_86c21a1faf484a2b920a4d462c970343"
            ],
            "layout": "IPY_MODEL_df116b5fe1a341a3a495ddffa9231586"
          }
        },
        "11a270ac0d334daeb83bbb5fb95f6eb4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_02db1ba11461478eade0bbe6ad4c2fe2",
            "placeholder": "​",
            "style": "IPY_MODEL_efdfe7e365b64f578b3d8dfccd42ba92",
            "value": "Downloading: 100%"
          }
        },
        "304b8c57448b4452a8bbb0cef094c9ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0740027ab0974fb792fb18901eea6f04",
            "max": 702,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b78b2d8673934040aa9ed253624df1c6",
            "value": 702
          }
        },
        "86c21a1faf484a2b920a4d462c970343": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a61943b766bf47989bcd1c60adb60a96",
            "placeholder": "​",
            "style": "IPY_MODEL_6cd65f99693347cea59efb9e6f89e297",
            "value": " 702/702 [00:00&lt;00:00, 3.79kB/s]"
          }
        },
        "df116b5fe1a341a3a495ddffa9231586": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "02db1ba11461478eade0bbe6ad4c2fe2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "efdfe7e365b64f578b3d8dfccd42ba92": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0740027ab0974fb792fb18901eea6f04": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b78b2d8673934040aa9ed253624df1c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a61943b766bf47989bcd1c60adb60a96": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6cd65f99693347cea59efb9e6f89e297": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ceed663aaeb440ed8fb6db9056acb279": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7097bd57cd0647d18b025e3ddb079f4e",
              "IPY_MODEL_575bb2b3e57b473984ade675e491aac7",
              "IPY_MODEL_d3c9320278a243c29cc42a45c00a5f53"
            ],
            "layout": "IPY_MODEL_a4c72b21407f45dba820c5f35d777ed9"
          }
        },
        "7097bd57cd0647d18b025e3ddb079f4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_83d03101e3774a9bba6916207a2f7257",
            "placeholder": "​",
            "style": "IPY_MODEL_e82b5dc48b3b4c5ab4d4dde0ca54b33a",
            "value": "Downloading: 100%"
          }
        },
        "575bb2b3e57b473984ade675e491aac7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9c484e994d654be09f0e0d70bdb57f84",
            "max": 2329735129,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1bed24b3949d40478cb5cb64cdcfeb04",
            "value": 2329735129
          }
        },
        "d3c9320278a243c29cc42a45c00a5f53": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4d9ce98ad2124ee6892e4b12d0c86e34",
            "placeholder": "​",
            "style": "IPY_MODEL_248a6955f8b3427e8ae598070d378a04",
            "value": " 2.33G/2.33G [01:08&lt;00:00, 46.8MB/s]"
          }
        },
        "a4c72b21407f45dba820c5f35d777ed9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "83d03101e3774a9bba6916207a2f7257": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e82b5dc48b3b4c5ab4d4dde0ca54b33a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9c484e994d654be09f0e0d70bdb57f84": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1bed24b3949d40478cb5cb64cdcfeb04": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4d9ce98ad2124ee6892e4b12d0c86e34": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "248a6955f8b3427e8ae598070d378a04": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1745d07b84084683a6a31f51ba70928b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7582666307ed43ec965068e664ce0152",
              "IPY_MODEL_401973926c774bac8c835bcb4cf4f6f9",
              "IPY_MODEL_ac9bb915ea024690bdd7eb772a5fe4d7"
            ],
            "layout": "IPY_MODEL_c3dab2d522bc43d1ab3d2905983d0745"
          }
        },
        "7582666307ed43ec965068e664ce0152": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2314f473779647ef8190c6d203d463ab",
            "placeholder": "​",
            "style": "IPY_MODEL_35955b9703ed4d57917cc830bde4a8f3",
            "value": "100%"
          }
        },
        "401973926c774bac8c835bcb4cf4f6f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d959c643516a4d50991a859fc2594813",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e6e7b63561ad43d6a7e331c9135cb38b",
            "value": 3
          }
        },
        "ac9bb915ea024690bdd7eb772a5fe4d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5c3162b5919d4909b7c51a78c1f9d554",
            "placeholder": "​",
            "style": "IPY_MODEL_2a1fe63b67f3410e844b4d3fc17ac965",
            "value": " 3/3 [00:00&lt;00:00, 95.22it/s]"
          }
        },
        "c3dab2d522bc43d1ab3d2905983d0745": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2314f473779647ef8190c6d203d463ab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "35955b9703ed4d57917cc830bde4a8f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d959c643516a4d50991a859fc2594813": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e6e7b63561ad43d6a7e331c9135cb38b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5c3162b5919d4909b7c51a78c1f9d554": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2a1fe63b67f3410e844b4d3fc17ac965": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers --quiet\n",
        "!pip install datasets --quiet\n",
        "!pip install evaluate --quiet\n",
        "!pip install sentencepiece --quiet\n",
        "!pip install accelerate --quiet\n",
        "!pip install rouge_score --quiet\n",
        "!pip install bert_score --quiet\n",
        "#!pip install transformers[deepspeed] --quiet\n",
        "!pip install deepspeed --quiet"
      ],
      "metadata": {
        "id": "0PMhv2uUqbFa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c21b4774-5e81-4768-a58a-b94658bbcad1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 5.3 MB 5.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 71.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 163 kB 79.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 441 kB 5.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 115 kB 83.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 212 kB 77.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 127 kB 87.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 115 kB 82.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 72 kB 1.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 5.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 148 kB 5.0 MB/s \n",
            "\u001b[?25h  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 60 kB 3.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 665 kB 5.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 54 kB 2.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 120 kB 74.3 MB/s \n",
            "\u001b[?25h  Building wheel for deepspeed (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ds_report\n",
        "from deepspeed.runtime.zero.stage_1_and_2 import estimate_zero2_model_states_mem_needs_all_live\n",
        "from transformers import MT5ForConditionalGeneration, T5Tokenizer\n",
        "estimate_zero2_model_states_mem_needs_all_live(model = MT5ForConditionalGeneration.from_pretrained('google/mt5-base'), \n",
        "                                                                                    num_gpus_per_node=1, num_nodes=1, additional_buffer_factor=1.5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 189,
          "referenced_widgets": [
            "a4dc19e3f1394c988add13c5f293e85f",
            "11a270ac0d334daeb83bbb5fb95f6eb4",
            "304b8c57448b4452a8bbb0cef094c9ec",
            "86c21a1faf484a2b920a4d462c970343",
            "df116b5fe1a341a3a495ddffa9231586",
            "02db1ba11461478eade0bbe6ad4c2fe2",
            "efdfe7e365b64f578b3d8dfccd42ba92",
            "0740027ab0974fb792fb18901eea6f04",
            "b78b2d8673934040aa9ed253624df1c6",
            "a61943b766bf47989bcd1c60adb60a96",
            "6cd65f99693347cea59efb9e6f89e297",
            "ceed663aaeb440ed8fb6db9056acb279",
            "7097bd57cd0647d18b025e3ddb079f4e",
            "575bb2b3e57b473984ade675e491aac7",
            "d3c9320278a243c29cc42a45c00a5f53",
            "a4c72b21407f45dba820c5f35d777ed9",
            "83d03101e3774a9bba6916207a2f7257",
            "e82b5dc48b3b4c5ab4d4dde0ca54b33a",
            "9c484e994d654be09f0e0d70bdb57f84",
            "1bed24b3949d40478cb5cb64cdcfeb04",
            "4d9ce98ad2124ee6892e4b12d0c86e34",
            "248a6955f8b3427e8ae598070d378a04"
          ]
        },
        "id": "ZeA7rT8L5yvP",
        "outputId": "2aabb63c-bd6e-421f-a970-cae670a5f3c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/702 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a4dc19e3f1394c988add13c5f293e85f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/2.33G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ceed663aaeb440ed8fb6db9056acb279"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Estimated memory needed for params, optim states and gradients for a:\n",
            "HW: Setup with 1 node, 1 GPU per node.\n",
            "SW: Model with 582M total params.\n",
            "  per CPU  |  per GPU |   Options\n",
            "   13.02GB |   1.08GB | offload_optimizer=cpu \n",
            "    3.25GB |  10.85GB | offload_optimizer=none\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from deepspeed.runtime.zero.stage3 import estimate_zero3_model_states_mem_needs_all_live\n",
        "model = MT5ForConditionalGeneration.from_pretrained('google/mt5-base')\n",
        "estimate_zero3_model_states_mem_needs_all_live(model, num_gpus_per_node=1, num_nodes=1, additional_buffer_factor=1.5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DGorrrOnE_Se",
        "outputId": "0d5962cc-f537-4ae2-a426-7b653119718f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Estimated memory needed for params, optim states and gradients for a:\n",
            "HW: Setup with 1 node, 1 GPU per node.\n",
            "SW: Model with 582M total params, 192M largest layer params.\n",
            "  per CPU  |  per GPU |   Options\n",
            "   14.64GB |   0.72GB | offload_param=cpu , offload_optimizer=cpu , zero_init=1\n",
            "   14.64GB |   0.72GB | offload_param=cpu , offload_optimizer=cpu , zero_init=0\n",
            "   13.02GB |   1.80GB | offload_param=none, offload_optimizer=cpu , zero_init=1\n",
            "   13.02GB |   1.80GB | offload_param=none, offload_optimizer=cpu , zero_init=0\n",
            "    1.07GB |  10.48GB | offload_param=none, offload_optimizer=none, zero_init=1\n",
            "    3.25GB |  10.48GB | offload_param=none, offload_optimizer=none, zero_init=0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install torch==1.10.1+cu11.0 torchvision==0.8.2+cu101 torchaudio==0.7.2 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "\n",
        "#!pip3 install torch==1.10.1+cu113 torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu113"
      ],
      "metadata": {
        "id": "OxVA0LbbszDD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/drive/MyDrive/MscThesis/Evaluation_code/Bartscore.py /content"
      ],
      "metadata": {
        "id": "Mu3rnW6eya_1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f0JXDNInE7SD",
        "outputId": "c4bb8b7a-343b-466b-d975-692f3a0a8e54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat /proc/meminfo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SnsmPzb1Fo1S",
        "outputId": "f26b0890-fca7-4039-ed57-eae779f3f470"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MemTotal:       13297228 kB\n",
            "MemFree:         9152568 kB\n",
            "MemAvailable:   11974108 kB\n",
            "Buffers:           99244 kB\n",
            "Cached:          2833732 kB\n",
            "SwapCached:            0 kB\n",
            "Active:           762440 kB\n",
            "Inactive:        3110316 kB\n",
            "Active(anon):        980 kB\n",
            "Inactive(anon):   866624 kB\n",
            "Active(file):     761460 kB\n",
            "Inactive(file):  2243692 kB\n",
            "Unevictable:           0 kB\n",
            "Mlocked:               0 kB\n",
            "SwapTotal:             0 kB\n",
            "SwapFree:              0 kB\n",
            "Dirty:             10788 kB\n",
            "Writeback:             0 kB\n",
            "AnonPages:        939916 kB\n",
            "Mapped:           634700 kB\n",
            "Shmem:              1296 kB\n",
            "KReclaimable:     115680 kB\n",
            "Slab:             152672 kB\n",
            "SReclaimable:     115680 kB\n",
            "SUnreclaim:        36992 kB\n",
            "KernelStack:        6064 kB\n",
            "PageTables:        13764 kB\n",
            "NFS_Unstable:          0 kB\n",
            "Bounce:                0 kB\n",
            "WritebackTmp:          0 kB\n",
            "CommitLimit:     6648612 kB\n",
            "Committed_AS:    4027272 kB\n",
            "VmallocTotal:   34359738367 kB\n",
            "VmallocUsed:       50656 kB\n",
            "VmallocChunk:          0 kB\n",
            "Percpu:             1416 kB\n",
            "HardwareCorrupted:     0 kB\n",
            "AnonHugePages:         0 kB\n",
            "ShmemHugePages:        0 kB\n",
            "ShmemPmdMapped:        0 kB\n",
            "FileHugePages:         0 kB\n",
            "FilePmdMapped:         0 kB\n",
            "CmaTotal:              0 kB\n",
            "CmaFree:               0 kB\n",
            "HugePages_Total:       0\n",
            "HugePages_Free:        0\n",
            "HugePages_Rsvd:        0\n",
            "HugePages_Surp:        0\n",
            "Hugepagesize:       2048 kB\n",
            "Hugetlb:               0 kB\n",
            "DirectMap4k:      136000 kB\n",
            "DirectMap2M:     3006464 kB\n",
            "DirectMap1G:    12582912 kB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "#import os.path\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import torch \n",
        "import nltk\n",
        "\n",
        "from transformers import AutoTokenizer, T5ForConditionalGeneration, Seq2SeqTrainingArguments, Seq2SeqTrainer, pipeline, AutoModelForSeq2SeqLM\n",
        "from transformers import MT5ForConditionalGeneration, T5Tokenizer, DataCollatorForSeq2Seq\n",
        "import datasets\n",
        "import evaluate\n",
        "import accelerate\n",
        "import deepspeed\n",
        "\n",
        "#import Evaluation_Code.Parent as parent ## code for PARENT metric\n",
        "import Bartscore as bartscore ## code for Bartscore\n",
        "import gc\n",
        "import json\n",
        "from ast import literal_eval\n",
        "\n",
        "from google.colab import files\n"
      ],
      "metadata": {
        "id": "_fvOdUc_vAzC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "############################################################################## Begin Environment Setup ######################################################################################\n",
        "\n",
        "# DeepSpeed requires a distributed environment even when only one process is used.\n",
        "# This emulates a launcher in the notebook\n",
        "\n",
        "os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
        "os.environ[\"MASTER_PORT\"] = \"9994\"  # modify if RuntimeError: Address already in use\n",
        "os.environ[\"RANK\"] = \"0\"\n",
        "os.environ[\"LOCAL_RANK\"] = \"0\"\n",
        "os.environ[\"WORLD_SIZE\"] = \"1\"\n",
        "\n",
        "def ensure_cuda_compatability():\n",
        "    print(f'Torch version: {torch.__version__}')\n",
        "    print(f'Cuda version: {torch.version.cuda}')\n",
        "    print(f'Cudnn version: {torch.backends.cudnn.version()}')\n",
        "    print(f'Is cuda available: {torch.cuda.is_available()}')\n",
        "    print(f'Number of cuda devices: {torch.cuda.device_count()}')\n",
        "    print(f'Current default device: {torch.cuda.current_device()}')\n",
        "    print(f'First cuda device: {torch.cuda.device(0)}')\n",
        "    print(f'Name of the first cuda device: {torch.cuda.get_device_name(0)}\\n\\n')\n",
        "    torch.backends.cuda.matmul.allow_tf32 = True\n",
        "    torch.backends.cudnn.allow_tf32 = True\n",
        "    #Ensure we are really working with full capacity\n",
        "    gc.collect() \n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "############################################################################## End Environment Setup ######################################################################################\n",
        "\n",
        "############################################################################## Begin Model and Dataset Setup ######################################################################################\n",
        "\n",
        "def preprocess_model(model_name):\n",
        "    \"\"\"\n",
        "    Setup the model and tokenizer for preprocessing. This will be a pre-trained model collected from huggingface\n",
        "    \"\"\"\n",
        "\n",
        "    #with deepspeed.zero.Init():\n",
        "      #config = T5Config.from_pretrained(\"t5-small\")\n",
        "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    #model.cuda()\n",
        "\n",
        "    print('LOGGING: preprocess_model DONE \\n')\n",
        "    return model, tokenizer\n",
        "\n",
        "\n",
        "def load_CACAPO_data():\n",
        "    \"\"\"\n",
        "    This function retrieves the csv files and creates a dataset\n",
        "    \"\"\"\n",
        "    print('LOGGING: load_CACAPO_data DONE \\n')\n",
        "\n",
        "    return datasets.load_dataset(\"/content/drive/MyDrive/MscThesis/Data/Cleaned_data\", data_files={\"train\": \"Train.csv\", \"dev\": \"Dev.csv\", \"test\": \"Test.csv\"})\n",
        "\n",
        "\n",
        "def preprocess_data(data):\n",
        "    \"\"\"\n",
        "    Tokenize the data\n",
        "    \"\"\"\n",
        "    max_length = 256\n",
        "    RDFs = data[\"input\"]\n",
        "    texts = data[\"output\"]\n",
        "\n",
        "    ## When converting a pandas df to csv (used for loading dataset), a list of lists can transform to a long string\n",
        "    ## Here we convert it back with literal_eval\n",
        "\n",
        "    for rdf_iteration, rdf in enumerate(RDFs):\n",
        "        RDFs[rdf_iteration] = literal_eval(rdf)\n",
        "\n",
        "    model_inputs = tokenizer(RDFs, truncation=True, padding='max_length', return_tensors='pt',  max_length=max_length, is_split_into_words=True)\n",
        "    \n",
        "    # specially for seq2seq tokenizer, \"Temporarily sets the tokenizer for encoding the targets. Useful for tokenizer associated to sequence-to-sequence models that need a slightly different processing for the labels.\"\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        target_texts = tokenizer(texts, padding='max_length', truncation=True, return_tensors='pt',  max_length=max_length).input_ids\n",
        "        \n",
        "    model_inputs[\"labels\"] = target_texts\n",
        "\n",
        "    return model_inputs\n",
        "\n",
        "\n",
        "\n",
        "def transform_datasets(dataset):\n",
        "    \"\"\"\n",
        "    After loading in and creating the initial dataset, the text data is transformed, by tokenizing the input and output texts. The initial dataset is also split into train,val,test for training use.\n",
        "    NOTE That the test set will not be preprocessed here yet, this will be done in a different function\n",
        "    \"\"\"\n",
        "\n",
        "    ## Create smaller versions of the dataset\n",
        "    # small_train = dataset[\"train\"].shard(num_shards = 256, index = 0)\n",
        "    # small_val = dataset[\"dev\"].shard(num_shards = 256, index = 0)\n",
        "    # small_test = dataset[\"test\"].shard(num_shards = 256, index = 0)\n",
        "\n",
        "    small_train = dataset[\"train\"]\n",
        "    small_val = dataset[\"dev\"]\n",
        "    small_test = dataset[\"test\"]\n",
        "\n",
        "    # to use the actual articles for evaluation\n",
        "    true_articles_test = small_test['output']\n",
        "    # The Parent Metric requires the original RDFs\n",
        "    test_rdf_input = small_test['input']\n",
        "\n",
        "\n",
        "    ## Process the data in batches\n",
        "    small_train = small_train.map(preprocess_data, batched=True, remove_columns=dataset[\"train\"].column_names)\n",
        "    small_val = small_val.map(preprocess_data, batched=True, remove_columns=dataset[\"dev\"].column_names)\n",
        "    small_test = small_test.map(preprocess_data, batched=True, remove_columns=dataset[\"test\"].column_names)\n",
        "\n",
        "    # transform the datasets into torch sensors, as the model will expect this format\n",
        "    small_train.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels']) #, device=\"cuda\")\n",
        "    small_val.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels']) #, device=\"cuda\")\n",
        "    small_test.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels']) #, device=\"cuda\")\n",
        "\n",
        "    print('LOGGING: transform_datasets DONE \\n')\n",
        "\n",
        "    return small_train, small_val, small_test, true_articles_test, test_rdf_input\n",
        "\n",
        "############################################################################## End Model and Dataset Setup ######################################################################################\n",
        "\n",
        "############################################################################## Begin Evaluation Setup######################################################################################\n",
        "\n",
        "\n",
        "def load_eval_metrics():\n",
        "    \"\"\"\n",
        "    Loads in all metrics that will be used later on during evaluation. This is seperated to not load in the metrics a dozen of times during training.\n",
        "    \"\"\"\n",
        "    bleu = datasets.load_metric(\"bleu\")\n",
        "    rouge = evaluate.load('rouge')\n",
        "    meteor = evaluate.load('meteor')\n",
        "    perplexity = evaluate.load(\"perplexity\", module_type=\"metric\")\n",
        "    bertscore = evaluate.load(\"bertscore\")\n",
        "    bart_scorer = bartscore.BARTScorer(device='cuda:0',checkpoint='facebook/bart-base')\n",
        "\n",
        "    print('LOGGING: load_eval_metrics DONE \\n')\n",
        "\n",
        "    return bleu, rouge, meteor, perplexity, bertscore, bart_scorer\n",
        "\n",
        "\n",
        "def postprocess_text(preds, labels):\n",
        "    \"\"\"\n",
        "    Supplementary Method called in decode_text.\n",
        "\n",
        "    Returns list of split decoded labels and predictions for evaluation\n",
        "    \"\"\"\n",
        "    preds = [pred.split() for pred in preds]\n",
        "    labels = [[label.split()] for label in labels]\n",
        "    return preds, labels\n",
        "\n",
        "\n",
        "\n",
        "def decode_text(predictions, labels):\n",
        "    \"\"\"\n",
        "    Supplementary Method called in compute_metrics.\n",
        "\n",
        "    Returns decoded labels and predictions for evaluation\n",
        "    \"\"\"\n",
        "    if isinstance(predictions, tuple):\n",
        "            predictions = predictions[0]\n",
        "\n",
        "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "\n",
        "    # Replace -100 in the labels as we can't decode them.\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    return decoded_preds, decoded_labels\n",
        "\n",
        "############################################################################## End Evaluation Setup######################################################################################\n",
        "\n",
        "############################################################################## Begin Evaluation######################################################################################\n",
        "\n",
        "def evaluate_texts(decoded_preds, decoded_labels):\n",
        "    \"\"\"\n",
        "    Calculates metrics given a list of decoded predictions and decoded labels\n",
        "    \"\"\"\n",
        "    #post_process for BLEU\n",
        "    blue_preds, blue_labels = postprocess_text(decoded_preds,  decoded_labels)\n",
        "\n",
        "    # setup metrics for use\n",
        "    bleu, rouge, meteor, perplexity, bertscore, bart_scorer = load_eval_metrics()\n",
        "\n",
        "    # Calculate the metrics\n",
        "    print(f'\\n LOGGING: Calculating Blue')\n",
        "    bleu_output = bleu.compute(predictions=blue_preds, references=blue_labels)\n",
        "    print(f'\\n LOGGING: Calculating Rouge')\n",
        "    rouge_output = rouge.compute(predictions=decoded_preds, references=decoded_labels)\n",
        "    print(f'\\n LOGGING: Calculating Meteor')\n",
        "    meteor_output = meteor.compute(predictions=decoded_preds, references=decoded_labels)\n",
        "    print(f'\\n LOGGING: Calculating Perplexity')\n",
        "    perp_output = perplexity.compute(predictions=decoded_preds, model_id='gpt2')\n",
        "    print(f'\\n LOGGING: Calculating Bertscore')\n",
        "    bertscore_output = bertscore.compute(predictions=decoded_preds, references=decoded_labels, lang=\"en\")\n",
        "    print(f'\\n LOGGING: Calculating Bartscore')\n",
        "    bart_scores_output = bart_scorer.score(srcs=decoded_preds, tgts=decoded_labels, batch_size=16)\n",
        "\n",
        "    return bleu_output, rouge_output, meteor_output, perp_output, bertscore_output, bart_scores_output\n",
        "\n",
        "\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    \"\"\"\"\n",
        "    Metrics to be evaluated during training and validation\n",
        "    Metrics used: BLEU, ROUGE, METEOR, Bertscore, BARTScore\n",
        "    \"\"\"\n",
        "    # decode the predictions and labels for eval\n",
        "    predictions, labels = pred\n",
        "    decoded_preds, decoded_labels = decode_text(predictions, labels)\n",
        "\n",
        "    bleu_output, rouge_output, meteor_output, perp_output, bertscore_output, bart_scores_output = evaluate_texts(decoded_preds, decoded_labels)\n",
        "\n",
        "    ## Huggingsface trainer requires a dict if multiple metrics are used\n",
        "    return {\"blue_output\": bleu_output, \"rouge_output\": rouge_output, \"meteor_results\": meteor_output,\n",
        "             \"perp_output\": perp_output, \"bertscore_output\": bertscore_output,  \"bart_scores_output\": bart_scores_output}\n",
        "\n",
        "\n",
        "############################################################################## End Evaluation Section######################################################################################\n",
        "\n",
        "############################################################################## Begin Huggingface Trainer Setup ######################################################################################\n",
        "\n",
        "def set_training_args(model_name, learning_rate, num_train_epochs, evaluation_strategy, generation_num_beams, generation_max_length,\n",
        "                         gradient_accumulation_steps, per_device_train_batch_size, per_device_eval_batch_size):\n",
        "    \"\"\"\n",
        "    Setup the training arguments that will be used during training.\n",
        "    \"\"\"\n",
        "    model_dir = f\"/content/drive/MyDrive/MscThesis/Results/{model_name}\"\n",
        "\n",
        "    training_args = Seq2SeqTrainingArguments(\n",
        "                output_dir=model_dir,\n",
        "                learning_rate=learning_rate,\n",
        "                do_eval=True, # will be set to true if evaluation strategy is set\n",
        "                do_predict=True, #Whether to run predictions on the test set or not.\n",
        "                num_train_epochs=num_train_epochs,\n",
        "                evaluation_strategy= evaluation_strategy,\n",
        "                eval_steps= 2500, # Number of update steps between two evaluations if evaluation_strategy=\"steps\". Will default to the same value as logging_steps if not set.\n",
        "                save_steps=500, # Number of updates steps before two checkpoint saves if save_strategy=\"steps\".\n",
        "                #max_steps=10, # the total number of training steps to perform\n",
        "                save_total_limit= 10, # the maximum number of models to keep before deleting the oldest one\n",
        "                predict_with_generate=True, # Whether to use generate to calculate generative metrics (ROUGE, BLEU).\n",
        "                generation_num_beams=generation_num_beams,  #The num_beams to use on each evaluation loop when predict_with_generate=True. Will default to the num_beams value of the model configuration\n",
        "                gradient_checkpointing=True, #\n",
        "                generation_max_length=generation_max_length,\n",
        "                gradient_accumulation_steps=gradient_accumulation_steps, #Number of updates steps to accumulate the gradients for, before performing a backward/update pass\n",
        "                per_device_train_batch_size=per_device_train_batch_size, #The batch size per GPU/TPU core/CPU for training.\n",
        "                per_device_eval_batch_size=per_device_eval_batch_size, #The batch size per GPU/TPU core/CPU for evaluation.\n",
        "                #optim=\"adafactor\", #The optimizer to use: adamw_hf, adamw_torch, adamw_apex_fused, or adafactor.\n",
        "                #report_to=\"tensorboard\",\n",
        "                #fp16=True,\n",
        "                bf16=True, ## should now be possible with rtx 3070\n",
        "                tf32=True, #--> moet dan ook torch.backends.cuda.matmul.allow_tf32 = True\n",
        "                auto_find_batch_size=True, #Whether to find a batch size that will fit into memory automatically through exponential decay,\n",
        "                                            # avoiding CUDA Out-of-Memory errors. Requires accelerate to be installed (pip install accelerate)\n",
        "                #deepspeed = \"/content/drive/MyDrive/MscThesis/Deepseed_zero_3.json\",\n",
        "                deepspeed = \"/content/drive/MyDrive/MscThesis/Deepspeed_zero_2_test.json\",\n",
        "                #deepspeed = \"/content/drive/MyDrive/MscThesis/Dspeed_zero_test.json\",\n",
        "\n",
        "                eval_accumulation_steps=5  #Number of predictions steps to accumulate the output tensors for, \n",
        "                                            # before moving the results to the CPU. If left unset, the whole predictions are accumulated on GPU/TPU \n",
        "                                            # before being moved to the CPU (faster but requires more memory).\n",
        "       )\n",
        "\n",
        "    print('LOGGING: set_training_args DONE \\n')\n",
        "\n",
        "    return training_args\n",
        "\n",
        "\n",
        "def get_clean_model(model_name):\n",
        "    \"\"\"\n",
        "    Simple function to ensure that a new model is used for finetuning\n",
        "    \"\"\"\n",
        "    # with deepspeed.zero.Init():\n",
        "    #   model = AutoModelForSeq2SeqLM.from_pretrained(model_name).cuda()\n",
        "    return AutoModelForSeq2SeqLM.from_pretrained(model_name)#.cuda()\n",
        "\n",
        "def set_trainer(model_name, training_args, train_ds, val_ds, tokenizer):\n",
        "    \"\"\"\n",
        "    Initializes a trainer\n",
        "    Takes in: Model name, training arguments, training dataset, validation dataset, and tokenizer\n",
        "    Returns: Trainer instance\n",
        "    \"\"\"\n",
        "    clean_model = get_clean_model(model_name)\n",
        "    data_collator = DataCollatorForSeq2Seq(tokenizer)\n",
        "    trainer = Seq2SeqTrainer(\n",
        "                model=clean_model,\n",
        "                args=training_args,\n",
        "                train_dataset=train_ds,\n",
        "                eval_dataset=val_ds,\n",
        "                compute_metrics=compute_metrics,\n",
        "                data_collator = data_collator,\n",
        "                tokenizer=tokenizer\n",
        "                )\n",
        "\n",
        "    print('LOGGING: set_trainer DONE \\n')\n",
        "\n",
        "    return trainer\n",
        "############################################################################## End Huggingface Trainer Setup ######################################################################################\n",
        "\n",
        "############################################################################## Begin Train and Save ######################################################################################\n",
        "\n",
        "\n",
        "def train_and_save(trainer, path_model_name):\n",
        "    trainer.train()\n",
        "    trainer.save_model(f\"/content/drive/MyDrive/MscThesis/Models/{path_model_name}\")\n",
        "\n",
        "    print('LOGGING: train_and_save DONE \\n')\n",
        "\n",
        "############################################################################## End Train and Save ######################################################################################\n",
        "\n",
        "\n",
        "############################################################################## Begin Evaluation Process ######################################################################################\n",
        "\n",
        "def get_saved_model(path_model_name):\n",
        "\n",
        "    saved_model_path = f\"/content/drive/MyDrive/MscThesis/Models/{path_model_name}\"\n",
        "    #print(f'path to folder is: {saved_model_path}')\n",
        "\n",
        "    saved_model = T5ForConditionalGeneration.from_pretrained(saved_model_path, local_files_only=True)\n",
        "    saved_model.cuda()\n",
        "    tokenizer = AutoTokenizer.from_pretrained(saved_model_path ,local_files_only=True)\n",
        "    return saved_model, tokenizer\n",
        "\n",
        "def generate_predictions(saved_model, test_set):\n",
        "    \"\"\"\n",
        "    Generates predictions based on the test set, returns a list of predictions and the corresponding \"true\" articles\n",
        "    \"\"\"\n",
        "    encoded_inputs = test_set.remove_columns(\"labels\")\n",
        "\n",
        "    # set-up a dataloader to load in the tokenized test dataset\n",
        "    test_dataloader = torch.utils.data.DataLoader(encoded_inputs,  batch_size=16) #pin_memory=True, --> levert error op: RuntimeError: cannot pin 'torch.cuda.LongTensor' only dense CPU tensors can be pinned\n",
        "\n",
        "    # generate text for each batch\n",
        "    all_predictions = []\n",
        "    for i,batch in enumerate(test_dataloader):\n",
        "        predictions = saved_model.generate(**batch, max_new_tokens = 100, do_sample=True, num_beams = 5, top_p=0.7, repetition_penalty = 1.3) # .to(device) --> RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper__index_select)\n",
        "        all_predictions.append(predictions)\n",
        "\n",
        "    # flatten predictions\n",
        "    all_predictions_flattened = [pred for preds in all_predictions for pred in preds]\n",
        "\n",
        "    print('LOGGING: generate_predictions DONE \\n')\n",
        "\n",
        "\n",
        "    return all_predictions_flattened\n",
        "\n",
        "\n",
        "def decode_predictions(predictions, tokenizer):\n",
        "    \"\"\"\n",
        "    Decode the predictions made by the model\n",
        "    \"\"\"\n",
        "    decoded_predictions = []\n",
        "\n",
        "    for iteration, prediction in enumerate(predictions):\n",
        "        decoded_predictions.append(tokenizer.decode(prediction,skip_special_tokens=True))\n",
        "\n",
        "    print('LOGGING: decode_predictions DONE \\n')\n",
        "\n",
        "    return decoded_predictions\n",
        "\n",
        "\n",
        "\n",
        "def evaluate_test_set(path_model_name, test_set, true_articles_test, test_rdf_input):\n",
        "    \"\"\"\n",
        "    Transforms test set, retrieves predictions, and evaluates these predictions\n",
        "    \"\"\"\n",
        "    saved_model, saved_tokenizer = get_saved_model(path_model_name)\n",
        "\n",
        "    predictions = generate_predictions(saved_model, test_set)\n",
        "\n",
        "    decoded_test_predictions = decode_predictions(predictions, saved_tokenizer)\n",
        "\n",
        "    bleu_output, rouge_output, meteor_output, perp_output, bertscore_output, bart_scores_output = evaluate_texts(decoded_test_predictions, true_articles_test)\n",
        "\n",
        "    ## Huggingsface trainer requires a dict if multiple metrics are used\n",
        "    evaluation_results = {\"blue_output\": bleu_output, \"rouge_output\": rouge_output, \"meteor_results\": meteor_output, \"perp_output\": perp_output, \"bertscore_output\": bertscore_output, \"bart_scores_output\": bart_scores_output}\n",
        "\n",
        "    log_results(path_model_name, evaluation_results)\n",
        "\n",
        "    ##Additional PARENT evaluation\n",
        "    tables = test_rdf_input\n",
        "    references = true_articles_test\n",
        "    generations = decoded_test_predictions\n",
        "    parent_attempt(path_model_name, generations, references, tables)\n",
        "    \n",
        "    return evaluation_results\n",
        "\n",
        "\n",
        "def write_to_text_parent(path_model_name, decoded_predictions, true_articles, rdfs):\n",
        "    \"\"\"\n",
        "    Parent metric requires text files to work\n",
        "    \"\"\"\n",
        "\n",
        "    with open(f'/content/drive/MyDrive/MscThesis/Parent_test/{path_model_name}_true_articles.txt', 'w', encoding='utf-8') as f:\n",
        "        for articles in true_articles:\n",
        "            f.write(f'{articles} \\n')\n",
        "\n",
        "    with open(f'/content/drive/MyDrive/MscThesis/Parent_test/{path_model_name}_decode_predictions.txt', 'w', encoding='utf-8') as f:\n",
        "        for predictions in decoded_predictions:\n",
        "            f.write(f'{predictions} \\n')\n",
        "\n",
        "    with open(f'/content/drive/MyDrive/MscThesis/Parent_test/{path_model_name}_rdfs.txt', 'w', encoding='utf-8') as f:\n",
        "        for pairs in rdfs:\n",
        "            f.write(f'{pairs} \\n')\n",
        "\n",
        "\n",
        "def prepare_inputs_parent(RDFs):\n",
        "    \"\"\"\n",
        "    Cleans the RDF pairs and transforms them in the proper format so that the parent module can calculate with it.\n",
        "    \"\"\"\n",
        "\n",
        "    attribute_value_pairs = []\n",
        "\n",
        "    for iteration, inputRDF in enumerate(RDFs):\n",
        "        split_RDF = inputRDF.split(\", \")\n",
        "        entry=[]\n",
        "        for connected_pair in split_RDF:\n",
        "            if '[' in connected_pair:\n",
        "                connected_pair = connected_pair.replace('[', '')\n",
        "            if ']' in connected_pair:\n",
        "                connected_pair = connected_pair.replace(']', '')\n",
        "            if '_' in connected_pair:\n",
        "                connected_pair = connected_pair.replace('_', ' ')\n",
        "            split_pair = tuple(connected_pair.split(' | '))\n",
        "            entry.append((split_pair))\n",
        "        attribute_value_pairs.append(entry)\n",
        "    return attribute_value_pairs\n",
        "\n",
        "\n",
        "def parent_attempt(path_model_name, generations, references, rdfs):\n",
        "    \"\"\"\n",
        "    The Parent metric needs special treatment, as it only accepts specific inputs and file types.\n",
        "    \"\"\"\n",
        "    prepared_rdfs = prepare_inputs_parent(rdfs)\n",
        "    write_to_text_parent(path_model_name, generations, references, prepared_rdfs)\n",
        "\n",
        "    !python -i \"/content/drive/MyDrive/MscThesis/Evaluation_code/Parent.py\" --references f\"/content/drive/MyDrive/MscThesis/Parent_test/{path_model_name}_true_articles.txt\" \\\n",
        "                                                                              --generations f\"/content/drive/MyDrive/MscThesis/Parent_test/{path_model_name}_decode_predictions.txt\"  \\\n",
        "                                                                              --tables f\"/content/drive/MyDrive/MscThesis/Parent_test/{path_model_name}_rdfs.txt\"\n",
        "\n",
        "def log_results(path_model_name, results):\n",
        "    with open(f'/content/drive/MyDrive/MscThesis/Logging_Results/{path_model_name}_logResults.json', 'w') as convert_file:\n",
        "        convert_file.write(json.dumps(results))\n",
        "\n",
        "############################################################################## End Evaluation Process ######################################################################################\n",
        "\n",
        "############################################################################## Begin Full fine-tune setup######################################################################################\n",
        "\n",
        "def fine_tune_model(model_name):\n",
        "    # ensure cuda compatability\n",
        "    ensure_cuda_compatability()\n",
        "\n",
        "    # I instantiate the tokenizer as a global variable, as the .map function in transform_datasets was not working properly. \n",
        "    # This should not be an issue, as the tokenizer remains consistent during training and evaluation.\n",
        "    global tokenizer\n",
        "    \n",
        "    ## retrieve model and tokenizer from huggingface to prepare dataset\n",
        "    model, tokenizer = preprocess_model(model_name)\n",
        "    \n",
        "    ### retrieve the unprocessed data from the csv files\n",
        "    entire_dataset = load_CACAPO_data()\n",
        "    \n",
        "    # ## process the dataset and split it into its natural train, val, test split\n",
        "    train_ds, val_ds, test_ds, true_articles_test, test_rdf_inputs = transform_datasets(entire_dataset)\n",
        "\n",
        "    ### setup the training arguments \n",
        "    training_args = set_training_args(model_name=model_name, learning_rate = 0.01, \n",
        "                                      num_train_epochs= 25, evaluation_strategy = 'steps', generation_num_beams=5, generation_max_length = 100, \n",
        "                                      gradient_accumulation_steps = 3, per_device_train_batch_size= 64, per_device_eval_batch_size= 64)\n",
        "\n",
        "    ###create a trainer instance \n",
        "    trainer = set_trainer(model_name, training_args, train_ds, val_ds, tokenizer)\n",
        "\n",
        "    # Both mt5 and T5-dutch have / in their name, which makes pathing more chaotic\n",
        "    if '/' in model_name:\n",
        "       path_model_name = model_name.replace('/', '_')\n",
        "    elif '-' in model_name:\n",
        "       path_model_name = model_name.replace('-', '_')\n",
        "\n",
        "    ## Finally fine-tune the model and save it\n",
        "    train_and_save(trainer, path_model_name)\n",
        "\n",
        "    testset_evaluation_results = evaluate_test_set( path_model_name, test_ds, true_articles_test, test_rdf_inputs)\n",
        "\n",
        "    return testset_evaluation_results\n",
        "\n",
        "def main():\n",
        "    global model_name\n",
        "    models = [\"t5-base\", \"yhavinga/t5-v1.1-base-dutch-cased\", 'google/mt5-base']\n",
        "\n",
        "    model_name = 'google/mt5-base' \n",
        "    results = fine_tune_model(model_name)\n",
        "\n",
        "############################################################################## End Full fine-tune setup######################################################################################\n"
      ],
      "metadata": {
        "id": "ani598aJpyYM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %load_ext tensorboard\n",
        "# %tensorboard --logdir '/content/drive/MyDrive/MscThesis/Tensorboard'/runs"
      ],
      "metadata": {
        "id": "UXglSmgR4fIE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "1745d07b84084683a6a31f51ba70928b",
            "7582666307ed43ec965068e664ce0152",
            "401973926c774bac8c835bcb4cf4f6f9",
            "ac9bb915ea024690bdd7eb772a5fe4d7",
            "c3dab2d522bc43d1ab3d2905983d0745",
            "2314f473779647ef8190c6d203d463ab",
            "35955b9703ed4d57917cc830bde4a8f3",
            "d959c643516a4d50991a859fc2594813",
            "e6e7b63561ad43d6a7e331c9135cb38b",
            "5c3162b5919d4909b7c51a78c1f9d554",
            "2a1fe63b67f3410e844b4d3fc17ac965"
          ]
        },
        "id": "cPiL291Lp0O8",
        "outputId": "60b8bccc-510f-4d01-84b8-5a94e7217bbf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Torch version: 1.12.1+cu113\n",
            "Cuda version: 11.3\n",
            "Cudnn version: 8302\n",
            "Is cuda available: True\n",
            "Number of cuda devices: 1\n",
            "Current default device: 0\n",
            "First cuda device: <torch.cuda.device object at 0x7f9c26b5fa50>\n",
            "Name of the first cuda device: A100-SXM4-40GB\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/convert_slow_tokenizer.py:447: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
            "  \"The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option\"\n",
            "WARNING:datasets.builder:Using custom data configuration Cleaned_data-2a3548bcccec7ab2\n",
            "WARNING:datasets.builder:Found cached dataset csv (/root/.cache/huggingface/datasets/csv/Cleaned_data-2a3548bcccec7ab2/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOGGING: preprocess_model DONE \n",
            "\n",
            "LOGGING: load_CACAPO_data DONE \n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1745d07b84084683a6a31f51ba70928b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/Cleaned_data-2a3548bcccec7ab2/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-ea51b5fb13db3281.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/Cleaned_data-2a3548bcccec7ab2/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-7915005f5e1d7399.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/Cleaned_data-2a3548bcccec7ab2/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-d42cc046c894077f.arrow\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOGGING: transform_datasets DONE \n",
            "\n",
            "[2022-10-27 15:59:42,255] [INFO] [comm.py:635:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
            "LOGGING: set_training_args DONE \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cuda_amp half precision backend\n",
            "/usr/local/lib/python3.7/dist-packages/accelerate/memory_utils.py:26: FutureWarning: memory_utils has been reorganized to utils.memory. Import `find_executable_batchsize` from the main `__init__`: `from accelerate import find_executable_batch_size` to avoid this warning.\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOGGING: set_trainer DONE \n",
            "\n",
            "[2022-10-27 15:59:49,719] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed info: version=0.7.4, git-hash=unknown, git-branch=unknown\n",
            "[2022-10-27 15:59:54,249] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
            "Installed CUDA version 11.2 does not match the version torch was compiled with 11.3 but since the APIs are compatible, accepting this combination\n",
            "Using /root/.cache/torch_extensions/py37_cu113 as PyTorch extensions root...\n",
            "Detected CUDA files, patching ldflags\n",
            "Emitting ninja build file /root/.cache/torch_extensions/py37_cu113/cpu_adam/build.ninja...\n",
            "Building extension module cpu_adam...\n",
            "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
            "Loading extension module cpu_adam...\n",
            "Time to load cpu_adam op: 3.3992526531219482 seconds\n",
            "[2022-10-27 16:00:01,956] [INFO] [logging.py:68:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adamw as basic optimizer\n",
            "[2022-10-27 16:00:01,979] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam\n",
            "[2022-10-27 16:00:01,981] [INFO] [utils.py:53:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>\n",
            "[2022-10-27 16:00:01,983] [INFO] [logging.py:68:log_dist] [Rank 0] Creating fp16 ZeRO stage 2 optimizer\n",
            "[2022-10-27 16:00:01,985] [INFO] [stage_1_and_2.py:140:__init__] Reduce bucket size 200000000\n",
            "[2022-10-27 16:00:01,986] [INFO] [stage_1_and_2.py:141:__init__] Allgather bucket size 200000000\n",
            "[2022-10-27 16:00:01,988] [INFO] [stage_1_and_2.py:142:__init__] CPU Offload: True\n",
            "[2022-10-27 16:00:01,989] [INFO] [stage_1_and_2.py:143:__init__] Round robin gradient partitioning: False\n",
            "Using /root/.cache/torch_extensions/py37_cu113 as PyTorch extensions root...\n",
            "Emitting ninja build file /root/.cache/torch_extensions/py37_cu113/utils/build.ninja...\n",
            "Building extension module utils...\n",
            "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
            "Loading extension module utils...\n",
            "Time to load utils op: 0.6976799964904785 seconds\n",
            "Rank: 0 partition count [1] and sizes[(582401280, False)] \n",
            "[2022-10-27 16:00:04,971] [INFO] [utils.py:827:see_memory_usage] Before initializing optimizer states\n",
            "[2022-10-27 16:00:04,974] [INFO] [utils.py:832:see_memory_usage] MA 1.44 GB         Max_MA 1.44 GB         CA 2.2 GB         Max_CA 2 GB \n",
            "[2022-10-27 16:00:04,978] [INFO] [utils.py:837:see_memory_usage] CPU Virtual Memory:  used = 8.47 GB, percent = 10.2%\n",
            "[2022-10-27 16:00:07,891] [INFO] [utils.py:827:see_memory_usage] After initializing optimizer states\n",
            "[2022-10-27 16:00:07,893] [INFO] [utils.py:832:see_memory_usage] MA 1.44 GB         Max_MA 1.44 GB         CA 2.2 GB         Max_CA 2 GB \n",
            "[2022-10-27 16:00:07,896] [INFO] [utils.py:837:see_memory_usage] CPU Virtual Memory:  used = 16.9 GB, percent = 20.2%\n",
            "[2022-10-27 16:00:07,897] [INFO] [stage_1_and_2.py:523:__init__] optimizer state initialized\n",
            "[2022-10-27 16:00:08,092] [INFO] [utils.py:827:see_memory_usage] After initializing ZeRO optimizer\n",
            "[2022-10-27 16:00:08,096] [INFO] [utils.py:832:see_memory_usage] MA 1.44 GB         Max_MA 1.44 GB         CA 2.2 GB         Max_CA 2 GB \n",
            "[2022-10-27 16:00:08,099] [INFO] [utils.py:837:see_memory_usage] CPU Virtual Memory:  used = 16.9 GB, percent = 20.2%\n",
            "[2022-10-27 16:00:08,110] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed Final Optimizer = adamw\n",
            "[2022-10-27 16:00:08,111] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = WarmupLR\n",
            "[2022-10-27 16:00:08,112] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7f9b5a2eb290>\n",
            "[2022-10-27 16:00:08,113] [INFO] [logging.py:68:log_dist] [Rank 0] step=0, skipped=0, lr=[0.01], mom=[[0.9, 0.999]]\n",
            "[2022-10-27 16:00:08,116] [INFO] [config.py:1002:print] DeepSpeedEngine configuration:\n",
            "[2022-10-27 16:00:08,117] [INFO] [config.py:1006:print]   activation_checkpointing_config  {\n",
            "    \"partition_activations\": false, \n",
            "    \"contiguous_memory_optimization\": false, \n",
            "    \"cpu_checkpointing\": false, \n",
            "    \"number_checkpoints\": null, \n",
            "    \"synchronize_checkpoint_boundary\": false, \n",
            "    \"profile\": false\n",
            "}\n",
            "[2022-10-27 16:00:08,119] [INFO] [config.py:1006:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
            "[2022-10-27 16:00:08,120] [INFO] [config.py:1006:print]   amp_enabled .................. False\n",
            "[2022-10-27 16:00:08,121] [INFO] [config.py:1006:print]   amp_params ................... {'opt_level': 'O1'}\n",
            "[2022-10-27 16:00:08,122] [INFO] [config.py:1006:print]   autotuning_config ............ {\n",
            "    \"enabled\": false, \n",
            "    \"start_step\": null, \n",
            "    \"end_step\": null, \n",
            "    \"metric_path\": null, \n",
            "    \"arg_mappings\": null, \n",
            "    \"metric\": \"throughput\", \n",
            "    \"model_info\": null, \n",
            "    \"results_dir\": null, \n",
            "    \"exps_dir\": null, \n",
            "    \"overwrite\": true, \n",
            "    \"fast\": true, \n",
            "    \"start_profile_step\": 3, \n",
            "    \"end_profile_step\": 5, \n",
            "    \"tuner_type\": \"gridsearch\", \n",
            "    \"tuner_early_stopping\": 5, \n",
            "    \"tuner_num_trials\": 50, \n",
            "    \"model_info_path\": null, \n",
            "    \"mp_size\": 1, \n",
            "    \"max_train_batch_size\": null, \n",
            "    \"min_train_batch_size\": 1, \n",
            "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
            "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
            "    \"num_tuning_micro_batch_sizes\": 3\n",
            "}\n",
            "[2022-10-27 16:00:08,123] [INFO] [config.py:1006:print]   bfloat16_enabled ............. True\n",
            "[2022-10-27 16:00:08,124] [INFO] [config.py:1006:print]   checkpoint_parallel_write_pipeline  False\n",
            "[2022-10-27 16:00:08,126] [INFO] [config.py:1006:print]   checkpoint_tag_validation_enabled  True\n",
            "[2022-10-27 16:00:08,127] [INFO] [config.py:1006:print]   checkpoint_tag_validation_fail  False\n",
            "[2022-10-27 16:00:08,128] [INFO] [config.py:1006:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f9c26ba4b10>\n",
            "[2022-10-27 16:00:08,129] [INFO] [config.py:1006:print]   communication_data_type ...... None\n",
            "[2022-10-27 16:00:08,130] [INFO] [config.py:1006:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
            "[2022-10-27 16:00:08,131] [INFO] [config.py:1006:print]   curriculum_enabled ........... False\n",
            "[2022-10-27 16:00:08,133] [INFO] [config.py:1006:print]   curriculum_params ............ False\n",
            "[2022-10-27 16:00:08,134] [INFO] [config.py:1006:print]   dataloader_drop_last ......... False\n",
            "[2022-10-27 16:00:08,135] [INFO] [config.py:1006:print]   disable_allgather ............ False\n",
            "[2022-10-27 16:00:08,136] [INFO] [config.py:1006:print]   dump_state ................... False\n",
            "[2022-10-27 16:00:08,137] [INFO] [config.py:1006:print]   dynamic_loss_scale_args ...... None\n",
            "[2022-10-27 16:00:08,138] [INFO] [config.py:1006:print]   eigenvalue_enabled ........... False\n",
            "[2022-10-27 16:00:08,139] [INFO] [config.py:1006:print]   eigenvalue_gas_boundary_resolution  1\n",
            "[2022-10-27 16:00:08,141] [INFO] [config.py:1006:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
            "[2022-10-27 16:00:08,142] [INFO] [config.py:1006:print]   eigenvalue_layer_num ......... 0\n",
            "[2022-10-27 16:00:08,143] [INFO] [config.py:1006:print]   eigenvalue_max_iter .......... 100\n",
            "[2022-10-27 16:00:08,145] [INFO] [config.py:1006:print]   eigenvalue_stability ......... 1e-06\n",
            "[2022-10-27 16:00:08,146] [INFO] [config.py:1006:print]   eigenvalue_tol ............... 0.01\n",
            "[2022-10-27 16:00:08,147] [INFO] [config.py:1006:print]   eigenvalue_verbose ........... False\n",
            "[2022-10-27 16:00:08,148] [INFO] [config.py:1006:print]   elasticity_enabled ........... False\n",
            "[2022-10-27 16:00:08,150] [INFO] [config.py:1006:print]   flops_profiler_config ........ {\n",
            "    \"enabled\": false, \n",
            "    \"profile_step\": 1, \n",
            "    \"module_depth\": -1, \n",
            "    \"top_modules\": 1, \n",
            "    \"detailed\": true, \n",
            "    \"output_file\": null\n",
            "}\n",
            "[2022-10-27 16:00:08,151] [INFO] [config.py:1006:print]   fp16_auto_cast ............... None\n",
            "[2022-10-27 16:00:08,152] [INFO] [config.py:1006:print]   fp16_enabled ................. False\n",
            "[2022-10-27 16:00:08,153] [INFO] [config.py:1006:print]   fp16_master_weights_and_gradients  False\n",
            "[2022-10-27 16:00:08,154] [INFO] [config.py:1006:print]   global_rank .................. 0\n",
            "[2022-10-27 16:00:08,155] [INFO] [config.py:1006:print]   gradient_accumulation_steps .. 3\n",
            "[2022-10-27 16:00:08,156] [INFO] [config.py:1006:print]   gradient_clipping ............ 1.0\n",
            "[2022-10-27 16:00:08,158] [INFO] [config.py:1006:print]   gradient_predivide_factor .... 1.0\n",
            "[2022-10-27 16:00:08,159] [INFO] [config.py:1006:print]   initial_dynamic_scale ........ 1\n",
            "[2022-10-27 16:00:08,160] [INFO] [config.py:1006:print]   load_universal_checkpoint .... False\n",
            "[2022-10-27 16:00:08,161] [INFO] [config.py:1006:print]   loss_scale ................... 1.0\n",
            "[2022-10-27 16:00:08,162] [INFO] [config.py:1006:print]   memory_breakdown ............. False\n",
            "[2022-10-27 16:00:08,163] [INFO] [config.py:1006:print]   monitor_config ............... <deepspeed.monitor.config.DeepSpeedMonitorConfig object at 0x7f9c26ba4d90>\n",
            "[2022-10-27 16:00:08,165] [INFO] [config.py:1006:print]   nebula_config ................ {\n",
            "    \"enabled\": false, \n",
            "    \"persistent_storage_path\": null, \n",
            "    \"persistent_time_interval\": 100, \n",
            "    \"num_of_version_in_retention\": 2, \n",
            "    \"enable_nebula_load\": true, \n",
            "    \"load_path\": null\n",
            "}\n",
            "[2022-10-27 16:00:08,166] [INFO] [config.py:1006:print]   optimizer_legacy_fusion ...... False\n",
            "[2022-10-27 16:00:08,167] [INFO] [config.py:1006:print]   optimizer_name ............... adamw\n",
            "[2022-10-27 16:00:08,168] [INFO] [config.py:1006:print]   optimizer_params ............. {'lr': 0.01, 'betas': [0.9, 0.999], 'eps': 1e-08, 'weight_decay': 0.0}\n",
            "[2022-10-27 16:00:08,169] [INFO] [config.py:1006:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}\n",
            "[2022-10-27 16:00:08,171] [INFO] [config.py:1006:print]   pld_enabled .................. False\n",
            "[2022-10-27 16:00:08,172] [INFO] [config.py:1006:print]   pld_params ................... False\n",
            "[2022-10-27 16:00:08,173] [INFO] [config.py:1006:print]   prescale_gradients ........... False\n",
            "[2022-10-27 16:00:08,174] [INFO] [config.py:1006:print]   scheduler_name ............... WarmupLR\n",
            "[2022-10-27 16:00:08,176] [INFO] [config.py:1006:print]   scheduler_params ............. {'warmup_min_lr': 0, 'warmup_max_lr': 0.01, 'warmup_num_steps': 0}\n",
            "[2022-10-27 16:00:08,177] [INFO] [config.py:1006:print]   sparse_attention ............. None\n",
            "[2022-10-27 16:00:08,178] [INFO] [config.py:1006:print]   sparse_gradients_enabled ..... False\n",
            "[2022-10-27 16:00:08,179] [INFO] [config.py:1006:print]   steps_per_print .............. 2000\n",
            "[2022-10-27 16:00:08,181] [INFO] [config.py:1006:print]   train_batch_size ............. 192\n",
            "[2022-10-27 16:00:08,182] [INFO] [config.py:1006:print]   train_micro_batch_size_per_gpu  64\n",
            "[2022-10-27 16:00:08,183] [INFO] [config.py:1006:print]   wall_clock_breakdown ......... False\n",
            "[2022-10-27 16:00:08,184] [INFO] [config.py:1006:print]   world_size ................... 1\n",
            "[2022-10-27 16:00:08,185] [INFO] [config.py:1006:print]   zero_allow_untested_optimizer  False\n",
            "[2022-10-27 16:00:08,186] [INFO] [config.py:1006:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='cpu', nvme_path=None, buffer_count=5, buffer_size=100000000, max_in_cpu=1000000000, pin_memory=True) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False\n",
            "[2022-10-27 16:00:08,187] [INFO] [config.py:1006:print]   zero_enabled ................. True\n",
            "[2022-10-27 16:00:08,189] [INFO] [config.py:1006:print]   zero_optimization_stage ...... 2\n",
            "[2022-10-27 16:00:08,190] [INFO] [config.py:997:print_user_config]   json = {\n",
            "    \"bf16\": {\n",
            "        \"enabled\": true\n",
            "    }, \n",
            "    \"amp\": {\n",
            "        \"enabled\": false, \n",
            "        \"opt_level\": \"O1\"\n",
            "    }, \n",
            "    \"optimizer\": {\n",
            "        \"type\": \"AdamW\", \n",
            "        \"params\": {\n",
            "            \"lr\": 0.01, \n",
            "            \"betas\": [0.9, 0.999], \n",
            "            \"eps\": 1e-08, \n",
            "            \"weight_decay\": 0.0\n",
            "        }\n",
            "    }, \n",
            "    \"scheduler\": {\n",
            "        \"type\": \"WarmupLR\", \n",
            "        \"params\": {\n",
            "            \"warmup_min_lr\": 0, \n",
            "            \"warmup_max_lr\": 0.01, \n",
            "            \"warmup_num_steps\": 0\n",
            "        }\n",
            "    }, \n",
            "    \"zero_optimization\": {\n",
            "        \"stage\": 2, \n",
            "        \"offload_optimizer\": {\n",
            "            \"device\": \"cpu\", \n",
            "            \"pin_memory\": true\n",
            "        }, \n",
            "        \"offload_param\": {\n",
            "            \"device\": \"cpu\", \n",
            "            \"pin_memory\": true\n",
            "        }, \n",
            "        \"allgather_partitions\": true, \n",
            "        \"allgather_bucket_size\": 2.000000e+08, \n",
            "        \"overlap_comm\": true, \n",
            "        \"reduce_scatter\": true, \n",
            "        \"reduce_bucket_size\": 2.000000e+08, \n",
            "        \"contiguous_gradients\": true\n",
            "    }, \n",
            "    \"gradient_accumulation_steps\": 3, \n",
            "    \"gradient_clipping\": 1.0, \n",
            "    \"steps_per_print\": 2.000000e+03, \n",
            "    \"train_batch_size\": 192, \n",
            "    \"train_micro_batch_size_per_gpu\": 64, \n",
            "    \"wall_clock_breakdown\": false\n",
            "}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running training *****\n",
            "  Num examples = 15290\n",
            "  Num Epochs = 25\n",
            "  Instantaneous batch size per device = 64\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 192\n",
            "  Gradient Accumulation steps = 3\n",
            "  Total optimization steps = 1975\n",
            "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using /root/.cache/torch_extensions/py37_cu113 as PyTorch extensions root...\n",
            "No modifications detected for re-loaded extension module utils, skipping build step...\n",
            "Loading extension module utils...\n",
            "Time to load utils op: 0.0035567283630371094 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-263240bbee7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-2-b2fb80820e46>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'google/mt5-base'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 485\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfine_tune_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m \u001b[0;31m############################################################################## End Full fine-tune setup######################################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-b2fb80820e46>\u001b[0m in \u001b[0;36mfine_tune_model\u001b[0;34m(model_name)\u001b[0m\n\u001b[1;32m    472\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[0;31m## Finally fine-tune the model and save it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 474\u001b[0;31m     \u001b[0mtrain_and_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath_model_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    475\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m     \u001b[0mtestset_evaluation_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_test_set\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mpath_model_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_articles_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_rdf_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-b2fb80820e46>\u001b[0m in \u001b[0;36mtrain_and_save\u001b[0;34m(trainer, path_model_name)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_and_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath_model_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"/content/drive/MyDrive/MscThesis/Models/{path_model_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1502\u001b[0m             \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m             \u001b[0mtrial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1504\u001b[0;31m             \u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1505\u001b[0m         )\n\u001b[1;32m   1506\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/accelerate/utils/memory.py\u001b[0m in \u001b[0;36mdecorator\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No executable batch size found, reached zero.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshould_reduce_batch_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1740\u001b[0m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1742\u001b[0;31m                     \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1743\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1744\u001b[0m                 if (\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2500\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepspeed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2501\u001b[0m             \u001b[0;31m# loss gets scaled under gradient_accumulation_steps in deepspeed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2502\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepspeed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2503\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2504\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/deepspeed/utils/nvtx.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mwrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnvtx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__qualname__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/deepspeed/runtime/engine.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, loss, allreduce_gradients, release_loss, retain_graph, scale_wrt_gas)\u001b[0m\n\u001b[1;32m   1804\u001b[0m             self.optimizer.is_gradient_accumulation_boundary = self.is_gradient_accumulation_boundary(\n\u001b[1;32m   1805\u001b[0m             )\n\u001b[0;32m-> 1806\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1807\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamp_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1808\u001b[0m             \u001b[0;31m# AMP requires delaying unscale when inside gradient accumulation boundaries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/deepspeed/runtime/zero/stage_1_and_2.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, loss, retain_graph)\u001b[0m\n\u001b[1;32m   2009\u001b[0m             \u001b[0mscaled_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2010\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2011\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_scaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2013\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcheck_overflow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartition_gradients\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/deepspeed/runtime/fp16/loss_scaler.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, loss, retain_graph)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mscaled_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_scale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mscaled_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    394\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 396\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    173\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m def grad(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/function.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    251\u001b[0m                                \"of them.\")\n\u001b[1;32m    252\u001b[0m         \u001b[0muser_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvjp_fn\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mvjp_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mFunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvjp\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mbackward_fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0muser_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_jvp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/checkpoint.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(ctx, *args)\u001b[0m\n\u001b[1;32m    144\u001b[0m                 \u001b[0;34m\"none of output has requires_grad=True,\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m                 \" this checkpoint() is not necessary\")\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs_with_grad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs_with_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m         grads = tuple(inp.grad if isinstance(inp, torch.Tensor) else None\n\u001b[1;32m    148\u001b[0m                       for inp in detached_inputs)\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    173\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m def grad(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/deepspeed/runtime/zero/stage_1_and_2.py\u001b[0m in \u001b[0;36mreduce_partition_and_remove_grads\u001b[0;34m(*notneeded)\u001b[0m\n\u001b[1;32m    841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m                         \u001b[0;32mdef\u001b[0m \u001b[0mreduce_partition_and_remove_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnotneeded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 843\u001b[0;31m                             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_ready_partitions_and_remove_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    844\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    845\u001b[0m                         \u001b[0mgrad_acc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduce_partition_and_remove_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/deepspeed/runtime/zero/stage_1_and_2.py\u001b[0m in \u001b[0;36mreduce_ready_partitions_and_remove_grads\u001b[0;34m(self, param, i)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreduce_ready_partitions_and_remove_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartition_gradients\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_gradient_accumulation_boundary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_independent_p_g_buckets_and_remove_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mzero_reduced_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartition_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/deepspeed/runtime/zero/stage_1_and_2.py\u001b[0m in \u001b[0;36mreduce_independent_p_g_buckets_and_remove_grads\u001b[0;34m(self, param, i)\u001b[0m\n\u001b[1;32m    868\u001b[0m             self.report_ipg_memory_usage(\"In ipg_remove_grads before reduce_ipg_grads\",\n\u001b[1;32m    869\u001b[0m                                          param.numel())\n\u001b[0;32m--> 870\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_ipg_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    871\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous_gradients\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moverlap_comm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m                 \u001b[0;31m# Swap ipg_index between 0 and 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/deepspeed/runtime/zero/stage_1_and_2.py\u001b[0m in \u001b[0;36mreduce_ipg_grads\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1267\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextra_large_param_to_reduce\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1268\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1269\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maverage_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mipg_buffer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mipg_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1270\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m             self.buffered_reduce_fallback(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/deepspeed/runtime/zero/stage_1_and_2.py\u001b[0m in \u001b[0;36maverage_tensor\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m    938\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0maverage_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    939\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moverlap_comm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 940\u001b[0;31m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msynchronize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    941\u001b[0m             \u001b[0mstream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction_stream\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    942\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36msynchronize\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    494\u001b[0m     \u001b[0m_lazy_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 496\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_synchronize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    497\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: an illegal memory access was encountered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def add_parent_results_to_json(path_model_name):\n",
        "  with open(f'/content/drive/MyDrive/MscThesis/Logging_Results/google_mt5-base_logResults.json', 'r') as result_file:     \n",
        "    overal_results = json.load(result_file)   \n",
        "    #print(f'overal results:   {overal_results} \\n\\n\\n')\n",
        "\n",
        "\n",
        "  with open(f'/content/drive/MyDrive/MscThesis/Logging_Results/logResultsColabParent.json', 'r') as add_to_file:\n",
        "    parent_results = json.load(add_to_file)\n",
        "    #print(f'parent_results :   {parent_results} \\n\\n\\n')\n",
        "\n",
        "  overal_results.update(parent_results)\n",
        "  #print(f'overal_results v2 :   {overal_results} \\n\\n\\n')\n",
        "\n",
        "  with open(f'/content/drive/MyDrive/MscThesis/Logging_Results/google_mt5-base_logResults.json', 'w') as f: \n",
        "    json.dump(overal_results, f)\n",
        "\n"
      ],
      "metadata": {
        "id": "1RpJOjNLL2wN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "add_parent_results_to_json('google_mt5-base')"
      ],
      "metadata": {
        "id": "w4oeoqNQYotD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d956bcf3-6fa7-4d58-964f-0c3b81edb11b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "overal results:   {'blue_output': {'bleu': 0.0, 'precisions': [0.027972027972027972, 0.0, 0.0, 0.0], 'brevity_penalty': 0.7249313475262888, 'length_ratio': 0.7566137566137566, 'translation_length': 143, 'reference_length': 189}, 'rouge_output': {'rouge1': 0.05958574717765894, 'rouge2': 0.017592592592592594, 'rougeL': 0.04889025054466231, 'rougeLsum': 0.050389619231531}, 'meteor_results': {'meteor': 0.02181515786989303}, 'perp_output': {'perplexities': [770.9533081054688, 104.64604949951172, 587.6917724609375, 1762.919677734375, 171.27622985839844, 566.5110473632812, 171.27622985839844, 11.1726655960083, 246.65447998046875, 994.7477416992188, 513.990478515625, 336.4140625], 'mean_perplexity': 519.854478597641}, 'bertscore_output': {'precision': [0.7894889712333679, 0.7600829005241394, 0.7710351943969727, 0.7584834098815918, 0.7792298197746277, 0.7697867155075073, 0.7790033221244812, 0.7148011922836304, 0.8261810541152954, 0.7529188394546509, 0.7539244890213013, 0.7357916831970215], 'recall': [0.8532602787017822, 0.8073956370353699, 0.7882024049758911, 0.772606611251831, 0.7945789098739624, 0.8146900534629822, 0.8192657828330994, 0.7708232402801514, 0.8088906407356262, 0.7612584233283997, 0.7367560863494873, 0.7657870650291443], 'f1': [0.8201368451118469, 0.7830252647399902, 0.7795243263244629, 0.7654799222946167, 0.7868295311927795, 0.7916021943092346, 0.7986274361610413, 0.7417559623718262, 0.8174444437026978, 0.7570656538009644, 0.7452414035797119, 0.7504897713661194], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.23.1)'}, 'bart_scores_output': [-5.785937786102295, -6.004884719848633, -6.925654411315918, -8.229718208312988, -7.138926982879639, -6.087158203125, -8.622739791870117, -6.949731826782227, -5.302716255187988, -10.264586448669434, -8.865450859069824, -6.365853786468506]} \n",
            "\n",
            "\n",
            "\n",
            "parent_results :   {'Parent_Precison': 3.377914485479132e-05, 'Parent_recall': 9.999999999999997e-06, 'Parent_f_score': 4.649970587059579e-06, 'Parent_all_f': [1.882715253510714e-05, 0.0, 1.8447227990364592e-05, 0.0, 0.0, 0.0, 0.0, 0.0, 1.852526651924322e-05, 0.0, 0.0, 0.0]} \n",
            "\n",
            "\n",
            "\n",
            "overal_results v2 :   {'blue_output': {'bleu': 0.0, 'precisions': [0.027972027972027972, 0.0, 0.0, 0.0], 'brevity_penalty': 0.7249313475262888, 'length_ratio': 0.7566137566137566, 'translation_length': 143, 'reference_length': 189}, 'rouge_output': {'rouge1': 0.05958574717765894, 'rouge2': 0.017592592592592594, 'rougeL': 0.04889025054466231, 'rougeLsum': 0.050389619231531}, 'meteor_results': {'meteor': 0.02181515786989303}, 'perp_output': {'perplexities': [770.9533081054688, 104.64604949951172, 587.6917724609375, 1762.919677734375, 171.27622985839844, 566.5110473632812, 171.27622985839844, 11.1726655960083, 246.65447998046875, 994.7477416992188, 513.990478515625, 336.4140625], 'mean_perplexity': 519.854478597641}, 'bertscore_output': {'precision': [0.7894889712333679, 0.7600829005241394, 0.7710351943969727, 0.7584834098815918, 0.7792298197746277, 0.7697867155075073, 0.7790033221244812, 0.7148011922836304, 0.8261810541152954, 0.7529188394546509, 0.7539244890213013, 0.7357916831970215], 'recall': [0.8532602787017822, 0.8073956370353699, 0.7882024049758911, 0.772606611251831, 0.7945789098739624, 0.8146900534629822, 0.8192657828330994, 0.7708232402801514, 0.8088906407356262, 0.7612584233283997, 0.7367560863494873, 0.7657870650291443], 'f1': [0.8201368451118469, 0.7830252647399902, 0.7795243263244629, 0.7654799222946167, 0.7868295311927795, 0.7916021943092346, 0.7986274361610413, 0.7417559623718262, 0.8174444437026978, 0.7570656538009644, 0.7452414035797119, 0.7504897713661194], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.23.1)'}, 'bart_scores_output': [-5.785937786102295, -6.004884719848633, -6.925654411315918, -8.229718208312988, -7.138926982879639, -6.087158203125, -8.622739791870117, -6.949731826782227, -5.302716255187988, -10.264586448669434, -8.865450859069824, -6.365853786468506], 'Parent_Precison': 3.377914485479132e-05, 'Parent_recall': 9.999999999999997e-06, 'Parent_f_score': 4.649970587059579e-06, 'Parent_all_f': [1.882715253510714e-05, 0.0, 1.8447227990364592e-05, 0.0, 0.0, 0.0, 0.0, 0.0, 1.852526651924322e-05, 0.0, 0.0, 0.0]} \n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prefix discussion\n",
        "\n",
        "https://github.com/huggingface/transformers/issues/13249 states prefixes for mT5 are unnecessary, but not harmful. Maybe necessary for T5 and T5-dutch?"
      ],
      "metadata": {
        "id": "JLeFpmdH0-Gb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "{\n",
        "    \"fp16\": {\n",
        "        \"enabled\": \"auto\",\n",
        "    },\n",
        "\n",
        "    \"amp\": {\n",
        "      \"enabled\": \"auto\",\n",
        "      \"opt_level\": \"auto\"\n",
        "    },\n",
        "    \n",
        "    \"optimizer\": {\n",
        "        \"type\": \"AdamW\",\n",
        "        \"params\": {\n",
        "            \"lr\": \"auto\",\n",
        "            \"betas\": \"auto\",\n",
        "            \"eps\": \"auto\",\n",
        "            \"weight_decay\": \"auto\"\n",
        "        }\n",
        "    },\n",
        "\n",
        "    \"scheduler\": {\n",
        "        \"type\": \"WarmupLR\",\n",
        "        \"params\": {\n",
        "            \"warmup_min_lr\": \"auto\",\n",
        "            \"warmup_max_lr\": \"auto\",\n",
        "            \"warmup_num_steps\": \"auto\"\n",
        "        }\n",
        "    },\n",
        "\n",
        "    \"zero_optimization\": {\n",
        "        \"stage\": 3,\n",
        "        \"offload_optimizer\": {\n",
        "            \"device\": \"cpu\"\n",
        "        },\n",
        "        \"offload_param\": {\n",
        "            \"device\": \"cpu\"\n",
        "        },\n",
        "        \"overlap_comm\": true,\n",
        "        \"contiguous_gradients\": true,\n",
        "        \"sub_group_size\": 1e6,\n",
        "        \"reduce_bucket_size\": \"auto\",\n",
        "        \"stage3_prefetch_bucket_size\": \"auto\",\n",
        "        \"stage3_param_persistence_threshold\": \"auto\",\n",
        "        \"stage3_max_live_parameters\": 1e6,\n",
        "        \"stage3_max_reuse_distance\": 1e6,\n",
        "        \"stage3_gather_16bit_weights_on_model_save\": true\n",
        "    },\n",
        "\n",
        "    \"gradient_accumulation_steps\": \"auto\",\n",
        "    \"gradient_clipping\": \"auto\",\n",
        "    \"steps_per_print\": 2000,\n",
        "    \"train_batch_size\": \"auto\",\n",
        "    \"train_micro_batch_size_per_gpu\": \"auto\",\n",
        "    \"wall_clock_breakdown\": false\n",
        "}"
      ],
      "metadata": {
        "id": "AcCLRG-81Gx3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}