{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Anaconda\\envs\\CudaSupEnv\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os.path\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import torch \n",
    "import nltk\n",
    "\n",
    "from transformers import AutoTokenizer, T5ForConditionalGeneration, Seq2SeqTrainingArguments, Seq2SeqTrainer, pipeline, AutoModelForSeq2SeqLM\n",
    "import datasets\n",
    "import evaluate\n",
    "\n",
    "import Evaluation_Code.Parent as parent ## code for PARENT metric\n",
    "import Evaluation_Code.Bartscore as bartscore ## code for Bartscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_cuda_compatability():\n",
    "    print(f'Torch version: {torch.__version__}')\n",
    "    print(f'Cuda version: {torch.version.cuda}')\n",
    "    print(f'Cudnn version: {torch.backends.cudnn.version()}')\n",
    "    print(f'Is cuda available: {torch.cuda.is_available()}')\n",
    "    print(f'Number of cuda devices: {torch.cuda.device_count()}')\n",
    "    print(f'Current default device: {torch.cuda.current_device()}')\n",
    "    print(f'First cuda device: {torch.cuda.device(0)}')\n",
    "    print(f'Name of the first cuda device: {torch.cuda.get_device_name(0)}\\n\\n')\n",
    "\n",
    "\n",
    "def preprocess_model(model_name):\n",
    "    \"\"\"\n",
    "    Setup the model and tokenizer for preprocessing. This will be a pre-trained model collected from huggingface\n",
    "    \"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name) \n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "    \n",
    "    print('LOGGING: preprocess_model DONE \\n')\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def load_CACAPO_data():\n",
    "    \"\"\"\n",
    "    This function retrieves the csv files and creates a dataset\n",
    "    \"\"\"\n",
    "    print('LOGGING: load_CACAPO_data DONE \\n')\n",
    "\n",
    "    return datasets.load_dataset(\"../Data/Cleaned_data/\", data_files={\"train\": \"Train.csv\", \"dev\": \"Dev.csv\", \"test\": \"Test.csv\"})\n",
    "\n",
    "\n",
    "def preprocess_data(data):\n",
    "    \"\"\"\n",
    "    Tokenize the data\n",
    "    \"\"\"\n",
    "    max_length = 256\n",
    "    RDFs = data[\"input\"]\n",
    "    texts = data[\"output\"]\n",
    "\n",
    "    model_inputs = tokenizer(RDFs, truncation=True, padding='max_length', return_tensors='pt',  max_length=max_length)\n",
    "\n",
    "    # specially for seq2seq tokenizer, \"Temporarily sets the tokenizer for encoding the targets. Useful for tokenizer associated to sequence-to-sequence models that need a slightly different processing for the labels.\"\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        target_texts = tokenizer(texts, padding='max_length', truncation=True, return_tensors='pt',  max_length=max_length).input_ids\n",
    "    \n",
    "    model_inputs[\"labels\"] = target_texts\n",
    "    \n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "def transform_datasets(dataset):\n",
    "    \"\"\"\n",
    "    After loading in and creating the initial dataset, the text data is transformed, by tokenizing the input and output texts. The initial dataset is also split into train,val,test for training use. \n",
    "    \"\"\"\n",
    "\n",
    "    ## Create smaller versions of the dataset\n",
    "    small_train = dataset[\"train\"].shard(num_shards = 256, index = 0)\n",
    "    small_val = dataset[\"dev\"].shard(num_shards = 256, index = 0)\n",
    "    small_test = dataset[\"test\"].shard(num_shards = 256, index = 0)\n",
    "\n",
    "    ## Process the data in batches\n",
    "    small_train = small_train.map(preprocess_data, batched=True, remove_columns=dataset[\"train\"].column_names)  \n",
    "    small_val = small_val.map(preprocess_data, batched=True, remove_columns=dataset[\"dev\"].column_names)\n",
    "    small_test = small_test.map(preprocess_data, batched=True, remove_columns=dataset[\"test\"].column_names)\n",
    "\n",
    "    # transform the datasets into torch sensors, as the model will expect this format \n",
    "    small_train.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "    small_val.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "    small_test.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "    print('LOGGING: transform_datasets DONE \\n')\n",
    "\n",
    "    return small_train, small_val, small_test\n",
    "\n",
    "\n",
    "def load_eval_metrics():\n",
    "    \"\"\"\n",
    "    Loads in all metrics that will be used later on during evaluation. This is seperated to not load in the metrics a dozen of times during training.\n",
    "    \"\"\"\n",
    "    bleu = datasets.load_metric(\"bleu\")\n",
    "    rouge = evaluate.load('rouge')\n",
    "    meteor = evaluate.load('meteor')\n",
    "    perplexity = evaluate.load(\"perplexity\", module_type=\"metric\")\n",
    "    bertscore = evaluate.load(\"bertscore\")\n",
    "    bart_scorer = bartscore.BARTScorer(device='cuda:0', checkpoint='facebook/bart-large-cnn')\n",
    "\n",
    "    \n",
    "    print('LOGGING: load_eval_metrics DONE \\n')\n",
    "\n",
    "    return bleu, rouge, meteor, perplexity, bertscore, bart_scorer\n",
    "\n",
    "def postprocess_text(preds, labels):\n",
    "    \"\"\"\n",
    "    Supplementary Method called in decode_text.\n",
    "    \n",
    "    Returns list of split decoded labels and predictions for evaluation\n",
    "    \"\"\"\n",
    "    preds = [pred.split() for pred in preds]\n",
    "    labels = [[label.split()] for label in labels]\n",
    "    return preds, labels\n",
    "\n",
    "def decode_text(predictions, labels):\n",
    "    \"\"\"\n",
    "    Supplementary Method called in compute_metrics.\n",
    "    \n",
    "    Returns decoded labels and predictions for evaluation\n",
    "    \"\"\"\n",
    "    if isinstance(predictions, tuple):\n",
    "            predictions = predictions[0]\n",
    "        \n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    return decoded_preds, decoded_labels\n",
    "\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    \"\"\"\"\n",
    "    Metrics to be evaluated during training and validation\n",
    "    Metrics used: BLEU, ROUGE, METEOR, BARTScore, PARENT\n",
    "    \"\"\"\n",
    "    # decode the predictions and labels for eval\n",
    "    predictions, labels = pred\n",
    "    decoded_preds, decoded_labels = decode_text(predictions, labels)\n",
    "\n",
    "    #post_process for BLEU\n",
    "    blue_preds, blue_labels = postprocess_text(decoded_preds,  decoded_labels)\n",
    "\n",
    "    # setup metrics for use\n",
    "    bleu, rouge, meteor, perplexity, bertscore, bart_scorer = load_eval_metrics()\n",
    "\n",
    "    # Calculate the metrics\n",
    "    print(f'\\n LOGGING: Calculating Blue')\n",
    "    bleu_output = bleu.compute(predictions=blue_preds, references=blue_labels)\n",
    "    print(f'\\n LOGGING: Calculating Rouge')\n",
    "    rouge_output = rouge.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    print(f'\\n LOGGING: Calculating Meteor')\n",
    "    meteor_output = meteor.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    print(f'\\n LOGGING: Calculating Perplexity')\n",
    "    perp_output = perplexity.compute(predictions=decoded_preds, model_id='gpt2')\n",
    "    print(f'\\n LOGGING: Calculating Bertscore')\n",
    "    bertscore_output = bertscore.compute(predictions=decoded_preds, references=decoded_labels, lang=\"en\")\n",
    "    print(f'\\n LOGGING: Calculating Bartscore')\n",
    "    bart_scores_output = bart_scorer.score(srcs=decoded_preds, tgts=decoded_labels, batch_size=8) \n",
    "    ### Need to add parent \n",
    "\n",
    "    ## Huggingsface trainer requires a dict if multiple metrics are used\n",
    "    return {\"blue_output\": bleu_output, \"rouge_output\": rouge_output, \"meteor_results\": meteor_output, \n",
    "            \"perp_output\": perp_output, \"bertscore_output\": bertscore_output, \"bart_scores_output\": bart_scores_output}\n",
    "\n",
    "\n",
    "def set_training_args(model_name, learning_rate, num_train_epochs, evaluation_strategy, generation_num_beams, gradient_accumulation_steps, per_device_train_batch_size, per_device_eval_batch_size):\n",
    "    \"\"\"\n",
    "    Setup the training arguments that will be used during training.\n",
    "    \"\"\"\n",
    "    #model_name = \"t5-fp16-test\"\n",
    "    model_dir = f\"../Results/{model_name}\"\n",
    "\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "                output_dir=model_dir,\n",
    "                learning_rate=learning_rate,\n",
    "                do_eval=True, # will be set to true if evaluation strategy is set\n",
    "                do_predict=True, #Whether to run predictions on the test set or not.\n",
    "                num_train_epochs=num_train_epochs,\n",
    "                evaluation_strategy= evaluation_strategy, \n",
    "                #eval_steps= 100, # Number of update steps between two evaluations if evaluation_strategy=\"steps\". Will default to the same value as logging_steps if not set.\n",
    "                save_steps=500, # Number of updates steps before two checkpoint saves if save_strategy=\"steps\".\n",
    "                #max_steps=10, # the total number of training steps to perform\n",
    "                save_total_limit= 10, # the maximum number of models to keep before deleting the oldest one\n",
    "                predict_with_generate=True, # Whether to use generate to calculate generative metrics (ROUGE, BLEU).\n",
    "                generation_num_beams=generation_num_beams,  #The num_beams to use on each evaluation loop when predict_with_generate=True. Will default to the num_beams value of the model configuration\n",
    "                gradient_checkpointing=True, #\n",
    "                gradient_accumulation_steps=gradient_accumulation_steps, #Number of updates steps to accumulate the gradients for, before performing a backward/update pass\n",
    "                per_device_train_batch_size=per_device_train_batch_size, #The batch size per GPU/TPU core/CPU for training.\n",
    "                per_device_eval_batch_size=per_device_eval_batch_size, #The batch size per GPU/TPU core/CPU for evaluation.\n",
    "                optim=\"adafactor\", #The optimizer to use: adamw_hf, adamw_torch, adamw_apex_fused, or adafactor.\n",
    "                #report_to=\"tensorboard\",\n",
    "                fp16=True\n",
    "    )\n",
    "\n",
    "    print('LOGGING: set_training_args DONE \\n')\n",
    "\n",
    "    return training_args\n",
    "\n",
    "\n",
    "def get_clean_model(model_name):\n",
    "    \"\"\"\n",
    "    Simple function to ensure that a new model is used for finetuning\n",
    "    \"\"\"\n",
    "    return AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "def set_trainer(model_name, training_args, train_ds, val_ds, tokenizer):\n",
    "    \"\"\"\n",
    "    Initializes a trainer\n",
    "    Takes in: Model name, training arguments, training dataset, validation dataset, and tokenizer\n",
    "    Returns: Trainer instance\n",
    "    \"\"\"\n",
    "\n",
    "    #metrics = load_eval_metrics()\n",
    "    clean_model = get_clean_model(model_name)\n",
    "    trainer = Seq2SeqTrainer(\n",
    "                model=clean_model,\n",
    "                args=training_args,\n",
    "                train_dataset=train_ds,\n",
    "                eval_dataset=val_ds,\n",
    "                compute_metrics=compute_metrics,\n",
    "                tokenizer=tokenizer\n",
    "                )\n",
    "    \n",
    "    print('LOGGING: set_trainer DONE \\n')\n",
    "\n",
    "    return trainer\n",
    "\n",
    "def train_and_save(trainer, model_name):\n",
    "    trainer.train()\n",
    "    trainer.save_model(f\"../Models/{model_name}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune_model(model_name):\n",
    "    # ensure cuda compatability\n",
    "    ensure_cuda_compatability()\n",
    "\n",
    "    # I instantiate the tokenizer as a global variable, as the .map function in transform_datasets was not working properly. \n",
    "    # This should not be an issue, as the tokenizer remains consistent during training and evaluation.\n",
    "    global tokenizer\n",
    "    \n",
    "    # retrieve model and tokenizer from huggingface to prepare dataset\n",
    "    model, tokenizer = preprocess_model(model_name)\n",
    "    \n",
    "    #retrieve the unprocessed data from the csv files\n",
    "    entire_dataset = load_CACAPO_data()\n",
    "    \n",
    "    # process the dataset and split it into its natural train, val, test split\n",
    "    train_ds, val_ds, test_ds = transform_datasets(entire_dataset)\n",
    "\n",
    "    # setup the training arguments \n",
    "    # parameters = (model_name, learning_rate, num_train_epochs, evaluation_strategy, generation_num_beams, gradient_accumulation_steps, per_device_train_batch_size, per_device_eval_batch_size)\n",
    "    training_args = set_training_args(model_name, 0.001, 1, 'epoch', 10, 4, 8, 8)\n",
    "\n",
    "    # create a trainer instance \n",
    "    trainer = set_trainer(model_name, training_args, train_ds, val_ds, tokenizer)\n",
    "\n",
    "    # Finally fine-tune the model and save it\n",
    "    train_and_save(trainer, model_name)\n",
    "\n",
    "def main():\n",
    "    model_name = 't5-base'\n",
    "    fine_tune_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 1.12.1\n",
      "Cuda version: 11.3\n",
      "Cudnn version: 8302\n",
      "Is cuda available: True\n",
      "Number of cuda devices: 1\n",
      "Current default device: 0\n",
      "First cuda device: <torch.cuda.device object at 0x000002CDE0244A88>\n",
      "Name of the first cuda device: NVIDIA GeForce GTX 1070\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file config.json from cache at C:\\Users\\Simon/.cache\\huggingface\\hub\\models--t5-base\\snapshots\\23aa4f41cb7c08d4b05c8f327b22bfa0eb8c7ad9\\config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-base\",\n",
      "  \"architectures\": [\n",
      "    \"T5WithLMHeadModel\"\n",
      "  ],\n",
      "  \"d_ff\": 3072,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading file spiece.model from cache at C:\\Users\\Simon/.cache\\huggingface\\hub\\models--t5-base\\snapshots\\23aa4f41cb7c08d4b05c8f327b22bfa0eb8c7ad9\\spiece.model\n",
      "loading file tokenizer.json from cache at C:\\Users\\Simon/.cache\\huggingface\\hub\\models--t5-base\\snapshots\\23aa4f41cb7c08d4b05c8f327b22bfa0eb8c7ad9\\tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at None\n",
      "loading configuration file config.json from cache at C:\\Users\\Simon/.cache\\huggingface\\hub\\models--t5-base\\snapshots\\23aa4f41cb7c08d4b05c8f327b22bfa0eb8c7ad9\\config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-base\",\n",
      "  \"architectures\": [\n",
      "    \"T5WithLMHeadModel\"\n",
      "  ],\n",
      "  \"d_ff\": 3072,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "e:\\Anaconda\\envs\\CudaSupEnv\\lib\\site-packages\\transformers\\models\\t5\\tokenization_t5_fast.py:166: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  FutureWarning,\n",
      "loading configuration file config.json from cache at C:\\Users\\Simon/.cache\\huggingface\\hub\\models--t5-base\\snapshots\\23aa4f41cb7c08d4b05c8f327b22bfa0eb8c7ad9\\config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-base\",\n",
      "  \"architectures\": [\n",
      "    \"T5WithLMHeadModel\"\n",
      "  ],\n",
      "  \"d_ff\": 3072,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\Simon/.cache\\huggingface\\hub\\models--t5-base\\snapshots\\23aa4f41cb7c08d4b05c8f327b22bfa0eb8c7ad9\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at t5-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
      "Using custom data configuration Cleaned_data-bcb014efcf526ad6\n",
      "Found cached dataset csv (C:/Users/Simon/.cache/huggingface/datasets/csv/Cleaned_data-bcb014efcf526ad6/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOGGING: preprocess_model DONE \n",
      "\n",
      "LOGGING: load_CACAPO_data DONE \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 3/3 [00:00<00:00, 88.31it/s]\n",
      "Loading cached processed dataset at C:/Users/Simon/.cache/huggingface/datasets/csv/Cleaned_data-bcb014efcf526ad6/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\\cache-1c80317fa3b1799d.arrow\n",
      "Loading cached processed dataset at C:/Users/Simon/.cache/huggingface/datasets/csv/Cleaned_data-bcb014efcf526ad6/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\\cache-bdd640fb06671ad1.arrow\n",
      "Loading cached processed dataset at C:/Users/Simon/.cache/huggingface/datasets/csv/Cleaned_data-bcb014efcf526ad6/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\\cache-3eb13b9046685257.arrow\n",
      "PyTorch: setting up devices\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOGGING: transform_datasets DONE \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOGGING: set_training_args DONE \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\Simon/.cache\\huggingface\\hub\\models--t5-base\\snapshots\\23aa4f41cb7c08d4b05c8f327b22bfa0eb8c7ad9\\config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-base\",\n",
      "  \"architectures\": [\n",
      "    \"T5WithLMHeadModel\"\n",
      "  ],\n",
      "  \"d_ff\": 3072,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\Simon/.cache\\huggingface\\hub\\models--t5-base\\snapshots\\23aa4f41cb7c08d4b05c8f327b22bfa0eb8c7ad9\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at t5-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
      "Using cuda_amp half precision backend\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOGGING: set_trainer DONE \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bos_token, but it is not set yet.\n",
      "Using cls_token, but it is not set yet.\n",
      "Using mask_token, but it is not set yet.\n",
      "Using sep_token, but it is not set yet.\n",
      "***** Running training *****\n",
      "  Num examples = 2867\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 89\n",
      "\n",
      "\u001b[AYou're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('CudaSupEnv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8ce04b5a4dcd593beec21f85756cfa75ba50da4e44d5dd0d068d93735596035b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
