{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Anaconda\\envs\\CudaSupEnv\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os.path\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import torch \n",
    "import nltk\n",
    "\n",
    "from transformers import AutoTokenizer, T5ForConditionalGeneration, Seq2SeqTrainingArguments, Seq2SeqTrainer, pipeline, AutoModelForSeq2SeqLM\n",
    "import datasets\n",
    "import evaluate\n",
    "\n",
    "#import Evaluation_Code.Parent as parent ## code for PARENT metric\n",
    "import Evaluation_Code.Bartscore as bartscore ## code for Bartscore\n",
    "\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_cuda_compatability():\n",
    "    print(f'Torch version: {torch.__version__}')\n",
    "    print(f'Cuda version: {torch.version.cuda}')\n",
    "    print(f'Cudnn version: {torch.backends.cudnn.version()}')\n",
    "    print(f'Is cuda available: {torch.cuda.is_available()}')\n",
    "    print(f'Number of cuda devices: {torch.cuda.device_count()}')\n",
    "    print(f'Current default device: {torch.cuda.current_device()}')\n",
    "    print(f'First cuda device: {torch.cuda.device(0)}')\n",
    "    print(f'Name of the first cuda device: {torch.cuda.get_device_name(0)}\\n\\n')\n",
    "\n",
    "\n",
    "def preprocess_model(model_name):\n",
    "    \"\"\"\n",
    "    Setup the model and tokenizer for preprocessing. This will be a pre-trained model collected from huggingface\n",
    "    \"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name) \n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "    \n",
    "    print('LOGGING: preprocess_model DONE \\n')\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def load_CACAPO_data():\n",
    "    \"\"\"\n",
    "    This function retrieves the csv files and creates a dataset\n",
    "    \"\"\"\n",
    "    print('LOGGING: load_CACAPO_data DONE \\n')\n",
    "\n",
    "    return datasets.load_dataset(\"../Data/Cleaned_data/\", data_files={\"train\": \"Train.csv\", \"dev\": \"Dev.csv\", \"test\": \"Test.csv\"})\n",
    "\n",
    "\n",
    "def preprocess_data(data):\n",
    "    \"\"\"\n",
    "    Tokenize the data\n",
    "    \"\"\"\n",
    "    max_length = 256\n",
    "    RDFs = data[\"input\"]\n",
    "    texts = data[\"output\"]\n",
    "\n",
    "    model_inputs = tokenizer(RDFs, truncation=True, padding='max_length', return_tensors='pt',  max_length=max_length)\n",
    "\n",
    "    # specially for seq2seq tokenizer, \"Temporarily sets the tokenizer for encoding the targets. Useful for tokenizer associated to sequence-to-sequence models that need a slightly different processing for the labels.\"\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        target_texts = tokenizer(texts, padding='max_length', truncation=True, return_tensors='pt',  max_length=max_length).input_ids\n",
    "    \n",
    "    model_inputs[\"labels\"] = target_texts\n",
    "    \n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "\n",
    "def transform_datasets(dataset):\n",
    "    \"\"\"\n",
    "    After loading in and creating the initial dataset, the text data is transformed, by tokenizing the input and output texts. The initial dataset is also split into train,val,test for training use. \n",
    "    NOTE That the test set will not be preprocessed here yet, this will be done in a different function\n",
    "    \"\"\"\n",
    "\n",
    "    ## Create smaller versions of the dataset\n",
    "    small_train = dataset[\"train\"].shard(num_shards = 512, index = 0)\n",
    "    small_val = dataset[\"dev\"].shard(num_shards = 512, index = 0)\n",
    "    small_test = dataset[\"test\"].shard(num_shards = 512, index = 0)\n",
    "\n",
    "    ## Process the data in batches\n",
    "    small_train = small_train.map(preprocess_data, batched=True, remove_columns=dataset[\"train\"].column_names)  \n",
    "    small_val = small_val.map(preprocess_data, batched=True, remove_columns=dataset[\"dev\"].column_names)\n",
    "    #small_test = small_test.map(preprocess_data, batched=True, remove_columns=dataset[\"test\"].column_names)\n",
    "\n",
    "    # transform the datasets into torch sensors, as the model will expect this format \n",
    "    small_train.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "    small_val.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "    #small_test.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "    print('LOGGING: transform_datasets DONE \\n')\n",
    "\n",
    "    return small_train, small_val, small_test\n",
    "\n",
    "\n",
    "\n",
    "def load_eval_metrics():\n",
    "    \"\"\"\n",
    "    Loads in all metrics that will be used later on during evaluation. This is seperated to not load in the metrics a dozen of times during training.\n",
    "    \"\"\"\n",
    "    bleu = datasets.load_metric(\"bleu\")\n",
    "    rouge = evaluate.load('rouge')\n",
    "    meteor = evaluate.load('meteor')\n",
    "    perplexity = evaluate.load(\"perplexity\", module_type=\"metric\")\n",
    "    bertscore = evaluate.load(\"bertscore\")\n",
    "    bart_scorer = bartscore.BARTScorer(device='cuda:0', checkpoint='facebook/bart-large-cnn')\n",
    "\n",
    "    print('LOGGING: load_eval_metrics DONE \\n')\n",
    "\n",
    "    return bleu, rouge, meteor, perplexity, bertscore, bart_scorer\n",
    "\n",
    "\n",
    "\n",
    "def postprocess_text(preds, labels):\n",
    "    \"\"\"\n",
    "    Supplementary Method called in decode_text.\n",
    "    \n",
    "    Returns list of split decoded labels and predictions for evaluation\n",
    "    \"\"\"\n",
    "    preds = [pred.split() for pred in preds]\n",
    "    labels = [[label.split()] for label in labels]\n",
    "    return preds, labels\n",
    "\n",
    "def decode_text(predictions, labels):\n",
    "    \"\"\"\n",
    "    Supplementary Method called in compute_metrics.\n",
    "    \n",
    "    Returns decoded labels and predictions for evaluation\n",
    "    \"\"\"\n",
    "    if isinstance(predictions, tuple):\n",
    "            predictions = predictions[0]\n",
    "        \n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    return decoded_preds, decoded_labels\n",
    "\n",
    "\n",
    "def evaluate_texts(decoded_preds, decoded_labels):\n",
    "    \"\"\"\n",
    "    Calculates metrics given a list of decoded predictions and decoded labels\n",
    "    \"\"\"\n",
    "    #post_process for BLEU\n",
    "    blue_preds, blue_labels = postprocess_text(decoded_preds,  decoded_labels)\n",
    "\n",
    "    # setup metrics for use\n",
    "    bleu, rouge, meteor, perplexity, bertscore, bart_scorer = load_eval_metrics()\n",
    "\n",
    "    # Calculate the metrics\n",
    "    print(f'\\n LOGGING: Calculating Blue')\n",
    "    bleu_output = bleu.compute(predictions=blue_preds, references=blue_labels)\n",
    "    print(f'\\n LOGGING: Calculating Rouge')\n",
    "    rouge_output = rouge.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    print(f'\\n LOGGING: Calculating Meteor')\n",
    "    meteor_output = meteor.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    print(f'\\n LOGGING: Calculating Perplexity')\n",
    "    perp_output = perplexity.compute(predictions=decoded_preds, model_id='gpt2')\n",
    "    print(f'\\n LOGGING: Calculating Bertscore')\n",
    "    bertscore_output = bertscore.compute(predictions=decoded_preds, references=decoded_labels, lang=\"en\")\n",
    "    print(f'\\n LOGGING: Calculating Bartscore')\n",
    "    bart_scores_output = bart_scorer.score(srcs=decoded_preds, tgts=decoded_labels, batch_size=8) \n",
    "    ### Need to add parent \n",
    "\n",
    "    return bleu_output, rouge_output, meteor_output, perp_output, bertscore_output, bart_scores_output\n",
    "\n",
    "\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    \"\"\"\"\n",
    "    Metrics to be evaluated during training and validation\n",
    "    Metrics used: BLEU, ROUGE, METEOR, Bertscore, BARTScore\n",
    "    \"\"\"\n",
    "    # decode the predictions and labels for eval\n",
    "    predictions, labels = pred\n",
    "    decoded_preds, decoded_labels = decode_text(predictions, labels)\n",
    "\n",
    "    bleu_output, rouge_output, meteor_output, perp_output, bertscore_output, bart_scores_output = evaluate_texts(decoded_preds, decoded_labels)\n",
    "\n",
    "    ## Huggingsface trainer requires a dict if multiple metrics are used\n",
    "    return {\"blue_output\": bleu_output, \"rouge_output\": rouge_output, \"meteor_results\": meteor_output, \n",
    "            \"perp_output\": perp_output, \"bertscore_output\": bertscore_output, \"bart_scores_output\": bart_scores_output}\n",
    "\n",
    "\n",
    "\n",
    "def set_training_args(model_name, learning_rate, num_train_epochs, evaluation_strategy, generation_num_beams, gradient_accumulation_steps, per_device_train_batch_size, per_device_eval_batch_size):\n",
    "    \"\"\"\n",
    "    Setup the training arguments that will be used during training.\n",
    "    \"\"\"\n",
    "    #model_name = \"t5-fp16-test\"\n",
    "    model_dir = f\"../Results/{model_name}\"\n",
    "\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "                output_dir=model_dir,\n",
    "                learning_rate=learning_rate,\n",
    "                do_eval=True, # will be set to true if evaluation strategy is set\n",
    "                do_predict=True, #Whether to run predictions on the test set or not.\n",
    "                num_train_epochs=num_train_epochs,\n",
    "                evaluation_strategy= evaluation_strategy, \n",
    "                #eval_steps= 100, # Number of update steps between two evaluations if evaluation_strategy=\"steps\". Will default to the same value as logging_steps if not set.\n",
    "                save_steps=500, # Number of updates steps before two checkpoint saves if save_strategy=\"steps\".\n",
    "                #max_steps=10, # the total number of training steps to perform\n",
    "                save_total_limit= 10, # the maximum number of models to keep before deleting the oldest one\n",
    "                predict_with_generate=True, # Whether to use generate to calculate generative metrics (ROUGE, BLEU).\n",
    "                generation_num_beams=generation_num_beams,  #The num_beams to use on each evaluation loop when predict_with_generate=True. Will default to the num_beams value of the model configuration\n",
    "                gradient_checkpointing=True, #\n",
    "                gradient_accumulation_steps=gradient_accumulation_steps, #Number of updates steps to accumulate the gradients for, before performing a backward/update pass\n",
    "                per_device_train_batch_size=per_device_train_batch_size, #The batch size per GPU/TPU core/CPU for training.\n",
    "                per_device_eval_batch_size=per_device_eval_batch_size, #The batch size per GPU/TPU core/CPU for evaluation.\n",
    "                optim=\"adafactor\", #The optimizer to use: adamw_hf, adamw_torch, adamw_apex_fused, or adafactor.\n",
    "                #report_to=\"tensorboard\",\n",
    "                fp16=True\n",
    "    )\n",
    "\n",
    "    print('LOGGING: set_training_args DONE \\n')\n",
    "\n",
    "    return training_args\n",
    "\n",
    "\n",
    "def get_clean_model(model_name):\n",
    "    \"\"\"\n",
    "    Simple function to ensure that a new model is used for finetuning\n",
    "    \"\"\"\n",
    "    return AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "def set_trainer(model_name, training_args, train_ds, val_ds, tokenizer):\n",
    "    \"\"\"\n",
    "    Initializes a trainer\n",
    "    Takes in: Model name, training arguments, training dataset, validation dataset, and tokenizer\n",
    "    Returns: Trainer instance\n",
    "    \"\"\"\n",
    "    clean_model = get_clean_model(model_name)\n",
    "    trainer = Seq2SeqTrainer(\n",
    "                model=clean_model,\n",
    "                args=training_args,\n",
    "                train_dataset=train_ds,\n",
    "                eval_dataset=val_ds,\n",
    "                compute_metrics=compute_metrics,\n",
    "                tokenizer=tokenizer\n",
    "                )\n",
    "    \n",
    "    print('LOGGING: set_trainer DONE \\n')\n",
    "\n",
    "    return trainer\n",
    "\n",
    "def train_and_save(trainer, model_name):\n",
    "    trainer.train()\n",
    "    trainer.save_model(f\"../Models/{model_name}\") \n",
    "\n",
    "\n",
    "def get_saved_model(model_name):\n",
    "  saved_model = T5ForConditionalGeneration.from_pretrained(f'../Models/{model_name}', local_files_only=True)\n",
    "  tokenizer = AutoTokenizer.from_pretrained(f'../Models/{model_name}' ,local_files_only=True)\n",
    "  return saved_model, tokenizer\n",
    "\n",
    "\n",
    "\n",
    "def generate_predictions(test_set, saved_model):\n",
    "  \"\"\"\n",
    "  Generates predictions based on the test set, returns a list of predictions and the corresponding \"true\" articles\n",
    "  \"\"\"\n",
    "  #split the testset into input and output, so that we easily generate predictions and compare them to the true version\n",
    "  true_articles = test_set['output']\n",
    "\n",
    "  print(f' true_articles  {true_articles}' )\n",
    "\n",
    "  encoded_test_set = test_set.map(preprocess_data, batched=True, remove_columns=test_set.column_names)\n",
    "  encoded_test_set.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "  encoded_inputs = encoded_test_set.remove_columns(\"labels\")\n",
    "\n",
    "  # set-up a dataloader to load in the tokenized test dataset\n",
    "  test_dataloader = torch.utils.data.DataLoader(encoded_inputs, batch_size=32)\n",
    "\n",
    "  # generate text for each batch\n",
    "  all_predictions = []\n",
    "  for i,batch in enumerate(test_dataloader):\n",
    "\n",
    "    predictions = saved_model.generate(**batch)\n",
    "\n",
    "    all_predictions.append(predictions)\n",
    "\n",
    "  # flatten predictions\n",
    "  all_predictions_flattened = [pred for preds in all_predictions for pred in preds]\n",
    "\n",
    "  return all_predictions_flattened, true_articles\n",
    "\n",
    "\n",
    "\n",
    "def decode_predictions(predictions, tokenizer):\n",
    "  \"\"\"\n",
    "  Decode the predictions made by the model\n",
    "  \"\"\"\n",
    "  decoded_predictions = []\n",
    "\n",
    "  for iteration, prediction in enumerate(predictions):\n",
    "      decoded_predictions.append(tokenizer.decode(prediction,skip_special_tokens=True))\n",
    "\n",
    "  return decoded_predictions\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_test_set(test_set, model_name):\n",
    "  \"\"\"\n",
    "  Transforms test set, retrieves predictions, and evaluates these predictions\n",
    "  \"\"\"\n",
    "  saved_model, saved_tokenizer = get_saved_model(model_name)\n",
    "\n",
    "  predictions, test_articles = generate_predictions(test_set, saved_model)\n",
    "  \n",
    "  decoded_test_predictions = decode_predictions(predictions, saved_tokenizer)\n",
    "\n",
    "  bleu_output, rouge_output, meteor_output, perp_output, bertscore_output, bart_scores_output = evaluate_texts(decoded_test_predictions, test_articles)\n",
    "\n",
    "  evaluation_results = {\"blue_output\": bleu_output, \"rouge_output\": rouge_output, \"meteor_results\": meteor_output, \"perp_output\": perp_output, \"bertscore_output\": bertscore_output, \"bart_scores_output\": bart_scores_output}\n",
    "\n",
    "  log_results(evaluation_results)\n",
    "\n",
    "\n",
    "\n",
    "  # Additional PARENT evaluation\n",
    "  tables = test_set['input']\n",
    "  references = test_articles\n",
    "  generations = decoded_test_predictions\n",
    "  parent_attempt(generations, references, tables)\n",
    "  #\n",
    "\n",
    "  #bleu_output, rouge_output, meteor_output, perp_output, bertscore_output, bart_scores_output = evaluate_texts(decoded_test_predictions, test_articles)\n",
    "\n",
    "  ## Huggingsface trainer requires a dict if multiple metrics are used\n",
    "  return evaluation_results\n",
    "\n",
    "\n",
    "def write_to_text_parent(decoded_predictions, true_articles, rdfs):\n",
    "\n",
    "\n",
    "    with open('../Parent_test/true_articles.txt', 'w') as f:\n",
    "        for articles in true_articles:\n",
    "            f.write(f'{articles} \\n')\n",
    "   \n",
    "    with open('../Parent_test/decode_predictions.txt', 'w') as f:\n",
    "        for predictions in decoded_predictions:\n",
    "            f.write(f'{predictions} \\n')\n",
    "\n",
    "    with open('../Parent_test/rdfs.txt', 'w') as f:\n",
    "        for pairs in rdfs:\n",
    "            f.write(f'{pairs} \\n')\n",
    "\n",
    "\n",
    "def prepare_inputs_parent(RDFs):\n",
    "    \"\"\"\n",
    "    Cleans the RDF pairs and transforms them in the proper format so that the parent module can calculate with it.\n",
    "    \"\"\"\n",
    "    \n",
    "    attribute_value_pairs = []\n",
    "\n",
    "    for iteration, inputRDF in enumerate(RDFs):\n",
    "        if 'â‚¹' in inputRDF:\n",
    "        #print(f'{inputRDF} \\n')\n",
    "            inputRDF = inputRDF.replace('â‚¹', '')\n",
    "        \n",
    "        split_RDF = inputRDF.split(\", \")\n",
    "        entry=[]\n",
    "        for connected_pair in split_RDF:\n",
    "            if '[' in connected_pair:\n",
    "                connected_pair = connected_pair.replace('[', '')\n",
    "            if ']' in connected_pair:\n",
    "                connected_pair = connected_pair.replace(']', '')\n",
    "            if '_' in connected_pair:\n",
    "                connected_pair = connected_pair.replace('_', ' ')    \n",
    "            split_pair = tuple(connected_pair.split(' | '))\n",
    "            entry.append((split_pair))\n",
    "        attribute_value_pairs.append(entry)\n",
    "    return attribute_value_pairs\n",
    "\n",
    "\n",
    "\n",
    "def parent_attempt(generations, references, rdfs):\n",
    "    \"\"\"\n",
    "    The Parent metric needs special treatment, as it only accepts specific inputs and file types.\n",
    "    \"\"\"\n",
    "    prepared_rdfs = prepare_inputs_parent(rdfs)\n",
    "    write_to_text_parent(generations, references, rdfs)\n",
    "\n",
    "    %run -i \"~E:/ArriaThesis/MscThesis/Code/Evaluation_Code/Parent.py\" --references \"E:/ArriaThesis/MscThesis/Parent_test/true_articles.txt\" \\\n",
    "                                                                       --generations \"E:/ArriaThesis/MscThesis/Parent_test/decode_predictions.txt\"  \\\n",
    "                                                                       --tables \"E:/ArriaThesis/MscThesis/Parent_test/rdfs.txt\" \n",
    "\n",
    "def log_results(results):\n",
    "    with open('../Logging_Results/logResults.json', 'w') as convert_file:\n",
    "        convert_file.write(json.dumps(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune_model(model_name):\n",
    "    # ensure cuda compatability\n",
    "    ensure_cuda_compatability()\n",
    "\n",
    "    # I instantiate the tokenizer as a global variable, as the .map function in transform_datasets was not working properly. \n",
    "    # This should not be an issue, as the tokenizer remains consistent during training and evaluation.\n",
    "    global tokenizer\n",
    "    \n",
    "    # retrieve model and tokenizer from huggingface to prepare dataset\n",
    "    model, tokenizer = preprocess_model(model_name)\n",
    "    \n",
    "    #retrieve the unprocessed data from the csv files\n",
    "    entire_dataset = load_CACAPO_data()\n",
    "    \n",
    "    # process the dataset and split it into its natural train, val, test split\n",
    "    train_ds, val_ds, test_ds = transform_datasets(entire_dataset)\n",
    "\n",
    "    # setup the training arguments \n",
    "    # parameters = (model_name, learning_rate, num_train_epochs, evaluation_strategy, generation_num_beams, gradient_accumulation_steps, per_device_train_batch_size, per_device_eval_batch_size)\n",
    "    training_args = set_training_args(model_name, 0.001, 1, 'epoch', 10, 4, 8, 8)\n",
    "\n",
    "    # create a trainer instance \n",
    "    trainer = set_trainer(model_name, training_args, train_ds, val_ds, tokenizer)\n",
    "\n",
    "    # Finally fine-tune the model and save it\n",
    "    train_and_save(trainer, model_name)\n",
    "\n",
    "    testset_evaluation_results = evaluate_test_set(test_ds, model_name)\n",
    "\n",
    "    #log_results(testset_evaluation_results)\n",
    "\n",
    "    return testset_evaluation_results\n",
    "\n",
    "def main():\n",
    "    model_name = 't5-base'\n",
    "    results = fine_tune_model(model_name)\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 1.12.1\n",
      "Cuda version: 11.3\n",
      "Cudnn version: 8302\n",
      "Is cuda available: True\n",
      "Number of cuda devices: 1\n",
      "Current default device: 0\n",
      "First cuda device: <torch.cuda.device object at 0x000001FFCE92A888>\n",
      "Name of the first cuda device: NVIDIA GeForce GTX 1070\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Anaconda\\envs\\CudaSupEnv\\lib\\site-packages\\transformers\\models\\t5\\tokenization_t5_fast.py:166: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  FutureWarning,\n",
      "Using custom data configuration Cleaned_data-3c9b553877c933dc\n",
      "Found cached dataset csv (C:/Users/Simon/.cache/huggingface/datasets/csv/Cleaned_data-3c9b553877c933dc/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOGGING: preprocess_model DONE \n",
      "\n",
      "LOGGING: load_CACAPO_data DONE \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 998.80it/s]\n",
      "Parameter 'function'=<function preprocess_data at 0x000001FFCE9A8A68> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]e:\\Anaconda\\envs\\CudaSupEnv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3543: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  \"`as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your \"\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOGGING: transform_datasets DONE \n",
      "\n",
      "LOGGING: set_training_args DONE \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Using cuda_amp half precision backend\n",
      "***** Running training *****\n",
      "  Num examples = 30\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOGGING: set_trainer DONE \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:06<00:00,  6.46s/it]***** Running Evaluation *****\n",
      "  Num examples = 4\n",
      "  Batch size = 8\n",
      "e:\\Anaconda\\envs\\CudaSupEnv\\lib\\site-packages\\ipykernel_launcher.py:83: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Simon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Simon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Simon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "loading file vocab.json from cache at C:\\Users\\Simon/.cache\\huggingface\\hub\\models--facebook--bart-large-cnn\\snapshots\\c5121e42f57eca153aea31729f71cbedcd77a656\\vocab.json\n",
      "loading file merges.txt from cache at C:\\Users\\Simon/.cache\\huggingface\\hub\\models--facebook--bart-large-cnn\\snapshots\\c5121e42f57eca153aea31729f71cbedcd77a656\\merges.txt\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at None\n",
      "loading configuration file config.json from cache at C:\\Users\\Simon/.cache\\huggingface\\hub\\models--facebook--bart-large-cnn\\snapshots\\c5121e42f57eca153aea31729f71cbedcd77a656\\config.json\n",
      "Model config BartConfig {\n",
      "  \"_name_or_path\": \"facebook/bart-large-cnn\",\n",
      "  \"_num_labels\": 3,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"force_bos_token_to_be_generated\": true,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"length_penalty\": 2.0,\n",
      "  \"max_length\": 142,\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"min_length\": 56,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": \" \",\n",
      "  \"scale_embedding\": false,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 142,\n",
      "      \"min_length\": 56,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50264\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at C:\\Users\\Simon/.cache\\huggingface\\hub\\models--facebook--bart-large-cnn\\snapshots\\c5121e42f57eca153aea31729f71cbedcd77a656\\config.json\n",
      "Model config BartConfig {\n",
      "  \"_num_labels\": 3,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"force_bos_token_to_be_generated\": true,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"length_penalty\": 2.0,\n",
      "  \"max_length\": 142,\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"min_length\": 56,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": \" \",\n",
      "  \"scale_embedding\": false,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 142,\n",
      "      \"min_length\": 56,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50264\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\Simon/.cache\\huggingface\\hub\\models--facebook--bart-large-cnn\\snapshots\\c5121e42f57eca153aea31729f71cbedcd77a656\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BartForConditionalGeneration.\n",
      "\n",
      "All the weights of BartForConditionalGeneration were initialized from the model checkpoint at facebook/bart-large-cnn.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOGGING: load_eval_metrics DONE \n",
      "\n",
      "\n",
      " LOGGING: Calculating Blue\n",
      "\n",
      " LOGGING: Calculating Rouge\n",
      "\n",
      " LOGGING: Calculating Meteor\n",
      "\n",
      " LOGGING: Calculating Perplexity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\Simon/.cache\\huggingface\\hub\\models--gpt2\\snapshots\\75e09b43581151bd1d9ef6700faa605df408979f\\config.json\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"gpt2\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\Simon/.cache\\huggingface\\hub\\models--gpt2\\snapshots\\75e09b43581151bd1d9ef6700faa605df408979f\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file config.json from cache at C:\\Users\\Simon/.cache\\huggingface\\hub\\models--gpt2\\snapshots\\75e09b43581151bd1d9ef6700faa605df408979f\\config.json\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"gpt2\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "loading file vocab.json from cache at C:\\Users\\Simon/.cache\\huggingface\\hub\\models--gpt2\\snapshots\\75e09b43581151bd1d9ef6700faa605df408979f\\vocab.json\n",
      "loading file merges.txt from cache at C:\\Users\\Simon/.cache\\huggingface\\hub\\models--gpt2\\snapshots\\75e09b43581151bd1d9ef6700faa605df408979f\\merges.txt\n",
      "loading file tokenizer.json from cache at C:\\Users\\Simon/.cache\\huggingface\\hub\\models--gpt2\\snapshots\\75e09b43581151bd1d9ef6700faa605df408979f\\tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at None\n",
      "loading configuration file config.json from cache at C:\\Users\\Simon/.cache\\huggingface\\hub\\models--gpt2\\snapshots\\75e09b43581151bd1d9ef6700faa605df408979f\\config.json\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"gpt2\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "Using pad_token, but it is not set yet.\n",
      "Assigning <|endoftext|> to the pad_token key of the tokenizer\n",
      "\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 55.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " LOGGING: Calculating Bertscore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file config.json from cache at C:\\Users\\Simon/.cache\\huggingface\\hub\\models--roberta-large\\snapshots\\5069d8a2a32a7df4c69ef9b56348be04152a2341\\config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading file vocab.json from cache at C:\\Users\\Simon/.cache\\huggingface\\hub\\models--roberta-large\\snapshots\\5069d8a2a32a7df4c69ef9b56348be04152a2341\\vocab.json\n",
      "loading file merges.txt from cache at C:\\Users\\Simon/.cache\\huggingface\\hub\\models--roberta-large\\snapshots\\5069d8a2a32a7df4c69ef9b56348be04152a2341\\merges.txt\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at None\n",
      "loading configuration file config.json from cache at C:\\Users\\Simon/.cache\\huggingface\\hub\\models--roberta-large\\snapshots\\5069d8a2a32a7df4c69ef9b56348be04152a2341\\config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at C:\\Users\\Simon/.cache\\huggingface\\hub\\models--roberta-large\\snapshots\\5069d8a2a32a7df4c69ef9b56348be04152a2341\\config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\Simon/.cache\\huggingface\\hub\\models--roberta-large\\snapshots\\5069d8a2a32a7df4c69ef9b56348be04152a2341\\pytorch_model.bin\n",
      "All the weights of RobertaModel were initialized from the model checkpoint at roberta-large.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaModel for predictions without further training.\n",
      "Trainer is attempting to log a value of \"{'bleu': 0.0, 'precisions': [0.0, 0.0, 0.0, 0.0], 'brevity_penalty': 0.014541200706177667, 'length_ratio': 0.19117647058823528, 'translation_length': 13, 'reference_length': 68}\" of type <class 'dict'> for key \"eval/blue_output\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'rouge1': 0.2264957264957265, 'rouge2': 0.08575581395348837, 'rougeL': 0.20058275058275057, 'rougeLsum': 0.1987179487179487}\" of type <class 'dict'> for key \"eval/rouge_output\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'meteor': 0.03596663752913753}\" of type <class 'dict'> for key \"eval/meteor_results\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'perplexities': [1904.3201904296875, 868.296142578125, 3095.555419921875, 3135.15283203125], 'mean_perplexity': 2250.8311462402344}\" of type <class 'dict'> for key \"eval/perp_output\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': [0.805133581161499, 0.805006206035614, 0.790022611618042, 0.7857273817062378], 'recall': [0.802182674407959, 0.8623934388160706, 0.761752188205719, 0.7797167301177979], 'f1': [0.8036553859710693, 0.8327122330665588, 0.7756299376487732, 0.7827105522155762], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.23.1)'}\" of type <class 'dict'> for key \"eval/bertscore_output\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[-3.0330724716186523, -3.8443024158477783, -5.874936580657959, -6.724938869476318]\" of type <class 'list'> for key \"eval/bart_scores_output\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:23<00:00,  6.46s/it]\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:23<00:00, 23.79s/it]\n",
      "Saving model checkpoint to ../Models/t5-base\n",
      "Configuration saved in ../Models/t5-base\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " LOGGING: Calculating Bartscore\n",
      "{'eval_loss': 17.07703399658203, 'eval_blue_output': {'bleu': 0.0, 'precisions': [0.0, 0.0, 0.0, 0.0], 'brevity_penalty': 0.014541200706177667, 'length_ratio': 0.19117647058823528, 'translation_length': 13, 'reference_length': 68}, 'eval_rouge_output': {'rouge1': 0.2264957264957265, 'rouge2': 0.08575581395348837, 'rougeL': 0.20058275058275057, 'rougeLsum': 0.1987179487179487}, 'eval_meteor_results': {'meteor': 0.03596663752913753}, 'eval_perp_output': {'perplexities': [1904.3201904296875, 868.296142578125, 3095.555419921875, 3135.15283203125], 'mean_perplexity': 2250.8311462402344}, 'eval_bertscore_output': {'precision': [0.805133581161499, 0.805006206035614, 0.790022611618042, 0.7857273817062378], 'recall': [0.802182674407959, 0.8623934388160706, 0.761752188205719, 0.7797167301177979], 'f1': [0.8036553859710693, 0.8327122330665588, 0.7756299376487732, 0.7827105522155762], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.23.1)'}, 'eval_bart_scores_output': [-3.0330724716186523, -3.8443024158477783, -5.874936580657959, -6.724938869476318], 'eval_runtime': 17.3018, 'eval_samples_per_second': 0.231, 'eval_steps_per_second': 0.058, 'epoch': 1.0}\n",
      "{'train_runtime': 23.8119, 'train_samples_per_second': 1.26, 'train_steps_per_second': 0.042, 'train_loss': 15.083046913146973, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ../Models/t5-base\\pytorch_model.bin\n",
      "tokenizer config file saved in ../Models/t5-base\\tokenizer_config.json\n",
      "Special tokens file saved in ../Models/t5-base\\special_tokens_map.json\n",
      "loading configuration file ../Models/t5-base\\config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-base\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 3072,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading weights file ../Models/t5-base\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at ../Models/t5-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
      "loading file spiece.model\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at C:/Users/Simon/.cache/huggingface/datasets/csv/Cleaned_data-3c9b553877c933dc/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\\cache-c37459eef50bea63.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " true_articles  ['A bar fight ends in a violent shooting overnight in southeast Houston.', \"The Minnesota right-hander threw his first major league complete game and Carlos Gomez had four hits and two RBIs, leading the Twins to a 5-1 victory Thursday night that stretched KC's losing streak to 11 straight.\", 'ETF is a basket of securities traded on an exchange similar to stocks.', 'Ms Diamond said clear skies at night would result in a widespread frost from Wednesday.', 'Ook dat neemt PSV mee naar volgende week, naar de sleutelwedstrijd in de Kuip.', 'N i e u w bericht, vervangt: AEX opent met nipte winst']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Anaconda\\envs\\CudaSupEnv\\lib\\site-packages\\transformers\\generation_utils.py:1301: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 20 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  UserWarning,\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Simon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Simon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Simon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "loading file vocab.json from cache at C:\\Users\\Simon/.cache\\huggingface\\hub\\models--facebook--bart-large-cnn\\snapshots\\c5121e42f57eca153aea31729f71cbedcd77a656\\vocab.json\n",
      "loading file merges.txt from cache at C:\\Users\\Simon/.cache\\huggingface\\hub\\models--facebook--bart-large-cnn\\snapshots\\c5121e42f57eca153aea31729f71cbedcd77a656\\merges.txt\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at None\n",
      "loading configuration file config.json from cache at C:\\Users\\Simon/.cache\\huggingface\\hub\\models--facebook--bart-large-cnn\\snapshots\\c5121e42f57eca153aea31729f71cbedcd77a656\\config.json\n",
      "Model config BartConfig {\n",
      "  \"_name_or_path\": \"facebook/bart-large-cnn\",\n",
      "  \"_num_labels\": 3,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"force_bos_token_to_be_generated\": true,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"length_penalty\": 2.0,\n",
      "  \"max_length\": 142,\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"min_length\": 56,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": \" \",\n",
      "  \"scale_embedding\": false,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 142,\n",
      "      \"min_length\": 56,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50264\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at C:\\Users\\Simon/.cache\\huggingface\\hub\\models--facebook--bart-large-cnn\\snapshots\\c5121e42f57eca153aea31729f71cbedcd77a656\\config.json\n",
      "Model config BartConfig {\n",
      "  \"_num_labels\": 3,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"force_bos_token_to_be_generated\": true,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"length_penalty\": 2.0,\n",
      "  \"max_length\": 142,\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"min_length\": 56,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": \" \",\n",
      "  \"scale_embedding\": false,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 142,\n",
      "      \"min_length\": 56,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50264\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\Simon/.cache\\huggingface\\hub\\models--facebook--bart-large-cnn\\snapshots\\c5121e42f57eca153aea31729f71cbedcd77a656\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BartForConditionalGeneration.\n",
      "\n",
      "All the weights of BartForConditionalGeneration were initialized from the model checkpoint at facebook/bart-large-cnn.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOGGING: load_eval_metrics DONE \n",
      "\n",
      "\n",
      " LOGGING: Calculating Blue\n",
      "\n",
      " LOGGING: Calculating Rouge\n",
      "\n",
      " LOGGING: Calculating Meteor\n",
      "\n",
      " LOGGING: Calculating Perplexity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\Simon/.cache\\huggingface\\hub\\models--gpt2\\snapshots\\75e09b43581151bd1d9ef6700faa605df408979f\\config.json\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"gpt2\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\Simon/.cache\\huggingface\\hub\\models--gpt2\\snapshots\\75e09b43581151bd1d9ef6700faa605df408979f\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file config.json from cache at C:\\Users\\Simon/.cache\\huggingface\\hub\\models--gpt2\\snapshots\\75e09b43581151bd1d9ef6700faa605df408979f\\config.json\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"gpt2\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "loading file vocab.json from cache at C:\\Users\\Simon/.cache\\huggingface\\hub\\models--gpt2\\snapshots\\75e09b43581151bd1d9ef6700faa605df408979f\\vocab.json\n",
      "loading file merges.txt from cache at C:\\Users\\Simon/.cache\\huggingface\\hub\\models--gpt2\\snapshots\\75e09b43581151bd1d9ef6700faa605df408979f\\merges.txt\n",
      "loading file tokenizer.json from cache at C:\\Users\\Simon/.cache\\huggingface\\hub\\models--gpt2\\snapshots\\75e09b43581151bd1d9ef6700faa605df408979f\\tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at None\n",
      "loading configuration file config.json from cache at C:\\Users\\Simon/.cache\\huggingface\\hub\\models--gpt2\\snapshots\\75e09b43581151bd1d9ef6700faa605df408979f\\config.json\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"gpt2\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "Using pad_token, but it is not set yet.\n",
      "Assigning <|endoftext|> to the pad_token key of the tokenizer\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 45.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " LOGGING: Calculating Bertscore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file config.json from cache at C:\\Users\\Simon/.cache\\huggingface\\hub\\models--roberta-large\\snapshots\\5069d8a2a32a7df4c69ef9b56348be04152a2341\\config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading file vocab.json from cache at C:\\Users\\Simon/.cache\\huggingface\\hub\\models--roberta-large\\snapshots\\5069d8a2a32a7df4c69ef9b56348be04152a2341\\vocab.json\n",
      "loading file merges.txt from cache at C:\\Users\\Simon/.cache\\huggingface\\hub\\models--roberta-large\\snapshots\\5069d8a2a32a7df4c69ef9b56348be04152a2341\\merges.txt\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at None\n",
      "loading configuration file config.json from cache at C:\\Users\\Simon/.cache\\huggingface\\hub\\models--roberta-large\\snapshots\\5069d8a2a32a7df4c69ef9b56348be04152a2341\\config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at C:\\Users\\Simon/.cache\\huggingface\\hub\\models--roberta-large\\snapshots\\5069d8a2a32a7df4c69ef9b56348be04152a2341\\config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\Simon/.cache\\huggingface\\hub\\models--roberta-large\\snapshots\\5069d8a2a32a7df4c69ef9b56348be04152a2341\\pytorch_model.bin\n",
      "All the weights of RobertaModel were initialized from the model checkpoint at roberta-large.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaModel for predictions without further training.\n",
      "I1020 22:37:01.061661  6920 Parent.py:455] Evaluated 6 examples.\n",
      "I1020 22:37:01.061661  6920 Parent.py:457] Precision = 0.0340 Recall = 0.0333 F-score = 0.0337\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " LOGGING: Calculating Bartscore\n",
      "sum(precisions):  0.20412414523193154    len(precisions  6)\n",
      "sum(recalls):  0.20005000000000003    len(recalls) 6)\n",
      "sum(all_f_scores) :  0.2020410238678085    len(all_f_scores)  6)\n",
      "{'blue_output': {'bleu': 0.0, 'precisions': [0.0, 0.0, 0.0, 0.0], 'brevity_penalty': 0.05376799253902105, 'length_ratio': 0.2549019607843137, 'translation_length': 26, 'reference_length': 102}, 'rouge_output': {'rouge1': 0.19494454347395526, 'rouge2': 0.0898917259211377, 'rougeL': 0.1873712616359675, 'rougeLsum': 0.18732005295472787}, 'meteor_results': {'meteor': 0.028067127721660073}, 'perp_output': {'perplexities': [1461.142578125, 2210.488525390625, 124.29325103759766, 3785.825927734375, 2463.470947265625, 6312.71044921875], 'mean_perplexity': 2726.3219464619956}, 'bertscore_output': {'precision': [0.7856204509735107, 0.7702969312667847, 0.7882493734359741, 0.7831587195396423, 0.7831859588623047, 0.8187204599380493], 'recall': [0.8525662422180176, 0.7806869149208069, 0.8233743906021118, 0.8275227546691895, 0.7649168372154236, 0.789258599281311], 'f1': [0.8177254796028137, 0.7754570841789246, 0.8054291605949402, 0.8047297596931458, 0.7739435434341431, 0.8037196397781372], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.23.1)'}, 'bart_scores_output': [-3.9130499362945557, -4.045971393585205, -3.3393163681030273, -5.33759880065918, -5.242710590362549, -6.465360164642334]}\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('CudaSupEnv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8ce04b5a4dcd593beec21f85756cfa75ba50da4e44d5dd0d068d93735596035b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
