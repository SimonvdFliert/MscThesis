{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This file serves as a cleaner version of fine-tuning a model. \n",
    "\n",
    "Below several models can be added to the main function, afterwhich the code will prepare, train, evaluate, and save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Anaconda\\envs\\CudaSupEnv\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import WebNLG_xmlReader.benchmark_reader as xml_reader\n",
    "import os.path\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import torch \n",
    "import nltk\n",
    "#import tensorflow as tf\n",
    "from transformers import AutoTokenizer, T5ForConditionalGeneration, Seq2SeqTrainingArguments, Seq2SeqTrainer, pipeline, AutoModelForSeq2SeqLM\n",
    "\n",
    "import datasets\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_cuda_compatability():\n",
    "    print(f'Torch version: {torch.__version__}')\n",
    "    print(f'Cuda version: {torch.version.cuda}')\n",
    "    print(f'Cudnn version: {torch.backends.cudnn.version()}')\n",
    "    print(f'Is cuda available: {torch.cuda.is_available()}')\n",
    "    print(f'Number of cuda devices: {torch.cuda.device_count()}')\n",
    "    print(f'Current default device: {torch.cuda.current_device()}')\n",
    "    print(f'First cuda device: {torch.cuda.device(0)}')\n",
    "    print(f'Name of the first cuda device: {torch.cuda.get_device_name(0)}\\n\\n')\n",
    "\n",
    "\n",
    "def preprocess_model(model_name):\n",
    "    \"\"\"\n",
    "    Setup the model and tokenizer for preprocessing. This will be a pre-trained model collected from huggingface\n",
    "    \"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name) \n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "    \n",
    "    print('LOGGING: preprocess_model DONE \\n')\n",
    "    return model, tokenizer\n",
    "\n",
    "def load_CACAPO_data():\n",
    "    \"\"\"\n",
    "    This function retrieves the csv files and creates a dataset\n",
    "    \"\"\"\n",
    "    print('LOGGING: load_CACAPO_data DONE \\n')\n",
    "\n",
    "    return datasets.load_dataset(\"../Data/Cleaned_data/\", data_files={\"train\": \"Train.csv\", \"dev\": \"Dev.csv\", \"test\": \"Test.csv\"})\n",
    "\n",
    "def preprocess_data(data):\n",
    "    \"\"\"\n",
    "    Tokenize the data\n",
    "    \"\"\"\n",
    "    max_length = 256\n",
    "    RDFs = data[\"input\"]\n",
    "    texts = data[\"output\"]\n",
    "\n",
    "    model_inputs = tokenizer(RDFs, truncation=True, padding='max_length', return_tensors='pt',  max_length=max_length)\n",
    "\n",
    "    # specially for seq2seq tokenizer, \"Temporarily sets the tokenizer for encoding the targets. Useful for tokenizer associated to sequence-to-sequence models that need a slightly different processing for the labels.\"\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        target_texts = tokenizer(texts, padding='max_length', truncation=True, return_tensors='pt',  max_length=max_length).input_ids\n",
    "    \n",
    "    model_inputs[\"labels\"] = target_texts\n",
    "    \n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "def transform_datasets(dataset):\n",
    "    \"\"\"\n",
    "    After loading in and creating the initial dataset, the text data is transformed, by tokenizing the input and output texts. The initial dataset is also split into train,val,test for training use. \n",
    "    \"\"\"\n",
    "\n",
    "    ## Create smaller versions of the dataset\n",
    "    small_train_test = dataset[\"train\"].shard(num_shards = 64, index = 0)\n",
    "    small_val_test = dataset[\"dev\"].shard(num_shards = 64, index = 0)\n",
    "    small_test_test = dataset[\"test\"].shard(num_shards = 64, index = 0)\n",
    "\n",
    "    ## Process the data in batches\n",
    "    small_train_test = small_train_test.map(preprocess_data, batched=True, remove_columns=dataset[\"train\"].column_names)  \n",
    "    small_val_test = small_val_test.map(preprocess_data, batched=True, remove_columns=dataset[\"dev\"].column_names)\n",
    "    small_test_test = small_test_test.map(preprocess_data, batched=True, remove_columns=dataset[\"test\"].column_names)\n",
    "\n",
    "    # transform the datasets into torch sensors, as the model will expect this format \n",
    "    small_train_test.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "    small_val_test.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "    small_test_test.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "    print('LOGGING: transform_datasets DONE \\n')\n",
    "\n",
    "    return small_train_test, small_val_test, small_test_test\n",
    "\n",
    "\n",
    "def load_eval_metrics():\n",
    "    \"\"\"\n",
    "    Loads in all metrics that will be used later on during evaluation. This is seperated to not load in the metrics a dozen of times during training.\n",
    "    \"\"\"\n",
    "    bleu = datasets.load_metric(\"bleu\")\n",
    "    rouge = evaluate.load('rouge')\n",
    "    meteor = evaluate.load('meteor')\n",
    "    perplexity = evaluate.load(\"perplexity\", module_type=\"metric\")\n",
    "    bertscore = evaluate.load(\"bertscore\")\n",
    "    \n",
    "    print('LOGGING: load_eval_metrics DONE \\n')\n",
    "\n",
    "    return bleu, rouge, meteor, perplexity, bertscore\n",
    "\n",
    "def postprocess_text(preds, labels):\n",
    "    \"\"\"\n",
    "    Supplementary Method called in decode_text.\n",
    "    \n",
    "    Returns list of split decoded labels and predictions for evaluation\n",
    "    \"\"\"\n",
    "    preds = [pred.split() for pred in preds]\n",
    "    labels = [[label.split()] for label in labels]\n",
    "    return preds, labels\n",
    "\n",
    "# def rouge_postprocess(decoded_preds, decoded_labels):\n",
    "#        # Rouge expects a newline after each sentence\n",
    "#     decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip()))\n",
    "#                       for pred in decoded_preds]\n",
    "#     decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) \n",
    "#                       for label in decoded_labels]\n",
    "                \n",
    "    # return decoded_preds, decoded_labels\n",
    "\n",
    "def decode_text(predictions, labels):\n",
    "    \"\"\"\n",
    "    Supplementary Method called in compute_metrics.\n",
    "    \n",
    "    Returns decoded labels and predictions for evaluation\n",
    "    \"\"\"\n",
    "    if isinstance(predictions, tuple):\n",
    "            predictions = predictions[0]\n",
    "        \n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    # # Some simple post-processing\n",
    "    # decoded_preds, decoded_labels = postprocess_text(decoded_preds,\n",
    "    # decoded_labels)\n",
    "\n",
    "    return decoded_preds, decoded_labels\n",
    "\n",
    "\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    \"\"\"\"\n",
    "    Metrics to be evaluated during training and validation\n",
    "    Metrics used: BLEU, ROUGE, METEOR, BARTScore, PARENT\n",
    "    \"\"\"\n",
    "    # decode the predictions and labels for eval\n",
    "    predictions, labels = pred\n",
    "    decoded_preds, decoded_labels = decode_text(predictions, labels)\n",
    "\n",
    "    #post_process for BLEU\n",
    "    blue_preds, blue_labels = postprocess_text(decoded_preds,  decoded_labels)\n",
    "\n",
    "    # setup metrics for use\n",
    "    bleu, rouge, meteor, perplexity, bertscore = load_eval_metrics()\n",
    "\n",
    "    # Calculate the metrics\n",
    "    print(f'\\n Calculating Blue')\n",
    "    bleu_output = bleu.compute(predictions=blue_preds, references=blue_labels)\n",
    "    print(f'\\n Calculating rouge')\n",
    "    rouge_output = rouge.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    print(f'\\n Calculating meteor')\n",
    "    meteor_results = meteor.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    print(f'\\n Calculating perplexity')\n",
    "    perp_results = perplexity.compute(predictions=decoded_preds, model_id='gpt2')\n",
    "    print(f'\\n Calculating bertscore')\n",
    "    bertscore_output = bertscore.compute(predictions=decoded_preds, references=decoded_labels, lang=\"en\")\n",
    "    ### Need to add parent and bartscore\n",
    "\n",
    "    ## Huggingsface trainer requires a dict if multiple metrics are used\n",
    "    return {\" blue_output\": bleu_output, \"rouge_output\": rouge_output, \"meteor_results\": meteor_results, \"perp_results\": perp_results, \"bertscore_output\": bertscore_output}\n",
    "\n",
    "\n",
    "\n",
    "def set_training_args(model_name, learning_rate, num_train_epochs, evaluation_strategy, generation_num_beams, gradient_accumulation_steps, per_device_train_batch_size, per_device_eval_batch_size):\n",
    "    \"\"\"\n",
    "    Setup the training arguments that will be used during training.\n",
    "    \"\"\"\n",
    "    #model_name = \"t5-fp16-test\"\n",
    "    model_dir = f\"../Results/{model_name}\"\n",
    "\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "                output_dir=model_dir,\n",
    "                learning_rate=learning_rate,\n",
    "                do_eval=True, # will be set to true if evaluation strategy is set\n",
    "                do_predict=True, #Whether to run predictions on the test set or not.\n",
    "                num_train_epochs=num_train_epochs,\n",
    "                evaluation_strategy= evaluation_strategy, \n",
    "                #eval_steps= 100, # Number of update steps between two evaluations if evaluation_strategy=\"steps\". Will default to the same value as logging_steps if not set.\n",
    "                save_steps=500, # Number of updates steps before two checkpoint saves if save_strategy=\"steps\".\n",
    "                #max_steps=10, # the total number of training steps to perform\n",
    "                save_total_limit= 10, # the maximum number of models to keep before deleting the oldest one\n",
    "                predict_with_generate=True, # Whether to use generate to calculate generative metrics (ROUGE, BLEU).\n",
    "                generation_num_beams=generation_num_beams,  #The num_beams to use on each evaluation loop when predict_with_generate=True. Will default to the num_beams value of the model configuration\n",
    "                gradient_checkpointing=True, #\n",
    "                gradient_accumulation_steps=gradient_accumulation_steps, #Number of updates steps to accumulate the gradients for, before performing a backward/update pass\n",
    "                per_device_train_batch_size=per_device_train_batch_size, #The batch size per GPU/TPU core/CPU for training.\n",
    "                per_device_eval_batch_size=per_device_eval_batch_size, #The batch size per GPU/TPU core/CPU for evaluation.\n",
    "                optim=\"adafactor\", #The optimizer to use: adamw_hf, adamw_torch, adamw_apex_fused, or adafactor.\n",
    "                #report_to=\"tensorboard\",\n",
    "                fp16=True\n",
    "    )\n",
    "\n",
    "    print('LOGGING: set_training_args DONE \\n')\n",
    "\n",
    "    return training_args\n",
    "\n",
    "\n",
    "def get_clean_model(model_name):\n",
    "    \"\"\"\n",
    "    Simple function to ensure that a new model is used for finetuning\n",
    "    \"\"\"\n",
    "    return AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "def set_trainer(model_name, training_args, train_ds, val_ds, tokenizer):\n",
    "    \"\"\"\n",
    "    Initializes a trainer\n",
    "    Takes in: Model name, training arguments, training dataset, validation dataset, and tokenizer\n",
    "    Returns: Trainer instance\n",
    "    \"\"\"\n",
    "\n",
    "    #metrics = load_eval_metrics()\n",
    "    clean_model = get_clean_model(model_name)\n",
    "    trainer = Seq2SeqTrainer(\n",
    "                model=clean_model,\n",
    "                args=training_args,\n",
    "                train_dataset=train_ds,\n",
    "                eval_dataset=val_ds,\n",
    "                compute_metrics=compute_metrics,\n",
    "                tokenizer=tokenizer\n",
    "                )\n",
    "    \n",
    "    print('LOGGING: set_trainer DONE \\n')\n",
    "\n",
    "    return trainer\n",
    "\n",
    "def train_and_save(trainer, model_name):\n",
    "    trainer.train()\n",
    "    trainer.save_model(f\"../Models/{model_name}\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune_model(model_name):\n",
    "    # ensure cuda compatability\n",
    "    ensure_cuda_compatability()\n",
    "    # I instantiate the tokenizer as a global variable, as the .map function in transform_datasets was not working properly. \n",
    "    # This should not be an issue, as the tokenizer remains consistent during training and evaluation.\n",
    "    global tokenizer\n",
    "    # retrieve model and tokenizer from huggingface to prepare dataset\n",
    "    model, tokenizer = preprocess_model(model_name)\n",
    "    \n",
    "    #retrieve the unprocessed data from the csv files\n",
    "    entire_dataset = load_CACAPO_data()\n",
    "    \n",
    "    # process the dataset and split it into its natural train, val, test split\n",
    "    train_ds, val_ds, test_ds = transform_datasets(entire_dataset)\n",
    "\n",
    "    # setup the training arguments \n",
    "    # parameters = (model_name, learning_rate, num_train_epochs, evaluation_strategy, generation_num_beams, gradient_accumulation_steps, per_device_train_batch_size, per_device_eval_batch_size)\n",
    "    training_args = set_training_args(model_name, 0.001, 1, 'epoch', 10, 4, 8, 8)\n",
    "\n",
    "    # create a trainer instance \n",
    "    trainer = set_trainer(model_name, training_args, train_ds, val_ds, tokenizer)\n",
    "\n",
    "    # Finally fine-tune the model and save it\n",
    "    train_and_save(trainer, model_name)\n",
    "\n",
    "def main():\n",
    "    model_name = 't5-base'\n",
    "    fine_tune_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 1.12.1\n",
      "Cuda version: 11.3\n",
      "Cudnn version: 8302\n",
      "Is cuda available: True\n",
      "Number of cuda devices: 1\n",
      "Current default device: 0\n",
      "First cuda device: <torch.cuda.device object at 0x0000015290B19188>\n",
      "Name of the first cuda device: NVIDIA GeForce GTX 1070\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Anaconda\\envs\\CudaSupEnv\\lib\\site-packages\\transformers\\models\\t5\\tokenization_t5_fast.py:166: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  FutureWarning,\n",
      "Using custom data configuration Cleaned_data-bcb014efcf526ad6\n",
      "Found cached dataset csv (C:/Users/Simon/.cache/huggingface/datasets/csv/Cleaned_data-bcb014efcf526ad6/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOGGING: preprocess_model DONE \n",
      "\n",
      "LOGGING: load_CACAPO_data DONE \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 599.53it/s]\n",
      "Parameter 'function'=<function preprocess_data at 0x0000015290BF2948> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]e:\\Anaconda\\envs\\CudaSupEnv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3543: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  \"`as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your \"\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOGGING: transform_datasets DONE \n",
      "\n",
      "LOGGING: set_training_args DONE \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cuda_amp half precision backend\n",
      "***** Running training *****\n",
      "  Num examples = 717\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOGGING: set_trainer DONE \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/22 [00:00<?, ?it/s]You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "  5%|â–         | 1/22 [00:06<02:14,  6.42s/it]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "  9%|â–‰         | 2/22 [00:12<02:02,  6.11s/it]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      " 14%|â–ˆâ–Ž        | 3/22 [00:18<01:52,  5.92s/it]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      " 18%|â–ˆâ–Š        | 4/22 [00:23<01:43,  5.76s/it]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      " 23%|â–ˆâ–ˆâ–Ž       | 5/22 [00:29<01:37,  5.73s/it]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      " 27%|â–ˆâ–ˆâ–‹       | 6/22 [00:34<01:31,  5.70s/it]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      " 32%|â–ˆâ–ˆâ–ˆâ–      | 7/22 [00:40<01:25,  5.69s/it]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      " 36%|â–ˆâ–ˆâ–ˆâ–‹      | 8/22 [00:46<01:19,  5.67s/it]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      " 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 9/22 [00:51<01:13,  5.68s/it]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      " 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 10/22 [00:57<01:08,  5.70s/it]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 11/22 [01:03<01:03,  5.74s/it]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      " 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 12/22 [01:09<00:57,  5.77s/it]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      " 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 13/22 [01:15<00:52,  5.81s/it]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      " 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 14/22 [01:21<00:46,  5.86s/it]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      " 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 15/22 [01:27<00:41,  5.91s/it]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      " 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 16/22 [01:33<00:35,  5.96s/it]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      " 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 17/22 [01:39<00:30,  6.00s/it]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      " 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 18/22 [01:45<00:24,  6.07s/it]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      " 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 19/22 [01:51<00:18,  6.10s/it]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      " 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 20/22 [01:57<00:12,  6.07s/it]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      " 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 21/22 [02:03<00:06,  6.05s/it]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22/22 [02:09<00:00,  6.05s/it]***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 8\n",
      "e:\\Anaconda\\envs\\CudaSupEnv\\lib\\site-packages\\ipykernel_launcher.py:78: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Simon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Simon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Simon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOGGING: load_eval_metrics DONE \n",
      "\n",
      "\n",
      " Calculating Blue\n",
      "\n",
      " Calculating rouge\n",
      "\n",
      " Calculating meteor\n",
      "\n",
      " Calculating perplexity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\Simon/.cache\\huggingface\\hub\\models--gpt2\\snapshots\\909a290700bd99135e67c64eefc166960b67cfd2\\config.json\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"gpt2\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\Simon/.cache\\huggingface\\hub\\models--gpt2\\snapshots\\909a290700bd99135e67c64eefc166960b67cfd2\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file config.json from cache at C:\\Users\\Simon/.cache\\huggingface\\hub\\models--gpt2\\snapshots\\909a290700bd99135e67c64eefc166960b67cfd2\\config.json\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"gpt2\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "loading file vocab.json from cache at C:\\Users\\Simon/.cache\\huggingface\\hub\\models--gpt2\\snapshots\\909a290700bd99135e67c64eefc166960b67cfd2\\vocab.json\n",
      "loading file merges.txt from cache at C:\\Users\\Simon/.cache\\huggingface\\hub\\models--gpt2\\snapshots\\909a290700bd99135e67c64eefc166960b67cfd2\\merges.txt\n",
      "loading file tokenizer.json from cache at C:\\Users\\Simon/.cache\\huggingface\\hub\\models--gpt2\\snapshots\\909a290700bd99135e67c64eefc166960b67cfd2\\tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at None\n",
      "loading configuration file config.json from cache at C:\\Users\\Simon/.cache\\huggingface\\hub\\models--gpt2\\snapshots\\909a290700bd99135e67c64eefc166960b67cfd2\\config.json\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"gpt2\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "Using pad_token, but it is not set yet.\n",
      "Assigning <|endoftext|> to the pad_token key of the tokenizer\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:00<00:00, 15.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Calculating bertscore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file config.json from cache at C:\\Users\\Simon/.cache\\huggingface\\hub\\models--roberta-large\\snapshots\\5069d8a2a32a7df4c69ef9b56348be04152a2341\\config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading file vocab.json from cache at C:\\Users\\Simon/.cache\\huggingface\\hub\\models--roberta-large\\snapshots\\5069d8a2a32a7df4c69ef9b56348be04152a2341\\vocab.json\n",
      "loading file merges.txt from cache at C:\\Users\\Simon/.cache\\huggingface\\hub\\models--roberta-large\\snapshots\\5069d8a2a32a7df4c69ef9b56348be04152a2341\\merges.txt\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at None\n",
      "loading configuration file config.json from cache at C:\\Users\\Simon/.cache\\huggingface\\hub\\models--roberta-large\\snapshots\\5069d8a2a32a7df4c69ef9b56348be04152a2341\\config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at C:\\Users\\Simon/.cache\\huggingface\\hub\\models--roberta-large\\snapshots\\5069d8a2a32a7df4c69ef9b56348be04152a2341\\config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\Simon/.cache\\huggingface\\hub\\models--roberta-large\\snapshots\\5069d8a2a32a7df4c69ef9b56348be04152a2341\\pytorch_model.bin\n",
      "All the weights of RobertaModel were initialized from the model checkpoint at roberta-large.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaModel for predictions without further training.\n",
      "                                               \n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22/22 [02:47<00:00,  6.05s/it]\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22/22 [02:47<00:00,  7.62s/it]\n",
      "Saving model checkpoint to ../Models/t5-base\n",
      "Configuration saved in ../Models/t5-base\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2718319892883301, 'eval_ blue_output': {'bleu': 0.07287266233295649, 'precisions': [0.5236593059936908, 0.2791970802919708, 0.16846652267818574, 0.10290237467018469], 'brevity_penalty': 0.32478051013616505, 'length_ratio': 0.4706755753526355, 'translation_length': 634, 'reference_length': 1347}, 'eval_\\n\\n rouge_output': {'rouge1': 0.4567588175569386, 'rouge2': 0.24496511777338179, 'rougeL': 0.3927351243033804, 'rougeLsum': 0.3917153867199384}, 'eval_\\n\\n meteor_results': {'meteor': 0.32032203156915295}, 'eval_\\n\\n perp_results': {'perplexities': [20.996877670288086, 31.80162811279297, 130.133056640625, 21.80083465576172, 314.10064697265625, 307.6853942871094, 43.46910858154297, 63.4985237121582, 48.2667350769043, 199.16017150878906, 239.6354522705078, 53.45401382446289, 188.5612335205078, 75.34443664550781, 110.3641586303711, 284.09051513671875, 114.99337768554688, 390.0467834472656, 2665.14013671875, 1136.6309814453125, 1509.013671875, 104.45072937011719, 2010.5133056640625, 76.3468017578125, 1926.1522216796875, 664.9722900390625, 1206.977783203125, 1185.1558837890625, 991.4528198242188, 16.020071029663086, 1238.5740966796875, 38.87626647949219, 545.3893432617188, 1604.9569091796875, 28.608476638793945, 83.50199127197266, 14.282769203186035, 1665.4847412109375, 176.82354736328125, 159.08753967285156, 89.33053588867188, 89.45543670654297, 276.90985107421875, 115.15469360351562, 956.9725952148438, 69.87200164794922, 399.2905578613281, 727.4180908203125, 1249.9595947265625, 89.87092590332031, 454.5294494628906, 2337.52587890625, 314.84228515625, 1294.7320556640625, 1200.519775390625, 857.1856079101562, 324.7657165527344, 108.14068603515625, 24.14096450805664, 25.23661231994629, 51.558868408203125, 25.34062957763672, 188.331298828125, 20.128673553466797, 19.144855499267578, 20.45799446105957, 7.4819440841674805, 1052.0972900390625, 172.7534637451172, 33.41905975341797, 148.3409881591797, 52.105709075927734, 497.089599609375, 205.56578063964844, 425.9937744140625, 108.30779266357422, 501.35992431640625, 775.1016235351562, 1077.8814697265625, 1446.1898193359375, 541.2808227539062, 450.0174255371094, 657.400634765625, 650.222412109375, 2298.227294921875, 488.68548583984375], 'mean_perplexity': 518.6762008888777}, 'eval_\\n\\n bertscore_output': {'precision': [0.9334926605224609, 0.9867274761199951, 0.9497483968734741, 0.9317595958709717, 0.9363487958908081, 0.8807483315467834, 0.9565529823303223, 0.9082018136978149, 0.9361961483955383, 0.9436284899711609, 0.9345843195915222, 0.886117696762085, 0.85140460729599, 0.8807675838470459, 0.9138625860214233, 0.8609277009963989, 0.8713804483413696, 0.8514418601989746, 0.9506270885467529, 0.9739816188812256, 0.8976988792419434, 0.8946523070335388, 0.9119625091552734, 0.8306175470352173, 0.9501349329948425, 0.9382335543632507, 0.9309945702552795, 0.955836832523346, 0.9619504809379578, 0.8395967483520508, 0.8362917900085449, 0.9408414363861084, 0.9087762832641602, 0.7933051586151123, 0.8620415329933167, 0.9913303852081299, 0.9302580952644348, 0.8095094561576843, 0.8830627202987671, 0.9688853025436401, 0.9248403310775757, 0.9465937614440918, 0.9060804843902588, 0.9449180364608765, 0.8932747840881348, 0.833507776260376, 0.8372359275817871, 0.9082487225532532, 0.8808529376983643, 0.8532302379608154, 0.9334450960159302, 0.9210249185562134, 0.9537563323974609, 0.9549156427383423, 0.9373179078102112, 0.9435626268386841, 0.886102557182312, 0.8384405374526978, 0.9553669691085815, 0.9719711542129517, 0.9236207008361816, 0.9172406196594238, 0.9579915404319763, 0.8834618926048279, 0.9425545334815979, 0.8997973203659058, 0.8375704884529114, 0.8856254816055298, 0.904437780380249, 0.8890112042427063, 0.9242208003997803, 0.9216917753219604, 0.9196035265922546, 0.8751572370529175, 0.868653416633606, 0.8776208162307739, 0.8214789032936096, 0.9284807443618774, 0.877949595451355, 0.8724187612533569, 0.9199093580245972, 0.8779720067977905, 0.9389526844024658, 0.9525954723358154, 0.8723451495170593, 0.8976577520370483], 'recall': [0.8597669005393982, 0.9602724313735962, 0.8684101700782776, 0.8740862607955933, 0.9104424118995667, 0.8223261833190918, 0.8870865106582642, 0.9145761132240295, 0.9219262599945068, 0.9383320808410645, 0.8193848133087158, 0.8275443315505981, 0.8077785968780518, 0.881622314453125, 0.8762801885604858, 0.8192566633224487, 0.832137405872345, 0.8107674717903137, 0.8658872842788696, 0.9102668166160583, 0.8166793584823608, 0.8132790327072144, 0.8847262263298035, 0.839950680732727, 0.8828811049461365, 0.8917273283004761, 0.8537813425064087, 0.9294474720954895, 0.8769849538803101, 0.8548102378845215, 0.8878375887870789, 0.8759994506835938, 0.8573113679885864, 0.8408504128456116, 0.8485056161880493, 0.9790033102035522, 0.875836193561554, 0.8614481091499329, 0.884982168674469, 0.9604138731956482, 0.9112257957458496, 0.9262008666992188, 0.8939111232757568, 0.9137450456619263, 0.8509553670883179, 0.770474910736084, 0.8302592039108276, 0.797909140586853, 0.8652451634407043, 0.8502786159515381, 0.9038858413696289, 0.8891481757164001, 0.7509706020355225, 0.8713668584823608, 0.8571003675460815, 0.8298360109329224, 0.8641035556793213, 0.7714150547981262, 0.9154694080352783, 0.9676810503005981, 0.8559122681617737, 0.8525500297546387, 0.8524994850158691, 0.9151381254196167, 0.917510986328125, 0.8476902842521667, 0.8258150219917297, 0.9311541318893433, 0.8965322971343994, 0.8302886486053467, 0.9064013957977295, 0.9334449768066406, 0.8686196804046631, 0.8365374803543091, 0.84766685962677, 0.8095632791519165, 0.7618783712387085, 0.8642277717590332, 0.8628331422805786, 0.8204893469810486, 0.8401605486869812, 0.8090953826904297, 0.8970136642456055, 0.8816356658935547, 0.8993181586265564, 0.9001201391220093], 'f1': [0.8951141834259033, 0.9733201861381531, 0.9072598814964294, 0.9020019769668579, 0.9232138991355896, 0.8505352139472961, 0.9205110669136047, 0.9113777875900269, 0.9290064573287964, 0.9409728050231934, 0.8732014298439026, 0.8558300137519836, 0.8290179967880249, 0.8811947107315063, 0.8946768641471863, 0.839575469493866, 0.8513069152832031, 0.8306070566177368, 0.9062806367874146, 0.941046953201294, 0.8552746772766113, 0.852027177810669, 0.8981378674507141, 0.8352580666542053, 0.9152742028236389, 0.9143895506858826, 0.8907176852226257, 0.9424574971199036, 0.9175048470497131, 0.8471351861953735, 0.861294150352478, 0.9072633981704712, 0.8822939395904541, 0.8163861632347107, 0.8552200198173523, 0.9851282835006714, 0.9022272229194641, 0.8346715569496155, 0.8840213418006897, 0.9646310210227966, 0.9179826378822327, 0.9362863302230835, 0.8999546766281128, 0.9290701150894165, 0.8716017007827759, 0.8007528185844421, 0.8337329626083374, 0.8495110273361206, 0.8729793429374695, 0.8517518639564514, 0.9184277057647705, 0.9048058986663818, 0.8403022885322571, 0.9112301468849182, 0.8954160809516907, 0.8830527067184448, 0.8749647736549377, 0.8035324811935425, 0.9349927306175232, 0.9698213934898376, 0.8884784579277039, 0.8837130069732666, 0.9021720886230469, 0.8990211486816406, 0.929864227771759, 0.8729669451713562, 0.8316512107849121, 0.9078193306922913, 0.9004676342010498, 0.8586471676826477, 0.9152243733406067, 0.9275311827659607, 0.8933848142623901, 0.85541170835495, 0.8580318093299866, 0.842219352722168, 0.7905568480491638, 0.8952027559280396, 0.8703257441520691, 0.8456576466560364, 0.8782282471656799, 0.8421276807785034, 0.9175041913986206, 0.9157429933547974, 0.8856263756752014, 0.8988872766494751], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.23.1)'}, 'eval_runtime': 37.9265, 'eval_samples_per_second': 2.268, 'eval_steps_per_second': 0.29, 'epoch': 0.98}\n",
      "{'train_runtime': 167.7283, 'train_samples_per_second': 4.275, 'train_steps_per_second': 0.131, 'train_loss': 2.4390553561123935, 'epoch': 0.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ../Models/t5-base\\pytorch_model.bin\n",
      "tokenizer config file saved in ../Models/t5-base\\tokenizer_config.json\n",
      "Special tokens file saved in ../Models/t5-base\\special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('CudaSupEnv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8ce04b5a4dcd593beec21f85756cfa75ba50da4e44d5dd0d068d93735596035b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
