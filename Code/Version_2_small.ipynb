{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import WebNLG_xmlReader.benchmark_reader as xml_reader\n",
    "import os.path\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Anaconda\\envs\\pytorch_test\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "e:\\Anaconda\\envs\\pytorch_test\\lib\\site-packages\\transformers\\models\\t5\\tokenization_t5_fast.py:166: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "# from transformers import AutoTokenizer, AutoModelForSeq2SeqLM \n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"t5-base\") #\"t5-base\"\n",
    "\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\") #TFAutoModelForSeq2SeqLM\n",
    "\n",
    "from transformers import AutoTokenizer, T5ForConditionalGeneration\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration Cleaned_data-94f25176065263b6\n",
      "Found cached dataset csv (C:/Users/Simon/.cache/huggingface/datasets/csv/Cleaned_data-94f25176065263b6/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a)\n",
      "100%|██████████| 3/3 [00:00<00:00, 999.04it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "#dataset = load_dataset(\"../Data/Cleaned_data/\", data_files=\"Train.csv\")\n",
    "full_dataset = load_dataset(\"../Data/Cleaned_data/\", data_files={\"train\": \"Train.csv\", \"dev\": \"Dev.csv\", \"test\": \"Test.csv\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zie https://github.com/NielsRogge/Transformers-Tutorials/blob/master/T5/Fine_tuning_Dutch_T5_base_on_CNN_Daily_Mail_for_summarization_(on_TPU_using_HuggingFace_Accelerate).ipynb\n",
    "\n",
    "def preprocess_data(data):\n",
    "    max_length = 256\n",
    "    RDFs = data[\"input\"]\n",
    "    texts = data[\"output\"]\n",
    "\n",
    "    model_inputs = tokenizer(RDFs, truncation=True, padding=\"max_length\", return_tensors='pt')\n",
    "\n",
    "    # specially for seq2seq tokenizer, \"Temporarily sets the tokenizer for encoding the targets. Useful for tokenizer associated to sequence-to-sequence models that need a slightly different processing for the labels.\"\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        target_texts = tokenizer(texts, padding=\"max_length\", truncation=True, return_tensors='pt').input_ids\n",
    "\n",
    "    #target_texts = tokenizer(texts, truncation=True, padding=\"max_length\").input_ids\n",
    "    \n",
    "    #model_inputs['decoder_input_ids'] = np.zeros((len(target_texts), 0))\n",
    "    model_inputs[\"labels\"] = target_texts\n",
    "    #print(f'target_texts    {target_texts}')\n",
    "\n",
    "    # ook nodig\n",
    "\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:/Users/Simon/.cache/huggingface/datasets/csv/Cleaned_data-94f25176065263b6/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a\\cache-2ce23b6802578bd2.arrow\n",
      "Loading cached processed dataset at C:/Users/Simon/.cache/huggingface/datasets/csv/Cleaned_data-94f25176065263b6/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a\\cache-efb5afb85a7e199f.arrow\n",
      "Loading cached processed dataset at C:/Users/Simon/.cache/huggingface/datasets/csv/Cleaned_data-94f25176065263b6/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a\\cache-9ea15b37dbe0b54e.arrow\n"
     ]
    }
   ],
   "source": [
    "encoded_train_ds = full_dataset[\"train\"].map(preprocess_data, batched=True, remove_columns=full_dataset[\"train\"].column_names)  \n",
    "encoded_dev_ds = full_dataset[\"dev\"].map(preprocess_data, batched=True, remove_columns=full_dataset[\"dev\"].column_names)\n",
    "encoded_test_ds = full_dataset[\"test\"].map(preprocess_data, batched=True, remove_columns=full_dataset[\"test\"].column_names)\n",
    "\n",
    "encoded_train_ds.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "encoded_dev_ds.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "encoded_test_ds.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create smaller dataset chunk\n",
    "small_train = encoded_train_ds.shard(num_shards = 8, index = 0)\n",
    "small_val = encoded_dev_ds.shard(num_shards = 8, index = 0)\n",
    "small_test = encoded_test_ds.shard(num_shards = 8, index = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Anaconda\\envs\\pytorch_test\\lib\\site-packages\\ipykernel_launcher.py:3: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "# Create functions for evaluating the predictions/generations\n",
    "import datasets\n",
    "metric = datasets.load_metric(\"bleu\")\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.split() for pred in preds]\n",
    "    labels = [[label.split()] for label in labels]\n",
    "    return preds, labels\n",
    "\n",
    "# def compute_metrics(pred):\n",
    "#     predictions, labels = pred\n",
    "#     if isinstance(predictions, tuple):\n",
    "#         predictions = predictions[0]\n",
    "#     decoded_preds = tokenizer.batch_decode(predictions,\n",
    "#     skip_special_tokens=True)\n",
    "\n",
    "#     # Replace -100 in the labels as we can't decode them.\n",
    "#     labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "#     decoded_labels = tokenizer.batch_decode(labels,\n",
    "#     skip_special_tokens=True)\n",
    "#     # Some simple post-processing\n",
    "#     decoded_preds, decoded_labels = postprocess_text(decoded_preds,\n",
    "#     decoded_labels)\n",
    "\n",
    "#     bleu_output = bleu.compute(predictions=decoded_preds,\n",
    "#     references=decoded_labels)\n",
    "#     return bleu_output\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    #print(f\"eval_preds   {eval_preds} \\n\\n\")\n",
    "    preds, labels = eval_preds #only has 2 values to unpack\n",
    "    \n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    \n",
    "    print(f\"preds   {preds} \\n\\n\")\n",
    "    print(f\"labels   {labels} \\n\\n\")\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    #if data_args.ignore_pad_token_for_loss:\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    #print(f\"decoded_preds   {decoded_preds} \\n\\n\")\n",
    "    #print(f\"decoded_labels   {decoded_labels} \\n\\n\")\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    print(f'results     {result}')\n",
    "\n",
    "    result = {\"bleu\": result[\"score\"]}\n",
    "\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    result = {k: round(v, 4) for k, v in result.items()}\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    learning_rate=0.001,\n",
    "    do_eval=True, # will be set to true if evaluation strategy is set\n",
    "    do_predict=True, #Whether to run predictions on the test set or not.\n",
    "    evaluation_strategy=\"steps\", \n",
    "    eval_steps= 100, # Number of update steps between two evaluations if evaluation_strategy=\"steps\". Will default to the same value as logging_steps if not set.\n",
    "    save_steps=200, # Number of updates steps before two checkpoint saves if save_strategy=\"steps\".\n",
    "    max_steps=10, # the total number of training steps to perform\n",
    "    save_total_limit= 10, # the maximum number of models to keep before deleting the oldest one\n",
    "    predict_with_generate=True, # Whether to use generate to calculate generative metrics (ROUGE, BLEU).\n",
    "    generation_num_beams=10,  #The num_beams to use on each evaluation loop when predict_with_generate=True. Will default to the num_beams value of the model configuration\n",
    "    gradient_checkpointing=True, #\n",
    "    gradient_accumulation_steps=1, #Number of updates steps to accumulate the gradients for, before performing a backward/update pass\n",
    "    per_device_train_batch_size=4, #The batch size per GPU/TPU core/CPU for training.\n",
    "    per_device_eval_batch_size=4, #The batch size per GPU/TPU core/CPU for evaluation.\n",
    "    optim=\"adafactor\" #The optimizer to use: adamw_hf, adamw_torch, adamw_apex_fused, or adafactor.\n",
    "    #fp16=True\n",
    ")\n",
    "\n",
    "#stepsize vergroten (net als medium post)\n",
    "#fp16 toepassen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer,  return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train,\n",
    "    eval_dataset= small_val,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 5734\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      " 10%|█         | 1/10 [00:02<00:18,  2.01s/it]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      " 20%|██        | 2/10 [00:03<00:15,  1.93s/it]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      " 30%|███       | 3/10 [00:08<00:21,  3.04s/it]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      " 40%|████      | 4/10 [00:14<00:27,  4.50s/it]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      " 50%|█████     | 5/10 [00:21<00:26,  5.32s/it]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      " 60%|██████    | 6/10 [00:28<00:23,  5.79s/it]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      " 70%|███████   | 7/10 [00:35<00:18,  6.10s/it]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      " 80%|████████  | 8/10 [00:41<00:12,  6.29s/it]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      " 90%|█████████ | 9/10 [00:48<00:06,  6.43s/it]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "100%|██████████| 10/10 [00:55<00:00,  6.47s/it]\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "100%|██████████| 10/10 [00:55<00:00,  5.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 55.2744, 'train_samples_per_second': 0.724, 'train_steps_per_second': 0.181, 'train_loss': 2.6537424087524415, 'epoch': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=10, training_loss=2.6537424087524415, metrics={'train_runtime': 55.2744, 'train_samples_per_second': 0.724, 'train_steps_per_second': 0.181, 'train_loss': 2.6537424087524415, 'epoch': 0.01})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 687\n",
      "  Batch size = 4\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preds   [[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]] \n",
      "\n",
      "\n",
      "labels   [[   71  2335  7532 ...     0     0     0]\n",
      " [21512     6     3 ...     0     0     0]\n",
      " [   71  4509   568 ...     0     0     0]\n",
      " ...\n",
      " [ 4603    49  6899 ...     0     0     0]\n",
      " [10511  4049     9 ...     0     0     0]\n",
      " [   86    20  6494 ...     0     0     0]] \n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "float division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_25336\\2475608523.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msmall_val\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#evaluate on validation set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtest_op\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msmall_test\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# predict on test set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_op\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mskip_special_tokens\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\Anaconda\\envs\\pytorch_test\\lib\\site-packages\\transformers\\trainer_seq2seq.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, eval_dataset, ignore_keys, metric_key_prefix, **gen_kwargs)\u001b[0m\n\u001b[0;32m     76\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gen_kwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgen_kwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meval_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_keys\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mignore_keys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetric_key_prefix\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmetric_key_prefix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m     def predict(\n",
      "\u001b[1;32me:\\Anaconda\\envs\\pytorch_test\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   2792\u001b[0m             \u001b[0mprediction_loss_only\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_metrics\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2793\u001b[0m             \u001b[0mignore_keys\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mignore_keys\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2794\u001b[1;33m             \u001b[0mmetric_key_prefix\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmetric_key_prefix\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2795\u001b[0m         )\n\u001b[0;32m   2796\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\Anaconda\\envs\\pytorch_test\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mevaluation_loop\u001b[1;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   3070\u001b[0m                 )\n\u001b[0;32m   3071\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3072\u001b[1;33m                 \u001b[0mmetrics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEvalPrediction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mall_preds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel_ids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mall_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3073\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3074\u001b[0m             \u001b[0mmetrics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_25336\\2575085267.py\u001b[0m in \u001b[0;36mcompute_metrics\u001b[1;34m(eval_preds)\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[1;31m#print(f\"decoded_labels   {decoded_labels} \\n\\n\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmetric\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdecoded_preds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreferences\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdecoded_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'results     {result}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\Anaconda\\envs\\pytorch_test\\lib\\site-packages\\datasets\\metric.py\u001b[0m in \u001b[0;36mcompute\u001b[1;34m(self, predictions, references, **kwargs)\u001b[0m\n\u001b[0;32m    451\u001b[0m             \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0minput_name\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0minput_name\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0minput_name\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    452\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mtemp_seed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 453\u001b[1;33m                 \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcompute_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    454\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    455\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuf_writer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.cache\\huggingface\\modules\\datasets_modules\\metrics\\bleu\\961010342f0298f3e92cac3366046418f6eb9c89e6505f4803bbb2c73d3ae6a7\\bleu.py\u001b[0m in \u001b[0;36m_compute\u001b[1;34m(self, predictions, references, max_order, smooth)\u001b[0m\n\u001b[0;32m    114\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_compute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreferences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_order\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msmooth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m         score = compute_bleu(\n\u001b[1;32m--> 116\u001b[1;33m             \u001b[0mreference_corpus\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreferences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtranslation_corpus\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_order\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_order\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msmooth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msmooth\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    117\u001b[0m         )\n\u001b[0;32m    118\u001b[0m         \u001b[1;33m(\u001b[0m\u001b[0mbleu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprecisions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mratio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtranslation_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreference_length\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.cache\\huggingface\\modules\\datasets_modules\\metrics\\bleu\\961010342f0298f3e92cac3366046418f6eb9c89e6505f4803bbb2c73d3ae6a7\\nmt_bleu.py\u001b[0m in \u001b[0;36mcompute_bleu\u001b[1;34m(reference_corpus, translation_corpus, max_order, smooth)\u001b[0m\n\u001b[0;32m    106\u001b[0m     \u001b[0mbp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m     \u001b[0mbp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1.\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mratio\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m   \u001b[0mbleu\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgeo_mean\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mbp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: float division by zero"
     ]
    }
   ],
   "source": [
    "trainer.evaluate(small_val) #evaluate on validation set\n",
    "test_op = trainer.predict(small_test) # predict on test set\n",
    "print(tokenizer.decode(test_op[0][1],skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### future evals\n",
    "1. https://github.com/WanzhengZhu/GRUEN\n",
    "2. https://github.com/ufal/nlgi_eval\n",
    "3. https://github.com/google-research/bleurt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.is_tensor(small_train['input_ids'][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [10:23<00:00, 62.39s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_train_ds\n",
    "torch.is_tensor(encoded_train_ds['input_ids'][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 45870\n",
       "})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_train_ds.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.is_tensor(encoded_train_ds['input_ids'][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"['accidentAddress | Alberto’s_Mexican_Food,_1925_West_Holt_Ave.,_east_of_Fairplex_Drive', 'accidentDate | just_after_6:40_p.m.','shootingType | shooting']</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(small_train[3]['input_ids'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The shooting took place just after 6:40 p.m. at Alberto’s Mexican Food, 1925 West Holt Ave., east of Fairplex Drive, Pomona police Sgt. Steve Congalton said in a written statement.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(small_train[3]['labels'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Anaconda\\envs\\pytorch_test\\lib\\site-packages\\ipykernel_launcher.py:3: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "# pip install datasets\n",
    "import datasets\n",
    "bleu = datasets.load_metric(\"bleu\")\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.split() for pred in preds]\n",
    "    labels = [[label.split()] for label in labels]\n",
    "    return preds, labels\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    predictions, labels = pred\n",
    "    if isinstance(predictions, tuple):\n",
    "        predictions = predictions[0]\n",
    "\n",
    "    print(f'predictions[0]     {predictions[0]}\\n\\n')\n",
    "    print(f'labels[0]     {labels[0]}\\n\\n')\n",
    "     \n",
    "    decoded_preds = tokenizer.batch_decode(predictions,\n",
    "    skip_special_tokens=True)\n",
    "\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels,\n",
    "    skip_special_tokens=True)\n",
    "    # Some simple post-processing\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds,\n",
    "    decoded_labels)\n",
    "\n",
    "    bleu_output = bleu.compute(predictions=decoded_preds,\n",
    "    references=decoded_labels)\n",
    "    return bleu_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "model_name = \"t5-base-story-generation\"\n",
    "model_dir = f\"../Results/{model_name}\"\n",
    "\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=model_dir,\n",
    "    learning_rate=0.001,\n",
    "    do_eval=True, # will be set to true if evaluation strategy is set\n",
    "    do_predict=True, #Whether to run predictions on the test set or not.\n",
    "    evaluation_strategy=\"steps\", \n",
    "    eval_steps= 10, # Number of update steps between two evaluations if evaluation_strategy=\"steps\". Will default to the same value as logging_steps if not set.\n",
    "    save_steps=200, # Number of updates steps before two checkpoint saves if save_strategy=\"steps\".\n",
    "    max_steps=100, # the total number of training steps to perform\n",
    "    save_total_limit= 10, # the maximum number of models to keep before deleting the oldest one\n",
    "    predict_with_generate=True, # Whether to use generate to calculate generative metrics (ROUGE, BLEU).\n",
    "    generation_num_beams=10,  #The num_beams to use on each evaluation loop when predict_with_generate=True. Will default to the num_beams value of the model configuration\n",
    "    gradient_checkpointing=True, #\n",
    "    gradient_accumulation_steps=1, #Number of updates steps to accumulate the gradients for, before performing a backward/update pass\n",
    "    per_device_train_batch_size=4, #The batch size per GPU/TPU core/CPU for training.\n",
    "    per_device_eval_batch_size=4, #The batch size per GPU/TPU core/CPU for evaluation.\n",
    "    optim=\"adafactor\", #The optimizer to use: adamw_hf, adamw_torch, adamw_apex_fused, or adafactor.\n",
    "    report_to=\"tensorboard\"\n",
    "\n",
    "    #fp16=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\Simon/.cache\\huggingface\\hub\\models--t5-base\\snapshots\\23aa4f41cb7c08d4b05c8f327b22bfa0eb8c7ad9\\config.json\n",
      "Model config T5Config {\n",
      "  \"architectures\": [\n",
      "    \"T5WithLMHeadModel\"\n",
      "  ],\n",
      "  \"d_ff\": 3072,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.22.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\Simon/.cache\\huggingface\\hub\\models--t5-base\\snapshots\\23aa4f41cb7c08d4b05c8f327b22bfa0eb8c7ad9\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at t5-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "def model_init():\n",
    "    \"\"\"\n",
    "    Simple function to ensure that a new model is used for finetuning\n",
    "    \"\"\"\n",
    "    return T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "model_init=model_init,\n",
    "#model=model,\n",
    "args=training_args,\n",
    "train_dataset=small_train,\n",
    "eval_dataset=small_val,\n",
    "compute_metrics=compute_metrics,\n",
    "tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# max_steps is given, it will override any value given in\n",
    "# num_train_epochs\n",
    "# trainer.train(resume_from_checkpoint=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 3068), started 0:14:00 ago. (Use '!kill 3068' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-1c80317fa3b1799d\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-1c80317fa3b1799d\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Start TensorBoard before training to monitor it in progress\n",
    "\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir '{model_dir}'/runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\Simon/.cache\\huggingface\\hub\\models--t5-base\\snapshots\\23aa4f41cb7c08d4b05c8f327b22bfa0eb8c7ad9\\config.json\n",
      "Model config T5Config {\n",
      "  \"architectures\": [\n",
      "    \"T5WithLMHeadModel\"\n",
      "  ],\n",
      "  \"d_ff\": 3072,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.22.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\Simon/.cache\\huggingface\\hub\\models--t5-base\\snapshots\\23aa4f41cb7c08d4b05c8f327b22bfa0eb8c7ad9\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at t5-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
      "***** Running training *****\n",
      "  Num examples = 5734\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 100\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "  1%|          | 1/100 [00:02<03:30,  2.13s/it]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "  2%|▏         | 2/100 [00:03<03:09,  1.93s/it]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "  3%|▎         | 3/100 [00:05<03:01,  1.87s/it]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "  4%|▍         | 4/100 [00:07<02:56,  1.84s/it]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "  5%|▌         | 5/100 [00:09<02:52,  1.82s/it]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "  6%|▌         | 6/100 [00:11<02:49,  1.80s/it]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "  7%|▋         | 7/100 [00:12<02:46,  1.79s/it]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "  8%|▊         | 8/100 [00:14<02:44,  1.78s/it]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "  9%|▉         | 9/100 [00:16<02:41,  1.78s/it]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      " 10%|█         | 10/100 [00:18<02:39,  1.78s/it]***** Running Evaluation *****\n",
      "  Num examples = 687\n",
      "  Batch size = 4\n",
      "Trainer is attempting to log a value of \"[0.40294840294840295, 0.23333333333333334, 0.1440972222222222, 0.09745762711864407]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "                                                \n",
      " 10%|█         | 10/100 [06:46<02:39,  1.78s/it] `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions[0]     [    0 32099   636   254   485  1561  1561  1561  1561  1561  1561  1561\n",
      "  1561  1561  1561  1561  1561  1561  1561  1561]\n",
      "\n",
      "\n",
      "labels[0]     [   71  2335  7532   116     3     9  4740   348  2946  1472    30     3\n",
      "     9  2201  1583  3591  1088    44     3     9   636   896 28354  1561\n",
      "    16  1186  5132     3     9  9953    30  2875     3 14655     8  4740\n",
      "   348    22     7   384   225    43   801     3    88    47     3     9\n",
      "   105 12539    53    97  6417  1239     1     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0]\n",
      "\n",
      "\n",
      "{'eval_loss': 0.2006199210882187, 'eval_bleu': 4.4649701480079035e-07, 'eval_precisions': [0.40294840294840295, 0.23333333333333334, 0.1440972222222222, 0.09745762711864407], 'eval_brevity_penalty': 2.342308762499984e-06, 'eval_length_ratio': 0.07161080320225213, 'eval_translation_length': 814, 'eval_reference_length': 11367, 'eval_runtime': 387.865, 'eval_samples_per_second': 1.771, 'eval_steps_per_second': 0.443, 'epoch': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 11/100 [06:47<2:58:48, 120.54s/it]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      " 12%|█▏        | 12/100 [06:49<2:03:52, 84.47s/it] `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      " 13%|█▎        | 13/100 [06:51<1:26:13, 59.47s/it]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      " 14%|█▍        | 14/100 [06:53<1:00:20, 42.09s/it]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      " 15%|█▌        | 15/100 [06:55<42:29, 29.99s/it]  `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      " 16%|█▌        | 16/100 [06:57<30:10, 21.55s/it]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      " 17%|█▋        | 17/100 [06:59<21:40, 15.67s/it]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      " 18%|█▊        | 18/100 [07:01<15:48, 11.57s/it]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      " 19%|█▉        | 19/100 [07:03<11:45,  8.71s/it]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      " 20%|██        | 20/100 [07:05<08:55,  6.70s/it]***** Running Evaluation *****\n",
      "  Num examples = 687\n",
      "  Batch size = 4\n",
      "Trainer is attempting to log a value of \"[0.42515952597994533, 0.19104166666666667, 0.09380315917375456, 0.05276967930029155]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "                                                \n",
      " 20%|██        | 20/100 [13:40<08:55,  6.70s/it] `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions[0]     [   0  636  254  485 1561 1561   47 7532   16   46 3125   24 7532    3\n",
      "    9 2335   16 1186    5    1]\n",
      "\n",
      "\n",
      "labels[0]     [   71  2335  7532   116     3     9  4740   348  2946  1472    30     3\n",
      "     9  2201  1583  3591  1088    44     3     9   636   896 28354  1561\n",
      "    16  1186  5132     3     9  9953    30  2875     3 14655     8  4740\n",
      "   348    22     7   384   225    43   801     3    88    47     3     9\n",
      "   105 12539    53    97  6417  1239     1     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0]\n",
      "\n",
      "\n",
      "{'eval_loss': 0.17675265669822693, 'eval_bleu': 0.04845539152511083, 'eval_precisions': [0.42515952597994533, 0.19104166666666667, 0.09380315917375456, 0.05276967930029155], 'eval_brevity_penalty': 0.34219339611027755, 'eval_length_ratio': 0.4825371689979766, 'eval_translation_length': 5485, 'eval_reference_length': 11367, 'eval_runtime': 394.9975, 'eval_samples_per_second': 1.739, 'eval_steps_per_second': 0.435, 'epoch': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 21/100 [13:42<2:43:03, 123.84s/it]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      " 22%|██▏       | 22/100 [13:44<1:53:25, 87.25s/it] `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      " 23%|██▎       | 23/100 [13:46<1:19:07, 61.66s/it]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      " 24%|██▍       | 24/100 [13:48<55:25, 43.76s/it]  `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      " 25%|██▌       | 25/100 [13:50<39:01, 31.21s/it]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      " 26%|██▌       | 26/100 [13:52<27:40, 22.44s/it]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      " 27%|██▋       | 27/100 [13:54<19:49, 16.30s/it]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      " 28%|██▊       | 28/100 [13:56<14:24, 12.01s/it]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      " 29%|██▉       | 29/100 [13:58<10:38,  9.00s/it]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      " 30%|███       | 30/100 [14:00<08:01,  6.88s/it]***** Running Evaluation *****\n",
      "  Num examples = 687\n",
      "  Batch size = 4\n",
      "Trainer is attempting to log a value of \"[0.4706676958703203, 0.23061541879582315, 0.12963930998431783, 0.07572877059569075]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "\n",
      " 30%|███       | 30/100 [20:31<08:01,  6.88s/it] `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions[0]     [   0  636  896  975 5012   77 2552 1561   47 7532   16 1186    5    1\n",
      "    0    0    0    0    0    0]\n",
      "\n",
      "\n",
      "labels[0]     [   71  2335  7532   116     3     9  4740   348  2946  1472    30     3\n",
      "     9  2201  1583  3591  1088    44     3     9   636   896 28354  1561\n",
      "    16  1186  5132     3     9  9953    30  2875     3 14655     8  4740\n",
      "   348    22     7   384   225    43   801     3    88    47     3     9\n",
      "   105 12539    53    97  6417  1239     1     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0]\n",
      "\n",
      "\n",
      "{'eval_loss': 0.16017968952655792, 'eval_bleu': 0.054770399089737895, 'eval_precisions': [0.4706676958703203, 0.23061541879582315, 0.12963930998431783, 0.07572877059569075], 'eval_brevity_penalty': 0.30314179515788087, 'eval_length_ratio': 0.45588105920647487, 'eval_translation_length': 5182, 'eval_reference_length': 11367, 'eval_runtime': 391.1174, 'eval_samples_per_second': 1.757, 'eval_steps_per_second': 0.44, 'epoch': 0.02}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 31/100 [20:33<2:21:07, 122.72s/it]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      " 32%|███▏      | 32/100 [20:35<1:38:00, 86.47s/it] `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      " 33%|███▎      | 33/100 [20:37<1:08:13, 61.10s/it]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      " 34%|███▍      | 34/100 [20:39<47:40, 43.35s/it]  `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      " 35%|███▌      | 35/100 [20:41<33:29, 30.91s/it]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      " 36%|███▌      | 36/100 [20:42<23:41, 22.21s/it]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      " 37%|███▋      | 37/100 [20:44<16:57, 16.15s/it]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      " 38%|███▊      | 38/100 [20:46<12:17, 11.90s/it]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      " 39%|███▉      | 39/100 [20:48<09:04,  8.93s/it]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      " 40%|████      | 40/100 [20:50<06:51,  6.85s/it]***** Running Evaluation *****\n",
      "  Num examples = 687\n",
      "  Batch size = 4\n",
      "Trainer is attempting to log a value of \"[0.4481193255512322, 0.22149242838898012, 0.11931581143095536, 0.0679327976625274]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "\n",
      " 40%|████      | 40/100 [27:33<06:51,  6.85s/it] `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions[0]     [   0   86 1186    6    3    9 2335   47 7532   16    8  636  896  975\n",
      " 5012   77 2552 1561    5    1]\n",
      "\n",
      "\n",
      "labels[0]     [   71  2335  7532   116     3     9  4740   348  2946  1472    30     3\n",
      "     9  2201  1583  3591  1088    44     3     9   636   896 28354  1561\n",
      "    16  1186  5132     3     9  9953    30  2875     3 14655     8  4740\n",
      "   348    22     7   384   225    43   801     3    88    47     3     9\n",
      "   105 12539    53    97  6417  1239     1     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0]\n",
      "\n",
      "\n",
      "{'eval_loss': 0.15655165910720825, 'eval_bleu': 0.07249636019651173, 'eval_precisions': [0.4481193255512322, 0.22149242838898012, 0.11931581143095536, 0.0679327976625274], 'eval_brevity_penalty': 0.4304608789835497, 'eval_length_ratio': 0.5426233834784904, 'eval_translation_length': 6168, 'eval_reference_length': 11367, 'eval_runtime': 403.0251, 'eval_samples_per_second': 1.705, 'eval_steps_per_second': 0.427, 'epoch': 0.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 41/100 [27:36<2:04:13, 126.33s/it]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      " 42%|████▏     | 42/100 [27:38<1:26:05, 89.06s/it] `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      " 43%|████▎     | 43/100 [27:40<59:49, 62.98s/it]  `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      " 44%|████▍     | 44/100 [27:42<41:45, 44.73s/it]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      " 45%|████▌     | 45/100 [27:44<29:18, 31.97s/it]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      " 46%|████▌     | 46/100 [27:46<20:41, 22.99s/it]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      " 47%|████▋     | 47/100 [27:48<14:44, 16.69s/it]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      " 48%|████▊     | 48/100 [27:50<10:41, 12.33s/it]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      " 49%|████▉     | 49/100 [27:52<07:52,  9.27s/it]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      " 50%|█████     | 50/100 [27:54<05:54,  7.09s/it]***** Running Evaluation *****\n",
      "  Num examples = 687\n",
      "  Batch size = 4\n",
      "Trainer is attempting to log a value of \"[0.5432883240667196, 0.2835134513681306, 0.1513911620294599, 0.08380634390651086]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "                                                \n",
      " 50%|█████     | 50/100 [34:28<05:54,  7.09s/it] `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions[0]     [   0  636  896  975 5012   77 2552 1561   47 7532   16 1186    5    1\n",
      "    0    0    0    0    0    0]\n",
      "\n",
      "\n",
      "labels[0]     [   71  2335  7532   116     3     9  4740   348  2946  1472    30     3\n",
      "     9  2201  1583  3591  1088    44     3     9   636   896 28354  1561\n",
      "    16  1186  5132     3     9  9953    30  2875     3 14655     8  4740\n",
      "   348    22     7   384   225    43   801     3    88    47     3     9\n",
      "   105 12539    53    97  6417  1239     1     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0]\n",
      "\n",
      "\n",
      "{'eval_loss': 0.15215498208999634, 'eval_bleu': 0.05980984744667501, 'eval_precisions': [0.5432883240667196, 0.2835134513681306, 0.1513911620294599, 0.08380634390651086], 'eval_brevity_penalty': 0.2844640115572309, 'eval_length_ratio': 0.4430368610891176, 'eval_translation_length': 5036, 'eval_reference_length': 11367, 'eval_runtime': 393.9234, 'eval_samples_per_second': 1.744, 'eval_steps_per_second': 0.437, 'epoch': 0.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 51/100 [34:30<1:41:02, 123.72s/it]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      " 52%|█████▏    | 52/100 [34:32<1:09:45, 87.19s/it] `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      " 53%|█████▎    | 53/100 [34:34<48:16, 61.62s/it]  `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      " 54%|█████▍    | 54/100 [34:36<33:30, 43.71s/it]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      " 55%|█████▌    | 55/100 [34:38<23:22, 31.17s/it]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      " 56%|█████▌    | 56/100 [34:40<16:25, 22.39s/it]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      " 57%|█████▋    | 57/100 [34:42<11:38, 16.25s/it]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      " 58%|█████▊    | 58/100 [34:44<08:21, 11.95s/it]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      " 59%|█████▉    | 59/100 [34:46<06:06,  8.94s/it]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      " 60%|██████    | 60/100 [34:48<04:33,  6.83s/it]***** Running Evaluation *****\n",
      "  Num examples = 687\n",
      "  Batch size = 4\n",
      "Trainer is attempting to log a value of \"[0.5169156792031598, 0.2692757009345794, 0.1512019770838014, 0.09180153886972672]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions[0]     [   0  636  896  975 5012   77 2552 1561   16 1186 7532    3    9 2335\n",
      "    5    1    0    0    0    0]\n",
      "\n",
      "\n",
      "labels[0]     [   71  2335  7532   116     3     9  4740   348  2946  1472    30     3\n",
      "     9  2201  1583  3591  1088    44     3     9   636   896 28354  1561\n",
      "    16  1186  5132     3     9  9953    30  2875     3 14655     8  4740\n",
      "   348    22     7   384   225    43   801     3    88    47     3     9\n",
      "   105 12539    53    97  6417  1239     1     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \n",
      " 60%|██████    | 60/100 [41:12<04:33,  6.83s/it] `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.14757631719112396, 'eval_bleu': 0.08091334193474949, 'eval_precisions': [0.5169156792031598, 0.2692757009345794, 0.1512019770838014, 0.09180153886972672], 'eval_brevity_penalty': 0.3859349089797438, 'eval_length_ratio': 0.5122723673792557, 'eval_translation_length': 5823, 'eval_reference_length': 11367, 'eval_runtime': 384.8829, 'eval_samples_per_second': 1.785, 'eval_steps_per_second': 0.447, 'epoch': 0.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 61/100 [41:14<1:18:31, 120.81s/it]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      " 62%|██████▏   | 62/100 [41:16<53:55, 85.14s/it]   `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      " 63%|██████▎   | 63/100 [41:18<37:06, 60.17s/it]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      " 64%|██████▍   | 64/100 [41:20<25:36, 42.69s/it]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      " 65%|██████▌   | 65/100 [41:22<17:46, 30.46s/it]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      " 66%|██████▌   | 66/100 [41:24<12:24, 21.90s/it]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      " 67%|██████▋   | 67/100 [41:26<08:44, 15.90s/it]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      " 68%|██████▊   | 68/100 [41:28<06:14, 11.70s/it]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      " 69%|██████▉   | 69/100 [41:30<04:31,  8.76s/it]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      " 70%|███████   | 70/100 [41:32<03:21,  6.71s/it]***** Running Evaluation *****\n",
      "  Num examples = 687\n",
      "  Batch size = 4\n",
      "Trainer is attempting to log a value of \"[0.49973568281938324, 0.25851703406813625, 0.14395170652426284, 0.08514742353265363]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "\n",
      " 70%|███████   | 70/100 [47:55<03:21,  6.71s/it] `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions[0]     [   0   86 1186    6    3    9 2335   47 7532   16    8  636  896  975\n",
      " 5012   77 2552 1561    5    1]\n",
      "\n",
      "\n",
      "labels[0]     [   71  2335  7532   116     3     9  4740   348  2946  1472    30     3\n",
      "     9  2201  1583  3591  1088    44     3     9   636   896 28354  1561\n",
      "    16  1186  5132     3     9  9953    30  2875     3 14655     8  4740\n",
      "   348    22     7   384   225    43   801     3    88    47     3     9\n",
      "   105 12539    53    97  6417  1239     1     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0]\n",
      "\n",
      "\n",
      "{'eval_loss': 0.14391620457172394, 'eval_bleu': 0.07316594923933119, 'eval_precisions': [0.49973568281938324, 0.25851703406813625, 0.14395170652426284, 0.08514742353265363], 'eval_brevity_penalty': 0.3667790724144765, 'eval_length_ratio': 0.49925222134248265, 'eval_translation_length': 5675, 'eval_reference_length': 11367, 'eval_runtime': 383.6035, 'eval_samples_per_second': 1.791, 'eval_steps_per_second': 0.448, 'epoch': 0.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 71/100 [47:57<58:10, 120.35s/it]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      " 72%|███████▏  | 72/100 [47:59<39:34, 84.81s/it] `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      " 73%|███████▎  | 73/100 [48:01<26:58, 59.95s/it]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      " 74%|███████▍  | 74/100 [48:03<18:26, 42.56s/it]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      " 75%|███████▌  | 75/100 [48:05<12:39, 30.37s/it]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      " 76%|███████▌  | 76/100 [48:07<08:44, 21.84s/it]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      " 77%|███████▋  | 77/100 [48:09<06:04, 15.86s/it]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      " 78%|███████▊  | 78/100 [48:11<04:16, 11.67s/it]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      " 79%|███████▉  | 79/100 [48:12<03:03,  8.74s/it]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      " 80%|████████  | 80/100 [48:14<02:13,  6.69s/it]***** Running Evaluation *****\n",
      "  Num examples = 687\n",
      "  Batch size = 4\n",
      "Trainer is attempting to log a value of \"[0.5320135544854646, 0.2766260162601626, 0.15517241379310345, 0.08980855855855856]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "\n",
      " 80%|████████  | 80/100 [54:38<02:13,  6.69s/it] `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions[0]     [   0   86 1186    6    3    9 2335   47 7532   16    8  636  896  975\n",
      " 5012   77 2552 1561    5    1]\n",
      "\n",
      "\n",
      "labels[0]     [   71  2335  7532   116     3     9  4740   348  2946  1472    30     3\n",
      "     9  2201  1583  3591  1088    44     3     9   636   896 28354  1561\n",
      "    16  1186  5132     3     9  9953    30  2875     3 14655     8  4740\n",
      "   348    22     7   384   225    43   801     3    88    47     3     9\n",
      "   105 12539    53    97  6417  1239     1     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0]\n",
      "\n",
      "\n",
      "{'eval_loss': 0.14230963587760925, 'eval_bleu': 0.07618014171506018, 'eval_precisions': [0.5320135544854646, 0.2766260162601626, 0.15517241379310345, 0.08980855855855856], 'eval_brevity_penalty': 0.3579767211479077, 'eval_length_ratio': 0.4932699920823436, 'eval_translation_length': 5607, 'eval_reference_length': 11367, 'eval_runtime': 383.4634, 'eval_samples_per_second': 1.792, 'eval_steps_per_second': 0.449, 'epoch': 0.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 81/100 [54:40<38:05, 120.30s/it]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      " 82%|████████▏ | 82/100 [54:42<25:26, 84.78s/it] `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      " 83%|████████▎ | 83/100 [54:44<16:58, 59.92s/it]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      " 84%|████████▍ | 84/100 [54:45<11:20, 42.52s/it]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      " 85%|████████▌ | 85/100 [54:47<07:35, 30.34s/it]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      " 86%|████████▌ | 86/100 [54:49<05:05, 21.82s/it]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      " 87%|████████▋ | 87/100 [54:51<03:26, 15.86s/it]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      " 88%|████████▊ | 88/100 [54:53<02:20, 11.69s/it]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      " 89%|████████▉ | 89/100 [54:55<01:36,  8.75s/it]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      " 90%|█████████ | 90/100 [54:57<01:07,  6.70s/it]***** Running Evaluation *****\n",
      "  Num examples = 687\n",
      "  Batch size = 4\n",
      "Trainer is attempting to log a value of \"[0.5533476548514142, 0.29822412737293325, 0.17172675521821631, 0.10285391353489687]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "\n",
      " 90%|█████████ | 90/100 [1:01:21<01:07,  6.70s/it]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions[0]     [   0   86 1186    6    3    9 2335   47 7532   16    8  636  896  975\n",
      " 5012   77 2552 1561    5    1]\n",
      "\n",
      "\n",
      "labels[0]     [   71  2335  7532   116     3     9  4740   348  2946  1472    30     3\n",
      "     9  2201  1583  3591  1088    44     3     9   636   896 28354  1561\n",
      "    16  1186  5132     3     9  9953    30  2875     3 14655     8  4740\n",
      "   348    22     7   384   225    43   801     3    88    47     3     9\n",
      "   105 12539    53    97  6417  1239     1     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0]\n",
      "\n",
      "\n",
      "{'eval_loss': 0.14118291437625885, 'eval_bleu': 0.08254576010087311, 'eval_precisions': [0.5533476548514142, 0.29822412737293325, 0.17172675521821631, 0.10285391353489687], 'eval_brevity_penalty': 0.355258814459627, 'eval_length_ratio': 0.49142253892847715, 'eval_translation_length': 5586, 'eval_reference_length': 11367, 'eval_runtime': 383.723, 'eval_samples_per_second': 1.79, 'eval_steps_per_second': 0.448, 'epoch': 0.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 91/100 [1:01:23<18:03, 120.38s/it]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      " 92%|█████████▏| 92/100 [1:01:25<11:18, 84.84s/it] `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      " 93%|█████████▎| 93/100 [1:01:27<06:59, 59.96s/it]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      " 94%|█████████▍| 94/100 [1:01:28<04:15, 42.55s/it]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      " 95%|█████████▌| 95/100 [1:01:30<02:31, 30.37s/it]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      " 96%|█████████▌| 96/100 [1:01:32<01:27, 21.85s/it]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      " 97%|█████████▋| 97/100 [1:01:34<00:47, 15.88s/it]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      " 98%|█████████▊| 98/100 [1:01:36<00:23, 11.69s/it]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      " 99%|█████████▉| 99/100 [1:01:38<00:08,  8.75s/it]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "100%|██████████| 100/100 [1:01:40<00:00,  6.70s/it]***** Running Evaluation *****\n",
      "  Num examples = 687\n",
      "  Batch size = 4\n",
      "Trainer is attempting to log a value of \"[0.5570792520035619, 0.29829545454545453, 0.17153628652214892, 0.10201793721973094]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "\n",
      "100%|██████████| 100/100 [1:08:05<00:00,  6.70s/it]\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "100%|██████████| 100/100 [1:08:05<00:00, 40.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions[0]     [   0   86 1186    6    3    9 2335   47 7532   16    8  636  896  975\n",
      " 5012   77 2552 1561    5    1]\n",
      "\n",
      "\n",
      "labels[0]     [   71  2335  7532   116     3     9  4740   348  2946  1472    30     3\n",
      "     9  2201  1583  3591  1088    44     3     9   636   896 28354  1561\n",
      "    16  1186  5132     3     9  9953    30  2875     3 14655     8  4740\n",
      "   348    22     7   384   225    43   801     3    88    47     3     9\n",
      "   105 12539    53    97  6417  1239     1     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0]\n",
      "\n",
      "\n",
      "{'eval_loss': 0.14080317318439484, 'eval_bleu': 0.08336969970087459, 'eval_precisions': [0.5570792520035619, 0.29829545454545453, 0.17153628652214892, 0.10201793721973094], 'eval_brevity_penalty': 0.3590121915096162, 'eval_length_ratio': 0.49397378376000706, 'eval_translation_length': 5615, 'eval_reference_length': 11367, 'eval_runtime': 384.7666, 'eval_samples_per_second': 1.785, 'eval_steps_per_second': 0.447, 'epoch': 0.07}\n",
      "{'train_runtime': 4085.342, 'train_samples_per_second': 0.098, 'train_steps_per_second': 0.024, 'train_loss': 0.4066755294799805, 'epoch': 0.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=100, training_loss=0.4066755294799805, metrics={'train_runtime': 4085.342, 'train_samples_per_second': 0.098, 'train_steps_per_second': 0.024, 'train_loss': 0.4066755294799805, 'epoch': 0.07})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 687\n",
      "  Batch size = 4\n",
      "100%|██████████| 172/172 [06:08<00:00,  1.96s/it]Trainer is attempting to log a value of \"[0.5570792520035619, 0.29829545454545453, 0.17153628652214892, 0.10201793721973094]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "100%|██████████| 172/172 [06:08<00:00,  2.15s/it]\n",
      "***** Running Prediction *****\n",
      "  Num examples = 1136\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions[0]     [   0   86 1186    6    3    9 2335   47 7532   16    8  636  896  975\n",
      " 5012   77 2552 1561    5    1]\n",
      "\n",
      "\n",
      "labels[0]     [   71  2335  7532   116     3     9  4740   348  2946  1472    30     3\n",
      "     9  2201  1583  3591  1088    44     3     9   636   896 28354  1561\n",
      "    16  1186  5132     3     9  9953    30  2875     3 14655     8  4740\n",
      "   348    22     7   384   225    43   801     3    88    47     3     9\n",
      "   105 12539    53    97  6417  1239     1     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 284/284 [10:27<00:00,  2.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions[0]     [    0    37  9674  5262  6935  8521    16 26181  8018     5     1     0\n",
      "     0     0     0     0     0     0     0     0]\n",
      "\n",
      "\n",
      "labels[0]     [   71  1207  2870  5542    16     3     9  9674  5262  8521    16 26181\n",
      "  8018     5     1     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0]\n",
      "\n",
      "\n",
      "A 19-year-old man was arrested in connection with the 19-year-old man\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "trainer.evaluate(small_val)\n",
    "test_op = trainer.predict(small_test)\n",
    "print(tokenizer.decode(test_op[0][1],skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input RDF:  ['accidentAddress | southeast_Houston', 'accidentDate | overnight','shootingType | violent_shooting']\n",
      "Generation:  The violent shooting occurred overnight in southeast Houston.\n",
      "\n",
      "\n",
      "input RDF:  ['victimAge | 19', 'victimGender | men']\n",
      "Generation:  A 19-year-old man was arrested in connection with the 19-year-old man\n",
      "\n",
      "\n",
      "input RDF:  ['victimAge | 39-year-old', 'victimGender | female', 'victimRace | black', 'victimStatus | major_injuries']\n",
      "Generation:  The 39-year-old victim, a black woman, suffered major injuries, according to\n",
      "\n",
      "\n",
      "input RDF:  ['victimName | Adrian_Potts']\n",
      "Generation:  Adrian Potts was arrested in connection with the murder of Adrian Potts.\n",
      "\n",
      "\n",
      "input RDF:  [\"hospitalName | Cook_Children's_Medical_Center\", 'takenToHospital | True', 'victimAge | 3-year-olds', 'victimAge_Group | children', 'victimNumber | two', 'victimStatus | serious_condition']\n",
      "Generation:  The 3-year-olds were taken to the Cook Children's Medical Center for serious condition\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Comparing input RDF to trainer.predict output\n",
    "for pred_iteration, predictions in enumerate(test_op[0][:5]):\n",
    "    print(f\"input RDF:  {tokenizer.decode(small_test['input_ids'][pred_iteration],skip_special_tokens=True) }\\nGeneration:  {tokenizer.decode(predictions,skip_special_tokens=True)}\\n\\n\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model/tokenizer/trainer locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Seq2SeqTrainer' object has no attribute 'state_dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_24136\\1263028158.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# save the model after fine-tuning so that we can safely test at a later date\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#trainer.save_model(f\"../Models/{model_name}\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mf\"../Models/{model_name}/state_dict\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'Seq2SeqTrainer' object has no attribute 'state_dict'"
     ]
    }
   ],
   "source": [
    "# save the model after fine-tuning so that we can safely test at a later date\n",
    "#trainer.save_model(f\"../Models/{model_name}\")\n",
    "#torch.save(trainer.state_dict(), f\"../Models/{model_name}/state_dict\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ../Models/t5-base-story-generation/model\n",
      "Configuration saved in ../Models/t5-base-story-generation/model\\config.json\n",
      "Model weights saved in ../Models/t5-base-story-generation/model\\pytorch_model.bin\n",
      "tokenizer config file saved in ../Models/t5-base-story-generation/model\\tokenizer_config.json\n",
      "Special tokens file saved in ../Models/t5-base-story-generation/model\\special_tokens_map.json\n",
      "Copy vocab file to ../Models/t5-base-story-generation/model\\spiece.model\n",
      "tokenizer config file saved in ../Models/t5-base-story-generation/tokenizer\\tokenizer_config.json\n",
      "Special tokens file saved in ../Models/t5-base-story-generation/tokenizer\\special_tokens_map.json\n",
      "Copy vocab file to ../Models/t5-base-story-generation/tokenizer\\spiece.model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('../Models/t5-base-story-generation/tokenizer\\\\tokenizer_config.json',\n",
       " '../Models/t5-base-story-generation/tokenizer\\\\special_tokens_map.json',\n",
       " '../Models/t5-base-story-generation/tokenizer\\\\spiece.model',\n",
       " '../Models/t5-base-story-generation/tokenizer\\\\added_tokens.json',\n",
       " '../Models/t5-base-story-generation/tokenizer\\\\tokenizer.json')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Saves model and tokenizer in one\n",
    "trainer.save_model(f\"../Models/{model_name}/model\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ../Models/t5-base-story-generation/model\\config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-base\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 3072,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.22.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading weights file ../Models/t5-base-story-generation/model\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at ../Models/t5-base-story-generation/model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "saved_model = T5ForConditionalGeneration.from_pretrained('../Models/t5-base-story-generation/model', local_files_only=True)\n",
    "saved_tokenizer = AutoTokenizer.from_pretrained('../Models/t5-base-story-generation/model',local_files_only=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.12 ('pytorch_test')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "835bb5b643d767f7a377742b32e11c16532dde9c4b304154c887f70265b04043"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
