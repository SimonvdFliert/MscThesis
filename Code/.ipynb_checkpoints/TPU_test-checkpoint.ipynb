{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3dcbb17-9997-4fed-99d1-7a5f4b18198e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install nltk --quiet\n",
    "%pip install transformers --quiet\n",
    "%pip install datasets --quiet\n",
    "%pip install evaluate --quiet\n",
    "%pip install sentencepiece --quiet\n",
    "%pip install accelerate --quiet\n",
    "%pip install rouge_score --quiet\n",
    "%pip install bert_score --quiet\n",
    "%pip install torchvision --quiet\n",
    "%pip install tensorboard --quiet\n",
    "\n",
    "#%pip install tensorboardX --quiet\n",
    "#!pip install transformers[deepspeed] --quiet\n",
    "#!pip install deepspeed --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35dca2f8-3dd0-4c14-a212-f1c889f577fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import torch \n",
    "import nltk\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer, pipeline,  EarlyStoppingCallback, DataCollatorForSeq2Seq, Trainer\n",
    "from transformers import MT5ForConditionalGeneration, T5Tokenizer, T5ForConditionalGeneration, MT5TokenizerFast, is_torch_tpu_available\n",
    "\n",
    "import datasets\n",
    "import evaluate\n",
    "import accelerate\n",
    "#import deepspeed\n",
    "\n",
    "import Evaluation_Code.Bartscore as bartscore ## code for Bartscore\n",
    "import gc\n",
    "import json\n",
    "from ast import literal_eval\n",
    "\n",
    "import pprint\n",
    "import torch\n",
    "import torch_xla.core.xla_model as xm\n",
    "import torch_xla.distributed.parallel_loader as pl\n",
    "import torch_xla.distributed.xla_multiprocessing as xmp\n",
    "import torch_xla.debug.metrics as met\n",
    "\n",
    "os.environ['XRT_TPU_CONFIG'] = \"tpu_worker;0;10.109.134.218:8470\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e4e8091-1fc3-422b-8d59-89e2c8d44918",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Parameters\n",
    "FLAGS = {}\n",
    "FLAGS['model_name'] = 't5-base' \n",
    "FLAGS['max_token_length_preprocessing'] = 256\n",
    "FLAGS['early_stopping_patience'] = 3\n",
    "FLAGS['model_save_total_limit'] = 4\n",
    "FLAGS['training_optimizer'] = \"adafactor\"\n",
    "FLAGS['batch_size'] = 32\n",
    "FLAGS['gradient_accumulation_steps'] = 2\n",
    "FLAGS['learning_rate'] = 5e-05\n",
    "FLAGS['num_epochs'] = 200\n",
    "FLAGS['training_strategy'] = 'epoch'\n",
    "FLAGS['generation_num_beams'] = 5\n",
    "FLAGS['generation_max_length'] = 100\n",
    "FLAGS['num_workers'] = 4\n",
    "FLAGS['num_cores'] = 8 if os.environ.get('TPU_NAME', None) else 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36bb5c85-fb4f-4d3a-9c38-006a5925a8df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "############################################################################## Begin Environment Setup ######################################################################################\n",
    "\n",
    "def ensure_cuda_compatability():\n",
    "    print(f'Torch version: {torch.__version__}')\n",
    "    print(f'Cuda version: {torch.version.cuda}')\n",
    "    print(f'Cudnn version: {torch.backends.cudnn.version()}')\n",
    "    #print(f'Is cuda available: {torch.cuda.is_available()}')\n",
    "    #print(f'Number of cuda devices: {torch.cuda.device_count()}')\n",
    "    #print(f'Current default device: {torch.cuda.current_device()}')\n",
    "    #print(f'First cuda device: {torch.cuda.device(0)}')\n",
    "    #print(f'Name of the first cuda device: {torch.cuda.get_device_name(0)}\\n\\n')\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "    #Ensure we are really working with full GPU capacity\n",
    "    gc.collect() \n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "############################################################################## End Environment Setup ######################################################################################\n",
    "\n",
    "############################################################################## Begin Model and Dataset Setup ######################################################################################\n",
    "\n",
    "def preprocess_model(model_name):\n",
    "    \"\"\"\n",
    "    Setup the model and tokenizer for preprocessing. This will be a pre-trained model collected from huggingface\n",
    "    \"\"\"\n",
    "    #model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "    #model = Wrapped_model(model)\n",
    "    #model.to(TPU_device)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    print('LOGGING: preprocess_model DONE \\n')\n",
    "    #return model, tokenizer\n",
    "    return tokenizer\n",
    "\n",
    "\n",
    "def load_CACAPO_data():\n",
    "    \"\"\"\n",
    "    This function retrieves the csv files and creates a dataset\n",
    "    \"\"\"\n",
    "    #print('LOGGING: load_CACAPO_data DONE \\n')\n",
    "\n",
    "    return datasets.load_dataset(\"../Data/Cleaned_data/\", data_files={\"train\": \"Train.csv\", \"dev\": \"Dev.csv\", \"test\": \"Test.csv\"})\n",
    "\n",
    "\n",
    "def preprocess_data(data):\n",
    "    \"\"\"\n",
    "    Tokenize the data\n",
    "    \"\"\"\n",
    "    max_length = FLAGS['max_token_length_preprocessing']\n",
    "    RDFs = data[\"input\"]\n",
    "    texts = data[\"output\"]\n",
    "\n",
    "    ## When converting a pandas df to csv (used for loading dataset), a list of lists can transform to a long string\n",
    "    ## Here we convert it back with literal_eval\n",
    "\n",
    "    for rdf_iteration, rdf in enumerate(RDFs):\n",
    "        RDFs[rdf_iteration] = literal_eval(rdf)\n",
    "\n",
    "    #This creates a dataset object of model_inputs{input_ids, attention_mask}\n",
    "    model_inputs = tokenizer(RDFs, truncation=True, padding='max_length', return_tensors='pt',  max_length=max_length, is_split_into_words=True)\n",
    "    \n",
    "    # specially for seq2seq tokenizer, \"Temporarily sets the tokenizer for encoding the targets. Useful for tokenizer associated to sequence-to-sequence models that need a slightly different processing for the labels.\"\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        target_texts = tokenizer(texts, padding='max_length', truncation=True, return_tensors='pt',  max_length=max_length).input_ids\n",
    "\n",
    "    #This adds the tokenized target texts as a new column in the dataset object, thus becoming model_inputs{input_ids, attention_mask, labels}\n",
    "    model_inputs[\"labels\"] = target_texts\n",
    "\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "def transform_datasets(dataset):\n",
    "    \"\"\"\n",
    "    After loading in and creating the initial dataset, the text data is transformed, by tokenizing the input and output texts. The initial dataset is also split into train,val,test for training use.\n",
    "    NOTE That the test set will not be preprocessed here yet, this will be done in a different function\n",
    "    \"\"\"\n",
    "\n",
    "    train_ds = dataset[\"train\"]\n",
    "    val_ds = dataset[\"dev\"]\n",
    "    test_ds = dataset[\"test\"]\n",
    "\n",
    "    # to use the actual articles for evaluation\n",
    "    true_articles_test = test_ds['output']\n",
    "    # The Parent Metric requires the original RDFs\n",
    "    test_rdf_input = test_ds['input']\n",
    "\n",
    "\n",
    "    ## Process the data in batches\n",
    "    train_ds = train_ds.map(preprocess_data, batched=True, remove_columns=dataset[\"train\"].column_names)\n",
    "    val_ds = val_ds.map(preprocess_data, batched=True, remove_columns=dataset[\"dev\"].column_names)\n",
    "    test_ds = test_ds.map(preprocess_data, batched=True, remove_columns=dataset[\"test\"].column_names)\n",
    "\n",
    "    # transform the datasets into torch sensors, as the model will expect this format\n",
    "    train_ds.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])#, device= serial_exec)\n",
    "    val_ds.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])#, device= serial_exec)\n",
    "    test_ds.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])# , device= serial_exec)\n",
    "\n",
    "    print('LOGGING: transform_datasets DONE \\n')\n",
    "\n",
    "    return train_ds, val_ds, test_ds, true_articles_test, test_rdf_input\n",
    "\n",
    "############################################################################## End Model and Dataset Setup ######################################################################################\n",
    "\n",
    "############################################################################## Begin Evaluation Setup######################################################################################\n",
    "\n",
    "\n",
    "def load_eval_metrics():\n",
    "    \"\"\"\n",
    "    Loads in all metrics that will be used later on during evaluation. This is seperated to not load in the metrics a dozen of times during training.\n",
    "    \"\"\"\n",
    "    bleu = datasets.load_metric(\"bleu\")\n",
    "    rouge = evaluate.load('rouge')\n",
    "    meteor = evaluate.load('meteor')\n",
    "    perplexity = evaluate.load(\"perplexity\", module_type=\"metric\")\n",
    "    bertscore = evaluate.load(\"bertscore\")\n",
    "    bart_scorer = bartscore.BARTScorer(device = TPU_device, checkpoint='facebook/bart-base') \n",
    "\n",
    "    print('LOGGING: load_eval_metrics DONE \\n')\n",
    "\n",
    "    return bleu, rouge, meteor, perplexity, bertscore, bart_scorer\n",
    "\n",
    "\n",
    "def postprocess_text(preds, labels):\n",
    "    \"\"\"\n",
    "    Supplementary Method called in decode_text.\n",
    "\n",
    "    Returns list of split decoded labels and predictions for evaluation\n",
    "    \"\"\"\n",
    "    preds = [pred.split() for pred in preds]\n",
    "    labels = [[label.split()] for label in labels]\n",
    "    return preds, labels\n",
    "\n",
    "\n",
    "\n",
    "def decode_text(predictions, labels):\n",
    "    \"\"\"\n",
    "    Supplementary Method called in compute_metrics.\n",
    "\n",
    "    Returns decoded labels and predictions for evaluation\n",
    "    \"\"\"\n",
    "    if isinstance(predictions, tuple):\n",
    "            predictions = predictions[0]\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    \n",
    "    with open(\"../GraphMetricLogging/decoded_preds.txt\", 'w', encoding='utf-8') as tester:\n",
    "        for pairs in decoded_preds:\n",
    "            tester.write(f'{pairs} \\n')\n",
    "    with open(\"../GraphMetricLogging/decoded_labels.txt\", 'w', encoding='utf-8') as test:\n",
    "        for labels in decoded_labels:\n",
    "            test.write(f'{labels} \\n')\n",
    "    with open(\"../GraphMetricLogging/preds.txt\", 'w', encoding='utf-8') as a:\n",
    "        for pred_pairs in predictions:\n",
    "            a.write(f'{pred_pairs} \\n')\n",
    "    with open(\"../GraphMetricLogging/labels.txt\", 'w', encoding='utf-8') as b:\n",
    "        for pred_labels in decoded_preds:\n",
    "            b.write(f'{pred_labels} \\n')\n",
    "    \n",
    "    return decoded_preds, decoded_labels\n",
    "\n",
    "############################################################################## End Evaluation Setup######################################################################################\n",
    "\n",
    "############################################################################## Begin Evaluation######################################################################################\n",
    "\n",
    "def evaluate_texts(decoded_preds, decoded_labels):\n",
    "    \"\"\"\n",
    "    Calculates metrics given a list of decoded predictions and decoded labels\n",
    "    \"\"\"\n",
    "    #post_process for BLEU\n",
    "    blue_preds, blue_labels = postprocess_text(decoded_preds,  decoded_labels)\n",
    "\n",
    "    # setup metrics for use\n",
    "    bleu, rouge, meteor,perplexity, bertscore, bart_scorer = load_eval_metrics()\n",
    "\n",
    "    #Calculate the metrics\n",
    "    print(f'\\n LOGGING: Calculating Blue')\n",
    "    bleu_output = bleu.compute(predictions=blue_preds, references=blue_labels)\n",
    "    print(f'\\n LOGGING: Calculating Rouge')\n",
    "    rouge_output = rouge.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    print(f'\\n LOGGING: Calculating Meteor')\n",
    "    meteor_output = meteor.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    print(f'\\n LOGGING: Calculating Perplexity')\n",
    "    perp_output = perplexity.compute(predictions=decoded_preds, model_id='gpt2')\n",
    "    print(f'\\n LOGGING: Calculating Bertscore')\n",
    "    bertscore_output = bertscore.compute(predictions=decoded_preds, references=decoded_labels, lang=\"en\")\n",
    "    print(f'\\n LOGGING: Calculating Bartscore')\n",
    "    bart_scores_output = bart_scorer.score(srcs=decoded_preds, tgts=decoded_labels, batch_size=FLAGS['batch_size'])\n",
    "\n",
    "    print(f'\\n LOGGING: Done calculations')\n",
    "\n",
    "    return bleu_output, rouge_output, meteor_output, perp_output, bertscore_output, bart_scores_output\n",
    "\n",
    "\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    \"\"\"\"\n",
    "    Metrics to be evaluated during training and validation\n",
    "    Metrics used: BLEU, ROUGE, METEOR, Bertscore, BARTScore\n",
    "    \"\"\"\n",
    "    # decode the predictions and labels for eval\n",
    "    predictions, labels = pred\n",
    "    decoded_preds, decoded_labels = decode_text(predictions, labels)\n",
    "\n",
    "    bleu_output, rouge_output, meteor_output, bertscore_output, bart_scores_output = evaluate_texts(decoded_preds, decoded_labels)\n",
    "    \n",
    "    ## Huggingsface trainer requires a dict if multiple metrics are used\n",
    "    evaluation_results = {\"blue_output\": bleu_output, \"rouge_output\": rouge_output, \"meteor_results\": meteor_output, \n",
    "                          \"bertscore_output\": bertscore_output, \"bart_scores_output\": bart_scores_output}  \n",
    "    \n",
    "    # Tensorboard doesn't like the dict format of our calculated methods, so we write them to a file so that we can create our own figures later on.\n",
    "    logging_for_graphs_path = f\"../GraphMetricLogging/{model_name}_metrics.txt\"\n",
    "    \n",
    "    # if the file doesn't exist yet, create it and write first evaluation results to it\n",
    "    if not os.path.exists(logging_for_graphs_path):\n",
    "        with open(logging_for_graphs_path, 'w', encoding='utf-8') as logging_creation:\n",
    "            logging_creation.write(f'{evaluation_results} \\n')\n",
    "    # metric file already exists, so now we merely append to the existing file. We need a seperate opener, as otherwise we would overwrite the file\n",
    "    else:\n",
    "        with open(logging_for_graphs_path, 'a', encoding='utf-8') as logging_appending:\n",
    "            logging_appending.write(f'{evaluation_results} \\n')\n",
    "\n",
    "    #During training we can see the intermediary results, however Bartscore, Bertscore and Perplexity, make it far mor difficult to read. Tensorboard also ignores these outputs.\n",
    "    #Therefore we only give bleu, rouge and meteor back to the trainer for logging. We do not lose any results, as we store the total results in a text file                \n",
    "    return {\"blue_output\": bleu_output, \"rouge_output\": rouge_output, \"meteor_results\": meteor_output}\n",
    "\n",
    "\n",
    "############################################################################## End Evaluation Section######################################################################################\n",
    "\n",
    "############################################################################## Begin Huggingface Trainer Setup ######################################################################################\n",
    "\n",
    "def set_training_args(model_name, learning_rate, num_train_epochs, evaluation_strategy, generation_num_beams, generation_max_length,\n",
    "                      gradient_accumulation_steps, per_device_train_batch_size, per_device_eval_batch_size):\n",
    "    \"\"\"\n",
    "    Setup the training arguments that will be used during training.\n",
    "    \"\"\"\n",
    "    model_dir = f\"../Results/{model_name}\"\n",
    "\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "                output_dir=model_dir,\n",
    "                learning_rate=learning_rate,\n",
    "                do_eval=True, # will be set to true if evaluation strategy is set\n",
    "                do_predict=True, #Whether to run predictions on the test set or not.\n",
    "                num_train_epochs=num_train_epochs,\n",
    "                evaluation_strategy= evaluation_strategy,\n",
    "                save_strategy=evaluation_strategy,\n",
    "                logging_strategy = evaluation_strategy,\n",
    "                save_total_limit= FLAGS['model_save_total_limit'], # the maximum number of models to keep before deleting the oldest one\n",
    "                predict_with_generate=True, # Whether to use generate to calculate generative metrics (ROUGE, BLEU).\n",
    "                generation_num_beams=generation_num_beams,  #The num_beams to use on each evaluation loop when predict_with_generate=True. Will default to the num_beams value of the model configuration\n",
    "                gradient_checkpointing=True, #\n",
    "                generation_max_length=generation_max_length,\n",
    "                gradient_accumulation_steps=gradient_accumulation_steps, #Number of updates steps to accumulate the gradients for, before performing a backward/update pass\n",
    "                per_device_train_batch_size=per_device_train_batch_size, #The batch size per GPU/TPU core/CPU for training.\n",
    "                per_device_eval_batch_size=per_device_eval_batch_size, #The batch size per GPU/TPU core/CPU for evaluation.\n",
    "                optim= FLAGS['training_optimizer'], #The optimizer to use: adamw_hf, adamw_torch, adamw_apex_fused, or adafactor.\n",
    "                report_to=\"tensorboard\",\n",
    "                load_best_model_at_end = True, #required for early stopping callback \n",
    "       )\n",
    "\n",
    "    print('LOGGING: set_training_args DONE \\n')\n",
    "\n",
    "    return training_args\n",
    "\n",
    "\n",
    "def get_clean_model(model_name):\n",
    "    \"\"\"\n",
    "    Ensures that a new, fresh model is used for finetuning\n",
    "    \"\"\"\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "    #model = xmp.MpModelWrapper(model)\n",
    "    model.to(TPU_device)\n",
    "\n",
    "    #model = Wrapped_model(model)\n",
    "\n",
    "    return model\n",
    "\n",
    "     \n",
    "\n",
    "def set_trainer(model_name, training_args, train_ds, val_ds, tokenizer):\n",
    "    \"\"\"\n",
    "    Initializes a trainer\n",
    "    Takes in: Model name, training arguments, training dataset, validation dataset, and tokenizer\n",
    "    Returns: Trainer instance\n",
    "    \"\"\"\n",
    "    clean_model = get_clean_model(model_name)\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer)\n",
    "    trainer = Seq2SeqTrainer(\n",
    "                model=clean_model,\n",
    "                args=training_args,\n",
    "                train_dataset=train_ds,\n",
    "                eval_dataset=val_ds,\n",
    "                compute_metrics=compute_metrics,\n",
    "                data_collator = data_collator,\n",
    "                tokenizer=tokenizer,\n",
    "                callbacks=[EarlyStoppingCallback(early_stopping_patience=FLAGS['early_stopping_patience'])], #Earlystopping metric is by default the validation loss\n",
    "                )\n",
    "\n",
    "    print('LOGGING: set_trainer DONE \\n')\n",
    "\n",
    "    return trainer\n",
    "############################################################################## End Huggingface Trainer Setup ######################################################################################\n",
    "\n",
    "############################################################################## Begin Train and Save ######################################################################################\n",
    "\n",
    "\n",
    "def train_and_save(trainer, path_model_name):\n",
    "    trainer.train()\n",
    "    trainer.save_model(f\"../Models/{path_model_name}\")\n",
    "\n",
    "    print('LOGGING: train_and_save DONE \\n')\n",
    "\n",
    "############################################################################## End Train and Save ######################################################################################\n",
    "\n",
    "\n",
    "############################################################################## Begin Evaluation Process ######################################################################################\n",
    "\n",
    "def get_saved_model(path_model_name):\n",
    "    \"\"\"\"\n",
    "    Retrieves the best model that was saved after fine-tuning\n",
    "    \"\"\"\n",
    "    saved_model_path = f\"../Models/{path_model_name}\"\n",
    "\n",
    "    saved_model = AutoModelForSeq2SeqLM.from_pretrained(saved_model_path, local_files_only=True)\n",
    "    #saved_model = xmp.MpModelWrapper(saved_model)\n",
    "    saved_model.to(TPU_device)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(saved_model_path ,local_files_only=True)\n",
    "    return saved_model, tokenizer\n",
    "\n",
    "def generate_predictions(saved_model, test_set):\n",
    "    \"\"\"\n",
    "    Generates predictions based on the test set, returns a list of predictions and the corresponding \"true\" articles\n",
    "    \"\"\"\n",
    "    encoded_inputs = test_set.remove_columns(\"labels\")\n",
    "\n",
    "    # set-up a dataloader to load in the tokenized test dataset\n",
    "    dataloader = torch.utils.data.DataLoader(encoded_inputs,  batch_size=FLAGS['batch_size']) \n",
    "    test_dataloader = pl.MpDeviceLoader(dataloader, TPU_device)\n",
    "\n",
    "    # generate text for each batch\n",
    "    all_predictions = []\n",
    "    for i,batch in enumerate(test_dataloader):\n",
    "        predictions = saved_model.generate(**batch, max_new_tokens = 100, do_sample=True, num_beams = 5, top_p=0.7, repetition_penalty = 1.3) \n",
    "        all_predictions.append(predictions)\n",
    "\n",
    "    # flatten predictions\n",
    "    all_predictions_flattened = [pred for preds in all_predictions for pred in preds]\n",
    "\n",
    "    print('LOGGING: generate_predictions DONE \\n')\n",
    "    return all_predictions_flattened\n",
    "\n",
    "\n",
    "def decode_predictions(predictions, tokenizer):\n",
    "    \"\"\"\n",
    "    Decode the predictions made by the model\n",
    "    \"\"\"\n",
    "    decoded_predictions = []\n",
    "\n",
    "    for iteration, prediction in enumerate(predictions):\n",
    "        # single_decoded_prediction = (tokenizer.decode(prediction,skip_special_tokens=True))\n",
    "        # if len(single_decoded_prediction) != 0:\n",
    "        decoded_predictions.append((tokenizer.decode(prediction,skip_special_tokens=True)))\n",
    "\n",
    "    print('LOGGING: decode_predictions DONE \\n')\n",
    "\n",
    "    return decoded_predictions\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_test_set(path_model_name, test_set, true_articles_test, test_rdf_input):\n",
    "    \"\"\"\n",
    "    Transforms test set, retrieves predictions, and evaluates these predictions\n",
    "    \"\"\"\n",
    "    saved_model, saved_tokenizer = get_saved_model(path_model_name)\n",
    "\n",
    "    predictions = generate_predictions(saved_model, test_set)\n",
    "\n",
    "    #decode the predictions in preperation of evaluation\n",
    "    decoded_test_predictions = decode_predictions(predictions, saved_tokenizer)\n",
    "\n",
    "    #calculate the evaluation metrics on the predictions\n",
    "    bleu_output, rouge_output, meteor_output, perp_output,  bertscore_output, bart_scores_output = evaluate_texts(decoded_test_predictions, true_articles_test)\n",
    "\n",
    "    ## Huggingsface trainer requires a dict if multiple metrics are used\n",
    "    evaluation_results = {\"blue_output\": bleu_output, \"rouge_output\": rouge_output, \"meteor_results\": meteor_output, \n",
    "                          \"perp_output\": perp_output, \"bertscore_output\": bertscore_output, \"bart_scores_output\": bart_scores_output}\n",
    "\n",
    "    log_results(path_model_name, evaluation_results)\n",
    "\n",
    "    ##Additional PARENT evaluation\n",
    "    tables = test_rdf_input\n",
    "    references = true_articles_test\n",
    "    generations = decoded_test_predictions\n",
    "    parent_attempt(path_model_name, generations, references, tables)\n",
    "    \n",
    "    return evaluation_results\n",
    "\n",
    "\n",
    "def write_to_text_parent(path_model_name, decoded_predictions, true_articles, rdfs):\n",
    "    \"\"\"\n",
    "    Parent script requires text files, so we create them here\n",
    "    \"\"\"\n",
    "\n",
    "    with open(f'../Parent_test/{path_model_name}_true_articles.txt', 'w', encoding='utf-8') as f:\n",
    "        for articles in true_articles:\n",
    "            f.write(f'{articles} \\n')\n",
    "\n",
    "    with open(f'../Parent_test/{path_model_name}_decode_predictions.txt', 'w', encoding='utf-8') as f:\n",
    "        for predictions in decoded_predictions:\n",
    "            f.write(f'{predictions} \\n')\n",
    "\n",
    "    with open(f'../Parent_test/{path_model_name}_rdfs.txt', 'w', encoding='utf-8') as f:\n",
    "        for pairs in rdfs:\n",
    "            f.write(f'{pairs} \\n')\n",
    "\n",
    "\n",
    "def prepare_inputs_parent(RDFs):\n",
    "    \"\"\"\n",
    "    Cleans the RDF pairs and transforms them in the proper format so that the parent module can calculate with it.\n",
    "    Input: RDF pairs of format \"Attribute | Value\"\n",
    "    Returns a list of lists containing tuples --> [ [ (Attribute, Value), (Attribute, Value), (Attribute, Value)] ...]\n",
    "    \"\"\"\n",
    "\n",
    "    attribute_value_pairs = []\n",
    "\n",
    "    for iteration, inputRDF in enumerate(RDFs):\n",
    "        split_RDF = inputRDF.split(\", \")\n",
    "        entry=[]\n",
    "        for connected_pair in split_RDF:\n",
    "            if '[' in connected_pair:\n",
    "                connected_pair = connected_pair.replace('[', '')\n",
    "            if ']' in connected_pair:\n",
    "                connected_pair = connected_pair.replace(']', '')\n",
    "            if '_' in connected_pair:\n",
    "                connected_pair = connected_pair.replace('_', ' ')\n",
    "            split_pair = tuple(connected_pair.split(' | '))\n",
    "            entry.append((split_pair))\n",
    "        attribute_value_pairs.append(entry)\n",
    "    return attribute_value_pairs\n",
    "\n",
    "\n",
    "def parent_attempt(path_model_name, generations, references, rdfs):\n",
    "    \"\"\"\n",
    "    The Parent metric needs special treatment, as it only accepts specific inputs and file types.\n",
    "    \"\"\"\n",
    "    prepared_rdfs = prepare_inputs_parent(rdfs)\n",
    "    write_to_text_parent(path_model_name, generations, references, prepared_rdfs)\n",
    "\n",
    "    !python -i \"Evaluation_code/Parent.py\" --references f\"../Parent_test/{path_model_name}_true_articles.txt\" \\\n",
    "                                                     --generations f\"../Parent_test/{path_model_name}_decode_predictions.txt\"  \\\n",
    "                                                     --tables f\"../Parent_test/{path_model_name}_rdfs.txt\"\n",
    "\n",
    "def log_results(path_model_name, results):\n",
    "    with open(f'../Logging_TestSet_Results/{path_model_name}_logResults.json', 'w') as convert_file:\n",
    "        convert_file.write(json.dumps(results))\n",
    "\n",
    "############################################################################## End Evaluation Process ######################################################################################\n",
    "\n",
    "############################################################################## Begin Full fine-tune setup######################################################################################\n",
    "\n",
    "def fine_tune_model(model_name):\n",
    "    # ensure cuda compatability\n",
    "    ensure_cuda_compatability()\n",
    "\n",
    "    # I instantiate the tokenizer as a global variable, as the .map function in transform_datasets was not working properly. \n",
    "    # This should not be an issue, as the tokenizer remains consistent during training and evaluation.\n",
    "    global tokenizer\n",
    "    global TPU_device\n",
    "    #global Wrapped_model\n",
    "    global serial_exec\n",
    "    \n",
    "    TPU_device = xm.xla_device()\n",
    "    serial_exec = xmp.MpSerialExecutor()\n",
    "\n",
    "    \n",
    "    ## retrieve model and tokenizer from huggingface to prepare dataset\n",
    "    tokenizer = preprocess_model(model_name)\n",
    "    #model, tokenizer = preprocess_model(model_name)\n",
    "\n",
    "    #Wrapped_model = xmp.MpModelWrapper(model)\n",
    "\n",
    "    ### retrieve the unprocessed data from the csv files\n",
    "    entire_dataset =load_CACAPO_data()\n",
    "    \n",
    "    # ## process the dataset and split it into its natural train, val, test split\n",
    "    #train_ds, val_ds, test_ds, true_articles_test, test_rdf_inputs =  serial_exec.run(transform_datasets(entire_dataset))\n",
    "    train_ds, val_ds, test_ds, true_articles_test, test_rdf_inputs =  transform_datasets(entire_dataset)\n",
    "\n",
    "\n",
    "    # ### setup the training arguments \n",
    "    training_args = set_training_args(model_name=model_name, learning_rate = FLAGS['learning_rate'], \n",
    "                                      num_train_epochs = FLAGS['num_epochs'], evaluation_strategy = FLAGS['training_strategy'], generation_num_beams=FLAGS['generation_num_beams'], \n",
    "                                      generation_max_length = FLAGS['generation_max_length'], gradient_accumulation_steps = FLAGS['gradient_accumulation_steps'], \n",
    "                                      per_device_train_batch_size= FLAGS['batch_size'] , per_device_eval_batch_size= FLAGS['batch_size'] )\n",
    "\n",
    "    # ###create a trainer instance \n",
    "    trainer = set_trainer(model_name, training_args, train_ds, val_ds, tokenizer)\n",
    "\n",
    "    # Both mt5 and T5-dutch have / in their name, which makes pathing more chaotic\n",
    "    if '/' in model_name:\n",
    "        path_model_name = model_name.replace('/', '_')\n",
    "    elif '-' in model_name:\n",
    "        path_model_name = model_name.replace('-', '_')\n",
    "\n",
    "    ## Finally fine-tune the model and save it\n",
    "    train_and_save(trainer, path_model_name)\n",
    "\n",
    "    testset_evaluation_results = evaluate_test_set( path_model_name, test_ds, true_articles_test, test_rdf_inputs)\n",
    "\n",
    "    return testset_evaluation_results\n",
    "\n",
    " \n",
    "def main():\n",
    "    #global model_name\n",
    "    #models = [\"t5-base\", \"yhavinga/t5-v1.1-base-dutch-cased\", 'google/mt5-base']\n",
    "\n",
    "    #model_name = 't5-base' \n",
    "    results = fine_tune_model(model_name)\n",
    "\n",
    "############################################################################## End Full fine-tune setup######################################################################################\n",
    "\n",
    "\n",
    "# Start training processes\n",
    "def _mp_fn(rank, flags):\n",
    "    global FLAGS\n",
    "    global model_name\n",
    "    \n",
    "    FLAGS = flags\n",
    "    model_name = FLAGS['model_name']\n",
    "    main()\n",
    "    #torch.set_default_tensor_type('torch.FloatTensor')\n",
    "    #accuracy, data, pred, target = train_resnet18()\n",
    "    #if rank == 0:\n",
    "        # Retrieve tensors that are on TPU core 0 and plot.\n",
    "    #    plot_results(data.cpu(), pred.cpu(), target.cpu())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005986f2-097f-48ad-a552-c3f563e1e658",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 1.12.1\n",
      "Cuda version: 11.3\n",
      "Cudnn version: 8302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/transformers/models/t5/tokenization_t5_fast.py:166: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  FutureWarning,\n",
      "Using custom data configuration Cleaned_data-eeba3ff9d9b47cf4\n",
      "Found cached dataset csv (/home/jupyter/.cache/huggingface/datasets/csv/Cleaned_data-eeba3ff9d9b47cf4/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOGGING: preprocess_model DONE \n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fca69786ca640b1a7ae54a6c5a2c978",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92d8c88188b04592bd13968a2ff5beca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:3547: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  \"`as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your \"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3be8ef69063646f9bc52649e37a2b482",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4fcb9efd4424272a94da5ee1d0a7155",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOGGING: transform_datasets DONE \n",
      "\n",
      "LOGGING: set_training_args DONE \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 15290\n",
      "  Num Epochs = 200\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 47800\n",
      "  Number of trainable parameters = 222903552\n",
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOGGING: set_trainer DONE \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='47800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [    3/47800 01:33 < 1244:09:21, 0.01 it/s, Epoch 0.01/200]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
     ]
    }
   ],
   "source": [
    "xmp.spawn(_mp_fn, args=(FLAGS,), nprocs=FLAGS['num_cores'],\n",
    "          start_method='fork')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf120bd7-581e-45f3-b47b-f872a87cb936",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_parent_results_to_json(path_model_name):\n",
    "    \"\"\"\n",
    "    Colab stops after executing the Parent file. Thus after having created the two files for the metrics, this function combines them into one.\n",
    "    \"\"\"\n",
    "    with open(f'MscThesis/Logging_TestSet_Results/google_mt5-base_logResults.json', 'r') as result_file:     \n",
    "        overal_results = json.load(result_file)   \n",
    "\n",
    "    with open(f'MscThesis/Logging_TestSet_Results/logResultsColabParent.json', 'r') as add_to_file:\n",
    "        parent_results = json.load(add_to_file)\n",
    "\n",
    "    overal_results.update(parent_results)\n",
    "\n",
    "    with open(f'MscThesis/Logging_TestSet_Results/google_mt5-base_logResults.json', 'w') as f: \n",
    "        json.dump(overal_results, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386117f3-a37e-45f6-a2ca-995e85adf0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir 'MscThesis/Results/yhavinga/t5-v1.1-base-dutch-cased'/runs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2a6f1300-2823-4442-984b-dbfa3e168a21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_torch_tpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd52b631-4e00-4f7a-b87b-e73fe0a7889d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-12.m99",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-12:m99"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
