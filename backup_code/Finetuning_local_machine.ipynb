{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Anaconda\\envs\\CudaSupEnv\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "NOTE: Redirects are currently not supported in Windows or MacOs.\n"
     ]
    }
   ],
   "source": [
    "import os.path\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import torch \n",
    "import nltk\n",
    "\n",
    "from transformers import AutoTokenizer, T5ForConditionalGeneration, Seq2SeqTrainingArguments, Seq2SeqTrainer, pipeline, AutoModelForSeq2SeqLM, T5TokenizerFast, T5ForConditionalGeneration, DataCollatorForSeq2Seq\n",
    "import datasets\n",
    "import evaluate\n",
    "import accelerate\n",
    "\n",
    "#import Evaluation_Code.Parent as parent ## code for PARENT metric\n",
    "import Evaluation_Code.Bartscore as bartscore ## code for Bartscore\n",
    "\n",
    "import json\n",
    "from ast import literal_eval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_cuda_compatability():\n",
    "    print(f'Torch version: {torch.__version__}')\n",
    "    print(f'Cuda version: {torch.version.cuda}')\n",
    "    print(f'Cudnn version: {torch.backends.cudnn.version()}')\n",
    "    print(f'Is cuda available: {torch.cuda.is_available()}')\n",
    "    print(f'Number of cuda devices: {torch.cuda.device_count()}')\n",
    "    print(f'Current default device: {torch.cuda.current_device()}')\n",
    "    print(f'First cuda device: {torch.cuda.device(0)}')\n",
    "    print(f'Name of the first cuda device: {torch.cuda.get_device_name(0)}\\n\\n')\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "def preprocess_model(model_name):\n",
    "    \"\"\"\n",
    "    Setup the model and tokenizer for preprocessing. This will be a pre-trained model collected from huggingface\n",
    "    \"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name) #, device_map=\"auto\", load_in_8bit=True, load_in_8bit_threshold=4.0) --> tried to bit to bytes\n",
    "    #tokenizer = T5TokenizerFast.from_pretrained(model_name, is_fast=True)\n",
    "    #model = T5ForConditionalGeneration.from_pretrained(model_name) #, device_map=\"auto\", load_in_8bit=True, load_in_8bit_threshold=4.0) --> tried to bit to bytes\n",
    "    #tokenizer = AutoTokenizer.from_pretrained(\"yhavinga/t5-v1.1-base-dutch-cased\")\n",
    "\n",
    "    #model = AutoModelForSeq2SeqLM.from_pretrained(\"yhavinga/t5-v1.1-base-dutch-cased\")\n",
    "    model.cuda()\n",
    "\n",
    "    print('LOGGING: preprocess_model DONE \\n')\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def load_CACAPO_data():\n",
    "    \"\"\"\n",
    "    This function retrieves the csv files and creates a dataset\n",
    "    \"\"\"\n",
    "    print('LOGGING: load_CACAPO_data DONE \\n')\n",
    "\n",
    "    return datasets.load_dataset(\"../Data/Cleaned_data/\", data_files={\"train\": \"Train.csv\", \"dev\": \"Dev.csv\", \"test\": \"Test.csv\"})\n",
    "\n",
    "\n",
    "def preprocess_data(data):\n",
    "    \"\"\"\n",
    "    Tokenize the data\n",
    "    \"\"\"\n",
    "    max_length = 256\n",
    "    RDFs = data[\"input\"]\n",
    "    texts = data[\"output\"]\n",
    "\n",
    "    ## When converting a pandas df to csv (used for loading dataset), a list of lists can transform to a long string\n",
    "    ## Here we convert it back with literal_eval\n",
    "\n",
    "    for rdf_iteration, rdf in enumerate(RDFs):\n",
    "        RDFs[rdf_iteration] = literal_eval(rdf)\n",
    "\n",
    "    model_inputs = tokenizer(RDFs, truncation=True, padding='max_length', return_tensors='pt',  max_length=max_length, is_split_into_words=True)#.to('cuda')\n",
    "    \n",
    "    # specially for seq2seq tokenizer, \"Temporarily sets the tokenizer for encoding the targets. Useful for tokenizer associated to sequence-to-sequence models that need a slightly different processing for the labels.\"\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        target_texts = tokenizer(texts, padding='max_length', truncation=True, return_tensors='pt',  max_length=max_length).input_ids\n",
    "    \n",
    "    # target_texts = tokenizer(texts, padding='max_length', truncation=True, return_tensors='pt',  max_length=max_length).input_ids    \n",
    "    \n",
    "    model_inputs[\"labels\"] = target_texts#.to('cuda')\n",
    "\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "\n",
    "def transform_datasets(dataset):\n",
    "    \"\"\"\n",
    "    After loading in and creating the initial dataset, the text data is transformed, by tokenizing the input and output texts. The initial dataset is also split into train,val,test for training use.\n",
    "    NOTE That the test set will not be preprocessed here yet, this will be done in a different function\n",
    "    \"\"\"\n",
    "\n",
    "    ## Create smaller versions of the dataset\n",
    "    # small_train = dataset[\"train\"].shard(num_shards = 64, index = 0)\n",
    "    # small_val = dataset[\"dev\"].shard(num_shards = 64, index = 0)\n",
    "    # small_test = dataset[\"test\"].shard(num_shards = 64, index = 0)\n",
    "\n",
    "    small_train = dataset[\"train\"]\n",
    "    small_val = dataset[\"dev\"]\n",
    "    small_test = dataset[\"test\"]\n",
    "\n",
    "    # to use the actual articles for evaluation\n",
    "    true_articles_test = small_test['output']\n",
    "    # The Parent Metric requires the original RDFs\n",
    "    test_rdf_input = small_test['input']\n",
    "\n",
    "\n",
    "    ## Process the data in batches\n",
    "    small_train = small_train.map(preprocess_data, batched=True, remove_columns=dataset[\"train\"].column_names)\n",
    "    small_val = small_val.map(preprocess_data, batched=True, remove_columns=dataset[\"dev\"].column_names)\n",
    "    small_test = small_test.map(preprocess_data, batched=True, remove_columns=dataset[\"test\"].column_names)\n",
    "\n",
    "    # transform the datasets into torch sensors, as the model will expect this format\n",
    "    small_train.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels']) #, device=\"cuda\")\n",
    "    small_val.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels']) #, device=\"cuda\")\n",
    "    small_test.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels']) #, device=\"cuda\")\n",
    "\n",
    "    print('LOGGING: transform_datasets DONE \\n')\n",
    "\n",
    "    return small_train, small_val, small_test, true_articles_test, test_rdf_input\n",
    "\n",
    "\n",
    "\n",
    "def load_eval_metrics():\n",
    "    \"\"\"\n",
    "    Loads in all metrics that will be used later on during evaluation. This is seperated to not load in the metrics a dozen of times during training.\n",
    "    \"\"\"\n",
    "    bleu = datasets.load_metric(\"bleu\")\n",
    "    rouge = evaluate.load('rouge')\n",
    "    meteor = evaluate.load('meteor')\n",
    "    perplexity = evaluate.load(\"perplexity\", module_type=\"metric\")\n",
    "    bertscore = evaluate.load(\"bertscore\")\n",
    "    bart_scorer = bartscore.BARTScorer(device='cuda:0', checkpoint='facebook/bart-base') #facebook/bart-base   facebook/bart-large-cnn\n",
    "\n",
    "    print('LOGGING: load_eval_metrics DONE \\n')\n",
    "\n",
    "    return bleu, rouge, meteor, perplexity, bertscore, bart_scorer\n",
    "    #return bleu, rouge, meteor, bertscore, bart_scorer\n",
    "\n",
    "\n",
    "\n",
    "def postprocess_text(preds, labels):\n",
    "    \"\"\"\n",
    "    Supplementary Method called in decode_text.\n",
    "\n",
    "    Returns list of split decoded labels and predictions for evaluation\n",
    "    \"\"\"\n",
    "    preds = [pred.split() for pred in preds]\n",
    "    labels = [[label.split()] for label in labels]\n",
    "    return preds, labels\n",
    "\n",
    "\n",
    "\n",
    "def decode_text(predictions, labels):\n",
    "    \"\"\"\n",
    "    Supplementary Method called in compute_metrics.\n",
    "\n",
    "    Returns decoded labels and predictions for evaluation\n",
    "    \"\"\"\n",
    "    if isinstance(predictions, tuple):\n",
    "            predictions = predictions[0]\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)#.to(device)\n",
    "\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)#.to(device)\n",
    "\n",
    "    return decoded_preds, decoded_labels\n",
    "\n",
    "\n",
    "def evaluate_texts(decoded_preds, decoded_labels):\n",
    "    \"\"\"\n",
    "    Calculates metrics given a list of decoded predictions and decoded labels\n",
    "    \"\"\"\n",
    "    #post_process for BLEU\n",
    "    blue_preds, blue_labels = postprocess_text(decoded_preds,  decoded_labels)\n",
    "\n",
    "    # setup metrics for use\n",
    "    bleu, rouge, meteor, perplexity, bertscore, bart_scorer = load_eval_metrics()\n",
    "    #bleu, rouge, meteor, bertscore, bart_scorer = load_eval_metrics()\n",
    "\n",
    "    # Calculate the metrics\n",
    "    print(f'\\n LOGGING: Calculating Blue')\n",
    "    bleu_output = bleu.compute(predictions=blue_preds, references=blue_labels)\n",
    "    print(f'\\n LOGGING: Calculating Rouge')\n",
    "    rouge_output = rouge.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    print(f'\\n LOGGING: Calculating Meteor')\n",
    "    meteor_output = meteor.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    print(f'\\n LOGGING: Calculating Perplexity')\n",
    "    perp_output = perplexity.compute(predictions=decoded_preds, model_id='gpt2')\n",
    "    print(f'\\n LOGGING: Calculating Bertscore')\n",
    "    bertscore_output = bertscore.compute(predictions=decoded_preds, references=decoded_labels, lang=\"en\")\n",
    "    print(f'\\n LOGGING: Calculating Bartscore')\n",
    "    bart_scores_output = bart_scorer.score(srcs=decoded_preds, tgts=decoded_labels, batch_size=4)\n",
    "\n",
    "    return bleu_output, rouge_output, meteor_output, perp_output, bertscore_output, bart_scores_output\n",
    "\n",
    "\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    \"\"\"\"\n",
    "    Metrics to be evaluated during training and validation\n",
    "    Metrics used: BLEU, ROUGE, METEOR, Bertscore, BARTScore\n",
    "    \"\"\"\n",
    "    # decode the predictions and labels for eval\n",
    "    predictions, labels = pred\n",
    "    decoded_preds, decoded_labels = decode_text(predictions, labels)\n",
    "\n",
    "    bleu_output, rouge_output, meteor_output, perp_output, bertscore_output, bart_scores_output = evaluate_texts(decoded_preds, decoded_labels)\n",
    "\n",
    "    ## Huggingsface trainer requires a dict if multiple metrics are used\n",
    "    return {\"blue_output\": bleu_output, \"rouge_output\": rouge_output, \"meteor_results\": meteor_output,\n",
    "             \"perp_output\": perp_output, \"bertscore_output\": bertscore_output, \"bart_scores_output\": bart_scores_output}\n",
    "\n",
    "\n",
    "\n",
    "def set_training_args(model_name, learning_rate, num_train_epochs, evaluation_strategy, generation_num_beams, generation_max_length,\n",
    "                         gradient_accumulation_steps, per_device_train_batch_size, per_device_eval_batch_size):\n",
    "    \"\"\"\n",
    "    Setup the training arguments that will be used during training.\n",
    "    \"\"\"\n",
    "    #model_name = \"t5-fp16-test\"\n",
    "    model_dir = f\"../Results/{model_name}\"\n",
    "\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "                output_dir=model_dir,\n",
    "                learning_rate=learning_rate,\n",
    "                do_eval=True, # will be set to true if evaluation strategy is set\n",
    "                do_predict=True, #Whether to run predictions on the test set or not.\n",
    "                num_train_epochs=num_train_epochs,\n",
    "                evaluation_strategy= evaluation_strategy,\n",
    "                eval_steps= 1000, # Number of update steps between two evaluations if evaluation_strategy=\"steps\". Will default to the same value as logging_steps if not set.\n",
    "                save_steps=500, # Number of updates steps before two checkpoint saves if save_strategy=\"steps\".\n",
    "                #max_steps=10, # the total number of training steps to perform\n",
    "                save_total_limit= 10, # the maximum number of models to keep before deleting the oldest one\n",
    "                predict_with_generate=True, # Whether to use generate to calculate generative metrics (ROUGE, BLEU).\n",
    "                generation_num_beams=generation_num_beams,  #The num_beams to use on each evaluation loop when predict_with_generate=True. Will default to the num_beams value of the model configuration\n",
    "                gradient_checkpointing=True, #\n",
    "                generation_max_length=generation_max_length,\n",
    "                gradient_accumulation_steps=gradient_accumulation_steps, #Number of updates steps to accumulate the gradients for, before performing a backward/update pass\n",
    "                per_device_train_batch_size=per_device_train_batch_size, #The batch size per GPU/TPU core/CPU for training.\n",
    "                per_device_eval_batch_size=per_device_eval_batch_size, #The batch size per GPU/TPU core/CPU for evaluation.\n",
    "                optim=\"adafactor\", #The optimizer to use: adamw_hf, adamw_torch, adamw_apex_fused, or adafactor.\n",
    "                #report_to=\"tensorboard\",\n",
    "                fp16=True,\n",
    "                #bf16=True, ## should now be possible with rtx 3070\n",
    "                #tf32=True, #--> moet dan ook torch.backends.cuda.matmul.allow_tf32 = True\n",
    "                #auto_find_batch_size = True, #Whether to find a batch size that will fit into memory automatically through exponential decay, avoiding CUDA Out-of-Memory errors.\n",
    "                eval_accumulation_steps=2  #Number of predictions steps to accumulate the output tensors for, \n",
    "                                            # before moving the results to the CPU. If left unset, the whole predictions are accumulated on GPU/TPU \n",
    "                                            # before being moved to the CPU (faster but requires more memory).\n",
    "    )\n",
    "\n",
    "    print('LOGGING: set_training_args DONE \\n')\n",
    "\n",
    "    return training_args\n",
    "\n",
    "\n",
    "def get_clean_model(model_name):\n",
    "    \"\"\"\n",
    "    Simple function to ensure that a new model is used for finetuning\n",
    "    \"\"\"\n",
    "    return AutoModelForSeq2SeqLM.from_pretrained(model_name).cuda()\n",
    "    #return T5ForConditionalGeneration.from_pretrained(model_name).cuda() #T5ForConditionalGeneration\n",
    "\n",
    "#def get_data_Collator(tokenizer):\n",
    "#    return DataCollatorForSeq2Seq(tokenizer=tokenizer,  return_tensors=\"pt\")\n",
    "\n",
    "def set_trainer(model_name, training_args, train_ds, val_ds, tokenizer):\n",
    "    \"\"\"\n",
    "    Initializes a trainer\n",
    "    Takes in: Model name, training arguments, training dataset, validation dataset, and tokenizer\n",
    "    Returns: Trainer instance\n",
    "    \"\"\"\n",
    "    #collator = get_data_Collator(tokenizer)\n",
    "    clean_model = get_clean_model(model_name)\n",
    "    trainer = Seq2SeqTrainer(\n",
    "                model=clean_model,\n",
    "                args=training_args,\n",
    "                train_dataset=train_ds,\n",
    "                eval_dataset=val_ds,\n",
    "                compute_metrics=compute_metrics,\n",
    "                #data_collator = collator,\n",
    "                tokenizer=tokenizer\n",
    "                )\n",
    "\n",
    "    print('LOGGING: set_trainer DONE \\n')\n",
    "\n",
    "    return trainer\n",
    "\n",
    "def train_and_save(trainer, path_model_name):\n",
    "    trainer.train()\n",
    "    trainer.save_model(f\"../Models/{path_model_name}\")\n",
    "\n",
    "    print('LOGGING: train_and_save DONE \\n')\n",
    "\n",
    "\n",
    "def get_saved_model(path_model_name):\n",
    "    saved_model = T5ForConditionalGeneration.from_pretrained(f'../Models/{path_model_name}', local_files_only=True)\n",
    "    saved_model.cuda()\n",
    "    tokenizer = AutoTokenizer.from_pretrained(f'../Models/{path_model_name}' ,local_files_only=True)\n",
    "    return saved_model, tokenizer\n",
    "\n",
    "\n",
    "\n",
    "def generate_predictions(saved_model, test_set):\n",
    "    \"\"\"\n",
    "    Generates predictions based on the test set, returns a list of predictions and the corresponding \"true\" articles\n",
    "    \"\"\"\n",
    "    encoded_inputs = test_set.remove_columns(\"labels\")\n",
    "\n",
    "    # set-up a dataloader to load in the tokenized test dataset\n",
    "    test_dataloader = torch.utils.data.DataLoader(encoded_inputs,  batch_size=8) #pin_memory=True, --> levert error op: RuntimeError: cannot pin 'torch.cuda.LongTensor' only dense CPU tensors can be pinned\n",
    "\n",
    "    # generate text for each batch\n",
    "    all_predictions = []\n",
    "    for i,batch in enumerate(test_dataloader):\n",
    "        predictions = saved_model.generate(**batch, max_new_tokens = 100, do_sample=True, num_beams = 5, top_p=0.7, repetition_penalty = 1.3) # .to(device) --> RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper__index_select)\n",
    "        all_predictions.append(predictions)\n",
    "\n",
    "    # flatten predictions\n",
    "    all_predictions_flattened = [pred for preds in all_predictions for pred in preds]\n",
    "\n",
    "    print('LOGGING: generate_predictions DONE \\n')\n",
    "\n",
    "\n",
    "    return all_predictions_flattened #, true_articles\n",
    "\n",
    "\n",
    "def decode_predictions(predictions, tokenizer):\n",
    "    \"\"\"\n",
    "    Decode the predictions made by the model\n",
    "    \"\"\"\n",
    "    decoded_predictions = []\n",
    "\n",
    "    for iteration, prediction in enumerate(predictions):\n",
    "        decoded_predictions.append(tokenizer.decode(prediction,skip_special_tokens=True))\n",
    "\n",
    "    print('LOGGING: decode_predictions DONE \\n')\n",
    "\n",
    "    return decoded_predictions\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_test_set(path_model_name, test_set, true_articles_test, test_rdf_input):\n",
    "    \"\"\"\n",
    "    Transforms test set, retrieves predictions, and evaluates these predictions\n",
    "    \"\"\"\n",
    "    saved_model, saved_tokenizer = get_saved_model(path_model_name)\n",
    "\n",
    "    #predictions, test_articles = generate_predictions(saved_model, test_set)\n",
    "    predictions = generate_predictions(saved_model, test_set)\n",
    "\n",
    "    decoded_test_predictions = decode_predictions(predictions, saved_tokenizer)\n",
    "\n",
    "    bleu_output, rouge_output, meteor_output, perp_output, bertscore_output, bart_scores_output = evaluate_texts(decoded_test_predictions, true_articles_test)\n",
    "    #bleu_output, rouge_output, meteor_output, bertscore_output, bart_scores_output = evaluate_texts(decoded_test_predictions, true_articles_test)\n",
    "\n",
    "\n",
    "    evaluation_results = {\"blue_output\": bleu_output, \"rouge_output\": rouge_output, \"meteor_results\": meteor_output, \"perp_output\": perp_output, \"bertscore_output\": bertscore_output, \"bart_scores_output\": bart_scores_output}\n",
    "    #evaluation_results = {\"blue_output\": bleu_output, \"rouge_output\": rouge_output, \"meteor_results\": meteor_output,  \"bertscore_output\": bertscore_output, \"bart_scores_output\": bart_scores_output}\n",
    "\n",
    "    log_results(path_model_name, evaluation_results)\n",
    "\n",
    "    ##Additional PARENT evaluation\n",
    "    tables = test_rdf_input\n",
    "    references = true_articles_test\n",
    "    generations = decoded_test_predictions\n",
    "    parent_attempt(path_model_name, generations, references, tables)\n",
    "    \n",
    "    ## Huggingsface trainer requires a dict if multiple metrics are used\n",
    "    return evaluation_results\n",
    "\n",
    "\n",
    "def write_to_text_parent(path_model_name, decoded_predictions, true_articles, rdfs):\n",
    "    \"\"\"\n",
    "    Parent metric requires text files to work\n",
    "    \"\"\"\n",
    "\n",
    "    with open(f'../Parent_test/{path_model_name}_true_articles.txt', 'w', encoding='utf-8') as f:\n",
    "        for articles in true_articles:\n",
    "            f.write(f'{articles} \\n')\n",
    "\n",
    "    with open(f'../Parent_test/{path_model_name}_decode_predictions.txt', 'w', encoding='utf-8') as f:\n",
    "        for predictions in decoded_predictions:\n",
    "            f.write(f'{predictions} \\n')\n",
    "\n",
    "    with open(f'../Parent_test/{path_model_name}_rdfs.txt', 'w', encoding='utf-8') as f:\n",
    "        for pairs in rdfs:\n",
    "            f.write(f'{pairs} \\n')\n",
    "\n",
    "\n",
    "def prepare_inputs_parent(RDFs):\n",
    "    \"\"\"\n",
    "    Cleans the RDF pairs and transforms them in the proper format so that the parent module can calculate with it.\n",
    "    \"\"\"\n",
    "\n",
    "    attribute_value_pairs = []\n",
    "\n",
    "    for iteration, inputRDF in enumerate(RDFs):\n",
    "        split_RDF = inputRDF.split(\", \")\n",
    "        entry=[]\n",
    "        for connected_pair in split_RDF:\n",
    "            if '[' in connected_pair:\n",
    "                connected_pair = connected_pair.replace('[', '')\n",
    "            if ']' in connected_pair:\n",
    "                connected_pair = connected_pair.replace(']', '')\n",
    "            if '_' in connected_pair:\n",
    "                connected_pair = connected_pair.replace('_', ' ')\n",
    "            split_pair = tuple(connected_pair.split(' | '))\n",
    "            entry.append((split_pair))\n",
    "        attribute_value_pairs.append(entry)\n",
    "    return attribute_value_pairs\n",
    "\n",
    "\n",
    "def parent_attempt(path_model_name, generations, references, rdfs):\n",
    "    \"\"\"\n",
    "    The Parent metric needs special treatment, as it only accepts specific inputs and file types.\n",
    "    \"\"\"\n",
    "    prepared_rdfs = prepare_inputs_parent(rdfs)\n",
    "    write_to_text_parent(path_model_name, generations, references, prepared_rdfs)\n",
    "\n",
    "    %run -i \"~E:/ArriaThesis/MscThesis/Code/Evaluation_Code/Parent.py\" --references f\"E:/ArriaThesis/MscThesis/Parent_test/{path_model_name}_true_articles.txt\" \\\n",
    "                                                                        --generations f\"E:/ArriaThesis/MscThesis/Parent_test/{path_model_name}_decode_predictions.txt\"  \\\n",
    "                                                                        --tables f\"E:/ArriaThesis/MscThesis/Parent_test/{path_model_name}_rdfs.txt\"\n",
    "\n",
    "def log_results(path_model_name, results):\n",
    "    ## Dutch model has / which ruins the pathing \n",
    "    with open(f'../Logging_Results/{path_model_name}_logResults.json', 'w') as convert_file:\n",
    "        convert_file.write(json.dumps(results))\n",
    "\n",
    "def fine_tune_model(model_name):\n",
    "    # ensure cuda compatability\n",
    "    ensure_cuda_compatability()\n",
    "\n",
    "    # I instantiate the tokenizer as a global variable, as the .map function in transform_datasets was not working properly. \n",
    "    # This should not be an issue, as the tokenizer remains consistent during training and evaluation.\n",
    "    global tokenizer\n",
    "    \n",
    "    #global device\n",
    "    #device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    ## retrieve model and tokenizer from huggingface to prepare dataset\n",
    "    model, tokenizer = preprocess_model(model_name)\n",
    "    \n",
    "    ### retrieve the unprocessed data from the csv files\n",
    "    entire_dataset = load_CACAPO_data()\n",
    "    \n",
    "    # ## process the dataset and split it into its natural train, val, test split\n",
    "    train_ds, val_ds, test_ds, true_articles_test, test_rdf_inputs = transform_datasets(entire_dataset)\n",
    "\n",
    "    ### setup the training arguments \n",
    "    training_args = set_training_args(model_name=model_name, learning_rate = 0.005, \n",
    "                                      num_train_epochs= 2, evaluation_strategy = 'steps', generation_num_beams=5, generation_max_length = 100, \n",
    "                                      gradient_accumulation_steps = 2, per_device_train_batch_size= 8, per_device_eval_batch_size= 4)\n",
    "\n",
    "    ###create a trainer instance \n",
    "    trainer = set_trainer(model_name, training_args, train_ds, val_ds, tokenizer)\n",
    "\n",
    "    # Both mt5 and T5-dutch have / in their name, which makes pathing more chaotic\n",
    "    if '/' in model_name:\n",
    "       path_model_name = model_name.replace('/', '_')\n",
    "\n",
    "    ## Finally fine-tune the model and save it\n",
    "    train_and_save(trainer, path_model_name)\n",
    "\n",
    "    testset_evaluation_results = evaluate_test_set( path_model_name, test_ds, true_articles_test, test_rdf_inputs)\n",
    "\n",
    "    return testset_evaluation_results\n",
    "\n",
    "def main():\n",
    "    global model_name\n",
    "    models = [\"yhavinga/t5-v1.1-base-dutch-cased\", \"google/mt5-small\",\"google/byt5-small\"] #  #'google/mt5-base' #'google/mt5-base'      #yhavinga/t5-base-dutch         #t5-base       #google/mt5-small   #google/byt5-base # facebook/nllb-200-distilled-600M\n",
    "\n",
    "    # for model_type in models:\n",
    "    #     model_name = model_type\n",
    "    #     results = fine_tune_model(model_name)\n",
    "        #print(results)\n",
    "    model_name = \"t5-base\"\n",
    "    results = fine_tune_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 1.12.1\n",
      "Cuda version: 11.3\n",
      "Cudnn version: 8302\n",
      "Is cuda available: True\n",
      "Number of cuda devices: 1\n",
      "Current default device: 0\n",
      "First cuda device: <torch.cuda.device object at 0x000002137F039F08>\n",
      "Name of the first cuda device: NVIDIA GeForce RTX 3070\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Anaconda\\envs\\CudaSupEnv\\lib\\site-packages\\transformers\\convert_slow_tokenizer.py:447: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  \"The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option\"\n",
      "Using custom data configuration Cleaned_data-4b8a9b4c5ecd8560\n",
      "Found cached dataset csv (C:/Users/Simon/.cache/huggingface/datasets/csv/Cleaned_data-4b8a9b4c5ecd8560/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOGGING: preprocess_model DONE \n",
      "\n",
      "LOGGING: load_CACAPO_data DONE \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 99.91it/s]\n",
      "Parameter 'function'=<function preprocess_data at 0x0000021309CEB798> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
      "  0%|          | 0/16 [00:00<?, ?ba/s]e:\\Anaconda\\envs\\CudaSupEnv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3543: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  \"`as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your \"\n",
      " 94%|█████████▍| 15/16 [00:01<00:00,  8.01ba/s]\n",
      " 50%|█████     | 1/2 [00:00<00:00,  5.10ba/s]\n",
      " 75%|███████▌  | 3/4 [00:00<00:00,  9.25ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOGGING: transform_datasets DONE \n",
      "\n",
      "LOGGING: set_training_args DONE \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cuda_amp half precision backend\n",
      "***** Running training *****\n",
      "  Num examples = 15290\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 1912\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOGGING: set_trainer DONE \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1912 [00:00<?, ?it/s]You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 978.00 MiB (GPU 0; 8.00 GiB total capacity; 6.36 GiB already allocated; 0 bytes free; 6.38 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15432\\451043146.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15432\\882471011.py\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    456\u001b[0m         \u001b[1;31m#print(results)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    457\u001b[0m     \u001b[0mmodel_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"google/mt5-base\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 458\u001b[1;33m     \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfine_tune_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15432\\882471011.py\u001b[0m in \u001b[0;36mfine_tune_model\u001b[1;34m(model_name)\u001b[0m\n\u001b[0;32m    441\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m     \u001b[1;31m## Finally fine-tune the model and save it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 443\u001b[1;33m     \u001b[0mtrain_and_save\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath_model_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    444\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    445\u001b[0m     \u001b[0mtestset_evaluation_results\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluate_test_set\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mpath_model_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_ds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrue_articles_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_rdf_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15432\\882471011.py\u001b[0m in \u001b[0;36mtrain_and_save\u001b[1;34m(trainer, path_model_name)\u001b[0m\n\u001b[0;32m    270\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    271\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtrain_and_save\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath_model_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 272\u001b[1;33m     \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    273\u001b[0m     \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"../Models/{path_model_name}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\Anaconda\\envs\\CudaSupEnv\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1502\u001b[0m             \u001b[0mresume_from_checkpoint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1503\u001b[0m             \u001b[0mtrial\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1504\u001b[1;33m             \u001b[0mignore_keys_for_eval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mignore_keys_for_eval\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1505\u001b[0m         )\n\u001b[0;32m   1506\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\Anaconda\\envs\\CudaSupEnv\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1740\u001b[0m                         \u001b[0mtr_loss_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1741\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1742\u001b[1;33m                     \u001b[0mtr_loss_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1743\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1744\u001b[0m                 if (\n",
      "\u001b[1;32me:\\Anaconda\\envs\\CudaSupEnv\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   2484\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2485\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_loss_context_manager\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2486\u001b[1;33m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2487\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2488\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_gpu\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\Anaconda\\envs\\CudaSupEnv\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[1;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[0;32m   2516\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2517\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2518\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2519\u001b[0m         \u001b[1;31m# Save past state if it exists\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2520\u001b[0m         \u001b[1;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\Anaconda\\envs\\CudaSupEnv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\Anaconda\\envs\\CudaSupEnv\\lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1679\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1680\u001b[0m             \u001b[0mloss_fct\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCrossEntropyLoss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1681\u001b[1;33m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_fct\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlm_logits\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlm_logits\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1682\u001b[0m             \u001b[1;31m# TODO(thom): Add z_loss https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/layers.py#L666\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1683\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\Anaconda\\envs\\CudaSupEnv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\Anaconda\\envs\\CudaSupEnv\\lib\\site-packages\\torch\\nn\\modules\\loss.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1164\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0;32m   1165\u001b[0m                                \u001b[0mignore_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1166\u001b[1;33m                                label_smoothing=self.label_smoothing)\n\u001b[0m\u001b[0;32m   1167\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1168\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\Anaconda\\envs\\CudaSupEnv\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3012\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3013\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3014\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3015\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3016\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 978.00 MiB (GPU 0; 8.00 GiB total capacity; 6.36 GiB already allocated; 0 bytes free; 6.38 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models voor experimenten:\n",
    "\n",
    "\n",
    "## Works\n",
    "- yhavinga/t5-base-dutch --> puurly dutch \n",
    "- t5-base --> english\n",
    "- google/mt5-small\n",
    "- google/byt5-base \n",
    "\n",
    "## Doesnt work\n",
    "- google/mT5-base --> multilingual --> Lijkt te werken op Colab\n",
    "- google/byt5-base --> Testen op Colab\n",
    "- - facebook/nllb-200-distilled-600M --> needs further testing --> testen op Colab\n",
    "\n",
    "## Needs testing\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Als dataloader, bf16 en tf32 niet werken --> check DeepSpeed ZeRO --> https://huggingface.co/docs/transformers/perf_train_gpu_one#choice-of-gpu\n",
    "#https://github.com/huggingface/transformers/issues/14608"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let op:\n",
    "\n",
    "Als je deepseed gebruikt, lees Shared Configuration\n",
    "https://huggingface.co/docs/transformers/main_classes/deepspeed#deployment-in-notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. https://huggingface.co/docs/transformers/main_classes/deepspeed\n",
    "2. https://www.deepspeed.ai/docs/config-json/\n",
    "3. https://github.com/microsoft/DeepSpeedExamples/blob/master/gan/gan_deepspeed_config.json\n",
    "4. https://huggingface.co/blog/zero-deepspeed-fairscale\n",
    "5. https://github.com/huggingface/transformers/issues/8771"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('CudaSupEnv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8ce04b5a4dcd593beec21f85756cfa75ba50da4e44d5dd0d068d93735596035b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
